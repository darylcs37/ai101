<html>
<head>
<title>
	AIGC Awesome List
</title>


<style>
body {
	margin-top   : 2pt;
	margin-left  : 2pt;
	margin-right : 2pt;

	scrollbar-face-color       : gray;
	font-size                  : 20pt;
	scrollbar-highlight-color  : white;
	scrollbar-shadow-color     : black;
	color                      : black;
	scrollbar-3dlight-color    : black;
	scrollbar-arrow-color      : black;
	scrollbar-track-color      : silver;
      font-family                : Consolas,monaco,monospace;
	scrollbar-darkshadow-color : white;
	text-align                 : left
}

table {
	font-size      : 12pt;
	border-spacing : 5px;
	table-layout   : fixed;
	margin-left    : auto;
	margin-right   : auto;
}

th {
	color  : green;
	border : 1px solid black;
}

td {
	word-wrap : break-word;
}


mark {
	background-color : #DAF7A6;
	color            : black;
}

pre {
    background  : #f4f4f4;
    border      : 1px solid #ddd;
    border-left : 3px solid #f36d33;
    color       : #666;
    page-break-inside: avoid;
    font-family   : monospace;
    font-size     : 15px;
    line-height   : 1.6;
    margin-bottom : 1.6em;
    max-width     : 95%;
    overflow      : auto;
    padding       : 1em 1.5em;
    display       : block;
//    word-wrap     : break-word;
    white-space: pre-wrap;       /* Since CSS 2.1 */
    word-wrap: break-word;       /* Internet Explorer 5.5+ */
}

/*---Title-------------------------------------------------------------------*/
H1 {
	border-left-width:  "20pt";
	border-left-style:  solid;
	border-left-color : black;
	border	: sunken;
	background-color : #00FFC3;
	color	           : #111111;
	font-size        : "30px";
	text-align       : left;

	padding-top      : 5pt;
	padding-left     : 5pt;
	padding-bottom   : 5pt;
	padding-right    : 5pt;
	margin           : 0pt;
}

A:link    { color: #2F4F4F; } /* unvisited link daa520 */
A:visited { color: #405888; } /* visited links */

/*---Header------------------------------------------------------------*/
/* FF5733  FFC300 */
H2 {
	background-color : #0070C0;
	color            : #ffffff;
	font-size        : 30px;

	padding-top    : 5pt;
	padding-left   : 5pt;
	padding-bottom : 5pt;
	padding-right  : 5pt;
	margin         : 0pt;
}

H3 {
	background-color : #C70039;
	color        : #FFFFFF;
	font-size    : 20px;

	padding-top    : 5pt;
	padding-left   : 5pt;
	padding-bottom : 5pt;
	padding-right  : 5pt;
	margin         : 0pt;
}

H4 {
	background-color : #CD7F32;
	color          : #FFFFFF;
	font-size      : 15px;

	padding-top    :  8pt;
	padding-left   :  8pt;
	padding-bottom :  8pt;
	padding-right  :  8pt;
}

mark {
	background-color : #DAF7A6;
	color            : black;
}

markBLUE {
	background-color : #A6DAF7;
	color            : black;
}

markGRAY {
	background-color : #C0C0C0;
	color            : black;
}

markORANGE {
	background-color : #F7C610;
	color            : black;
}

markRED {
	background-color : #F7A6DA;
	color            : black;
}

/*---3D boxes------------------------------------------------------------------*/
.GreyList {
	background-color : #dddddd;
	border-width   : 0px;
	padding-top    : 20px;
	padding-bottom : 5px;
	border-bottom-width : 2px;
}

.LightGreyList {
	background     : #eeeeee;
	border-width   : 0px;
	padding-top    : 20px;
	padding-bottom : 5px;
	border-bottom-width : 2px;
	padding-left        : 10px;
	padding-right       : 5px;
}

.CodeExample{
	background  : #eeeeee;
	font-family : Courier, Helvetica, Arial;
}

/*---Now Tags------------------------------------------------------------------*/
.tagBlue {
	background  : #3377ff;
	color       : #FFFFFF;
	href        : #FFFFFF;
	anchor      : #FFFFFF;
}

/*---------------------------------------------------------------------------*/
/* menu header */
  .tblBlueHeader {font-size: 12px; font-weight: bold;
                  background: #333399; border-top: solid 1px #394CB8; border-left: solid 1px #394CB8;}
  .tblBlueBody {font-size: 12px; color: #ffffff; background: #333366;
                border-top: solid 1px #434376; border-left: solid 1px #434376; border-bottom: solid 1px #232356; border-right: solid 1px #232356;}

  .tblGreenHeader {font-size: 12px; font-weight: bold; background: #4D9933;
                   border-top: solid 1px #437643; border-left: solid 1px #437643; border-bottom: solid 1px #235623; border-right: solid 1px #235623;}
  .tblGreenBody {font-size: 12px; background: #1B3D29;
                 border-top: solid 1px #437643; border-left: solid 1px #437643; border-bottom: solid 1px #235623; border-right: solid 1px #235623;}
  a.tblGreenLink {text-decoration: none; color: #000000; linkColor:#000000; alinkColor:#000000; vlinkColor :#000000;}
  a.tblGreenLink.link {color:#000000; }
  a.tblGreenLink.visited {color:#000000; }

/*---------------------------------------------------------------------------*/
</style>


</head>
<body>



<p/>

<h2>	TechScan - Artificial Intelligence Generated Content (AIGC) - Jun 2025	</h2>

<table width="100%">
<tr valign="top"><td width="30%"><!----------------------------------------------------------->

<!----------------------------------------------------------->
<h3>	Companies			</h3><ol start=1 type=1>

<li/> AIMageLab <a href="https://github.com/orgs/aimagelab/repositories">	aimagelab/repositories	</a>

<li/> AixonLab <a href="https://www.aixonlab.com/">		AixonLab	</a>,
	<a href="https://huggingface.co/aixonlab">		HuggingFace - AixonLab	</a>,
	<a href="https://huggingface.co/aixonlab/flux.1-lumiere-alpha/blob/main/comfy/lumiere_alpha_workflow.json">		lumiere_alpha	</a>

<li/>	
	<mark>
	Alibaba  <a href="https://github.com/alibaba/Tora">	Alibaba 	</a>,	
	<a href="https://huggingface.co/alibaba-pai">		Alibaba PAI (Platform for AI)	</a>, 
	<a href="https://github.com/ali-vilab/">		Alibaba TongYi Vision Intelligence Lab	</a>,
	<a href="https://qwenlm.github.io">		Qwen	</a>,
	<a href="https://github.com/QwenLM/Qwen2-VL">		Qwen2-VL	</a>,
	<a href="https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/tree/main">		Qwen2-VL-7B-Instruct	</a>,
	<a href="https://github.com/ali-vilab/ACE_plus">		ACE++	</a>,
	<a href="https://ali-vilab.github.io/In-Context-LoRA-Page/">		In-Context-LoRA	</a>,
	<a href="https://lucaria-academy.github.io/Animate-X/">	Animate-X	</a>,
	<a href="https://huggingface.co/ali-vilab/ACE_Plus/">	ACE Plus ++	</a>,
	<a href="https://humanaigc.github.io/omnitalker/">	Alibaba - OmniTalker	</a>, 
	<a href="https://github.com/Wan-Video/Wan2.1">	Wan: Open Large-Scale Video Generative Models </a>
	<a href="https://huggingface.co/Kijai/WanVideo_comfy/tree/main">	Models - Kijai/WanVideo_comfy </a>, 
	<a href="https://github.com/ali-vilab/UniAnimate-DiT">	UniAnimate-DiT	</a>,
	</mark>




<li/> Alimama  <a href="https://github.com/orgs/alimama-creative/repositories">		Github- Alimama Creative	</a>,
	<a href="https://huggingface.co/alimama-creative">		HuggingFace - alimama-creative	</a>

<li/> AllenAI <a href="https://huggingface.co/allenai">		AllenAI	</a>,
	<a href="https://allenai.org/open-data">	Open Data	</a>, 
	<a href="https://allenai.org/olmo">	Molml Olmo </a>,
	<a href="https://molmo.allenai.org/">	Molml Demo </a>,
	<a href="https://cychenyue.com/28204.html">	AllenAI Molmo 7B D		</a>,
	<a href="https://github.com/SeanScripts/ComfyUI-PixtralLlamaMolmoVision">	Pixtral Llama Molmo Vision	</a>, 
	<a href="https://github.com/aigc3d">	Applied Vision Lab, Institute for Intelligent Computing	</a>,
	<a href="https://huggingface.co/datasets/allenai/tulu-3-sft-personas-instruction-following">	allenai/tulu-3-sft-personas-instruction-following	</a>
	<a href="https://objaverse.allenai.org/">		Objaverse-XL - A Universe of 10M+ 3D Objects	</a>,
	<a href="https://huggingface.co/spaces/allenai/reward-bench">	RewardBench: Evaluating Reward Models	</a>, 
	<a href="https://huggingface.co/spaces/allenai/ZebraLogic">	ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning	</a>, 
	<a href="https://huggingface.co/spaces/allenai/super_leaderboard">	SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories	</a>, 
	<a href="https://huggingface.co/spaces/allenai/ZeroEval">	ZeroEval: Benchmarking LLMs for Reasoning	</a>, 
	<a href="https://huggingface.co/spaces/allenai/WildBench">	WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild	</a>, 


<li/> Alpha-VLLM <a href="https://github.com/Alpha-VLLM/Lumina-mGPT-2.0">	Lumina-mGPT 2.0: Stand-alone Autoregressive Image Modeling	</a>

<li/> ANT <a href="https://github.com/antgroup/">		ANT	</a>,
	<a href="https://github.com/antgroup/echomimic_v2">		EchoMimic	</a>, 
	<a href="https://huggingface.co/spaces/BadToBest/EchoMimic">		EchoMimic - Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditioning	</a>

<li/> Apple  <a href="https://huggingface.co/apple/DepthPro">	Apple DepthPro	</a>,
	<a href="https://github.com/spacepxl/ComfyUI-Depth-Pro">	ComfyUI-Depth-Pro	</a>

<li/> 
	<mark>
	Bilibili <a href="https://github.com/index-tts/index-tts">	IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System (based on Tortise/XTTS)	</a>,
	<a href="https://arxiv.org/abs/2502.05512">	IndexTTS Paper	</a>,
	<a href="https://index-tts.github.io/">	IndexTTS Samples	</a>,
	<a href="https://huggingface.co/spaces/IndexTeam/IndexTTS">	IndexTTS demo	</a>,
	<a href="https://github.com/billwuhao/ComfyUI_IndexTTS">	billwuhao/ComfyUI_IndexTTS	</a>, 
	</mark>

<li/> Black Forest Labs <a href="https://huggingface.co/black-forest-labs">	Black Forest Labs	</a>

<li/> BRIA <a href="https://huggingface.co/briaai/RMBG-2.0">		BRIA Background Removal v2.0	</a>

<li/> 
	<mark>
	ByteDance <a href="https://huggingface.co/ByteDance">	ByteDance	</a>,
	<a href="https://opensource.bytedance.com/project">	ByteDance Research </a>,
	<a href="https://chenglin-yang.github.io/1.58bit.flux.github.io/">	1.58-bit FLUX	</a>,
	<a href="https://github.com/bytedance/LatentSync">	GitHub - LipSync - LatentSync </a>,
	<a href="https://byteaigc.github.io/X-Portrait2/">	X-Portrait2	</a>,
	<mark><a href="https://omnihuman-lab.github.io/">	OmniHuman-1	</a></mark>,
	<mark><a href="https://grisoon.github.io/DreamActor-M1/">	DreamActor-M1	</a></mark>,
	<a href="https://github.com/Saiyan-World/goku">	GitHub - goku - Flow Based Video Generative Foundation Models </a>,

	<a href="https://github.com/bytedance/MegaTTS3">	bytedance - MegaTTS3 (voice cloning)	</a>, 
	<a href="https://huggingface.co/spaces/ByteDance/MegaTTS3">	MegaTTS3  Demo	</a>, 
	<a href="https://github.com/1038lab/ComfyUI-MegaTTS">	1038lab/ComfyUI-MegaTTS	</a>, 
	<a href="https://openart.ai/workflows/ailab/comfyui-megatts/K0urCR510JCn2FAcYxoW">	ailab/comfyui-megatts	</a>, 
	<a href="https://github.com/bytedance/UNO">	UNO (e2i)	</a>, 
	<a href="https://github.com/bytedance/RealCustom">	RealCustom	</a>, 
	<a href="https://github.com/bytedance/InfiniteYou">	InfiniteYou	</a>, 
	<a href="https://github.com/bytedance/DreamO">	DreamO		</a>, 
	<a href="https://github.com/bytedance/DreamFit">	DreamFit		</a>, 
	<a href="https://github.com/bytedance-seed/BAGEL">	BAGEL (ByteDance Adaptive Generative Language Model) 		</a>, 
	</mark>

<li/> CanopyLabs.ai <a href="https://canopylabs.ai/model-releases">	Voice Cloning - Natural intonation, emotion, and rhythm that is superior to SOTA closed source models (giggle, gasp, angry, happy) 	</a>,
	<a href="https://github.com/canopyai/Orpheus-TTS">	Orpheus-TTS	</a>, 
	<a href="https://huggingface.co/spaces/MohamedRashad/Orpheus-TTS">	Orpheus-TTS demo	</a>, 
	<a href="https://github.com/canopyai/Orpheus-TTS/tree/main/additional_inference_options/watermark_audio">	watermark_audio	</a>, 
	<a href="https://github.com/ShmuelRonen/ComfyUI-Orpheus-TTS">	ShmuelRonen/ComfyUI-Orpheus-TTS	</a>

<li/> Cohere  <a href="https://cohere.com/blog/command-r7b-arabic">	r7b-arabic	</a>,
	<a href="https://huggingface.co/CohereForAI">	HuggingFace - CohereForAI		</a>,
	<a href="https://huggingface.co/CohereForAI/c4ai-command-r7b-arabic-02-2025">	HuggingFace - r7b-arabic		</a>,

<li/>	ComfyUI 
	<a href="https://docs.comfy.org/tutorials/advanced/hidream">	HiDream t2i	</a>, 
	<a href="https://docs.comfy.org/tutorials/image/hidream/hidream-e1">	HiDream e1	</a>, 
	<a href="https://docs.comfy.org/tutorials/video/wan/wan-video">	Wan t2v	</a>, 
	<a href="https://comfyui.org/en/text-to-video-wanvideo-controlnet">	text-to-video wanvideo-controlnet	</a>, 
	<a href="https://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1">	text-to-music ace-step-v1	</a>, 

<li/>	DeepSeek 
	<a href="https://huggingface.co/deepseek-ai">	HuggingFace - DeepSeek 	</a>,
	<a href="https://huggingface.co/blog/open-r1">	HuggingFace - Open R1	</a>,
	<a href="https://github.com/huggingface/open-r1">	GitHub - Open R1	</a>, 
	<a href="https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c">	unsloth/deepseek-v3	</a>,

<li/> Facebook META  <a href="https://ai.meta.com/sam2/">	Facebook META SAM2(Segment Anything 2) </a>,
	<a href="https://github.com/kijai/ComfyUI-segment-anything-2">	ComfyUI-segment-anything-2	</a>,
	<a href="https://github.com/neverbiasu/ComfyUI-SAM2">	ComfyUI-SAM2	</a>,
	<a href="https://openart.ai/workflows/cgtips/comfyui---segment-anything-2-sam2---method-2/GTbSSKbVl6KpGu6uh4Lj">	OpenArt - SAM2	</a>,
	<a href="https://huggingface.co/city96/DiT/tree/main">	DiT	</a>,
	<a href="https://github.com/facebookresearch/DiT">	Facebook Research Scalable Diffusion Models with Transformers (DiT)	</a>,
	<a href="https://github.com/facebookresearch/seamless_communication">	Seamless4MT	</a>,
	<a href="https://seamless.metademolab.com/demo">	Seamless4MT Demo	</a>,
	<a href="https://congwei1230.github.io/MoCha/">	MoCha - Towards Movie-Grade Talking Character Synthesis	</a>,
	<a href="https://developers.meta.com/horizon/blog/AssetGen2">	AssetGen2 - Animated 3D game assets	</a>, 
	<a href="https://runsenxu.com/projects/Multi-SpatialMLLM/">	Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models	</a>


<li/>	FAL <a href="https://huggingface.co/fal">	fal	</a>

<li/> FreePik <a href="https://huggingface.co/Freepik">		HuggingFace - FreePik 	</a>, 
	<a href="https://www.freepik.com/blog/f-lite-freepik-and-fal-ai-unveil-open-source-image-model-trained-on-licensed-data/">	Freepik/F-Lite Blog </a>,
	<a href="https://github.com/fal-ai/f-lite">	fal-ai/f-lite	</a>, 
	<a href="https://huggingface.co/Freepik/F-Lite">	Freepik/F-Lite	</a>

<li/> Genmo	<a href="https://www.genmo.ai/">	Genmo	</a>
	<a href="https://github.com/genmoai/mochi">	Mochi	</a>,
	<a href="https://github.com/kijai/ComfyUI-MochiWrapper">	ComfyUI-MochiWrapper	</a>

<li/> Google <a href="https://github.com/google-deepmind/graphcast">		Google-deepmind	</a>,
	<a href="https://deepbrainai-research.github.io/float/">	Google DeepBrain - Generative Motion Latent FlOw MAtching for Audio-driven Talking Portrait (FLOAT)	</a>,
	<a href="https://deepmind.google/discover/blog/gencast-predicts-weather-and-the-risks-of-extreme-conditions-with-sota-accuracy/">	GraphCast / GenCast	</a>, 
 	<a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">	Google Genie2: Generative Interactive Environments	</a>

<li/> HelloVision <a href="https://github.com/HelloVision/">		HelloVision	</a>,
	<a href="https://github.com/HelloVision/ComfyUI_HelloMeme">		HelloMeme	</a>

<li/> HongKong University of Science & Technology (HKUST) <a href="https://huggingface.co/spaces/srinivasbilla/llasa-3b-tts">	Llasa-3B	</a>,
	<a href="https://huggingface.co/HKUSTAudio/Llasa-3B">	HKUSTAudio/Llasa-3B	</a>,
	<a href="https://huggingface.co/HKUSTAudio/AudioX">	HKUSTAudio/AudioX	</a>,
	<a href="https://github.com/ZeyueT/AudioX">	ZeyueT/AudioX	</a>, 
	<a href="https://replicate.com/kjjk10/llasa-3b-long">	Replicate - kjjk10/llasa-3b-long	</a>

<li/> HuggingFace 
	<a href="https://huggingface.co/HuggingFaceTB">		HuggingFace	</a>,
	<a href="https://huggingface.co/spaces">		HuggingFace	Spaces </a>,
	<a href="https://huggingface.co/latent-consistency">		Latent Consistency (LCM)	</a>, 
	<a href="https://huggingface.co/spaces/HuggingFaceTB/SmolVLM-500M-Instruct-WebGPU">		Vision Language Model - SmolVLM-500M-Instruct-WebGPU	</a>,

<li/> International Digital Economy Academy (IDEA-Research) <a href="https://github.com/IDEA-Research">	International Digital Economy Academy (IDEA-Research)	</a>,
	<a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">	IDEA-Research/Grounded-Segment-Anything	</a>,
	<a href="https://github.com/IDEA-Research/DINO-X-API">	DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding	</a>,
	<a href="https://deepdataspace.com/en/blog/dino-xseek/">	Dino-X blog	</a>,
	<a href="https://deepdataspace.com/playground/ivp">	TREX-2 - object counting </a>,
	<a href="https://cloud.deepdataspace.com/playground/dino-x">	dino-x - Detection - Segmentation -  Keypoints - Generative Understanding </a>,
	<a href="https://deepdataspace.com/playground/ivp_video">	Video - Object Tracking </a>,

<li/> Kuaishou  <a href="https://github.com/KwaiVGI">		Kuaishou Visual Generation and Interaction Center </a>,
	<a href="https://huggingface.co/Kwai-Kolors">		Kwai-Kolors	</a>,
	<a href="https://github.com/orgs/Kwai-Kolors/repositories">		Kwai-Kolors	</a>,
	<a href="https://github.com/kijai/ComfyUI-KwaiKolorsWrapper">	kijai/ComfyUI-KwaiKolorsWrapper	</a>,
	<a href="https://github.com/KwaiVGI/LivePortrait">		LivePortrait	</a>,
	<a href="https://github.com/KwaiVGI/3DTrajMaster">		3DTrajMaster	</a>,

<li/> Liblib AI <a href="https://github.com/Xiaojiu-z/EasyControl">		EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer 	</a>,
	<a href="https://github.com/jax-explorer/ComfyUI-easycontrol">		ComfyUI-easycontrol (40Gb VRAM)	</a>

<li/> 
	<mark>
	Lightricks 
	<a href="https://www.lightricks.com/">	Lightricks 	</a>, 
	<a href="https://huggingface.co/Lightricks/LTX-Video">		Lightricks - LTX-Video	</a>,
	<a href="https://github.com/Lightricks/ComfyUI-LTXVideo">		ComfyUI-LTXVideo	</a>,
	<a href="https://openart.ai/workflows/cat_untimely_42/ltx-video-enhance-stg/wMIwpeYhc372cOtpiiNo">	OpenArt - cat_untimely_42/ltx-video-enhance-stg	</a>,
	<a href="https://openart.ai/workflows/cat_untimely_42/ltx-video-video-to-video/kqyFkoCU8s16hsjdqZRu">	OpenArt - cat_untimely_42/ltx-video-video-to-video	</a>,
	<a href="https://openart.ai/workflows/monkey_hard-to-find_14/comfyui-ltx-video/NNBGp1H777pvivzDjFwa">		OpenArt - monkey_hard-to-find_14/comfyui-ltx-video	</a>,
	<a href="https://openart.ai/workflows/toucan_chilly_4/ltx/CvRMMGcKETGECx59xZFX">		OpenArt - toucan_chilly_4/ltx	</a>,
	<a href="https://civitai.com/models/995093?modelVersionId=1191358">	CivitAI - LTX IMAGE to VIDEO with STG, CAPTION & CLIP EXTEND workflow	</a>
	<a href="https://github.com/logtd/ComfyUI-LTXTricks">		GitHub - logtd/ComfyUI-LTXTricks	</a>,
	<a href="https://civitai.com/models/995093/ltx-image-to-video-with-stg-caption-and-clip-extend-workflow">	LTX IMAGE to VIDEO with STG, CAPTION & CLIP EXTEND workflow	</a>
	</mark>

<li/> LG <a href="https://felixtaubner.github.io/cap4d/">		LG - CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models	</a>

<li/>	MiaoshouAI <a href="https://www.miaoshouai.com/">	MiaoshouAI	</a>,
	<a href="https://huggingface.co/MiaoshouAI">	HuggingFace - Florence-2-base-PromptGen-v2.0 / Florence-2-large-PromptGen-v2.0	</a>
	<a href="https://huggingface.co/MiaoshouAI/Florence-2-large-PromptGen-v1.5">	Florence-2-large-PromptGen-v1.5	</a>
	<a href="https://github.com/miaoshouai/ComfyUI-Miaoshouai-Tagger">	ComfyUI-Miaoshouai-Tagger	</a>

<li/>	Microsoft <a href="https://huggingface.co/microsoft">	Microsoft (Florence2, Phi 3.5, Phi 4) </a>,
	<a href="https://huggingface.co/microsoft/Phi-4-multimodal-instruct">	Phi-4-multimodal-instruct	</a>, 
	<a href="https://wangrc.site/MoGePage/">	MoGe - Monocular 2D->3D </a>,
	<a href="https://github.com/microsoft/MoGe">	MoGe - Monocular 2D->3D </a>,
	<a href="https://github.com/kijai/ComfyUI-MoGe">	kijai/ComfyUI-MoGe </a>,
	<a href="https://huggingface.co/spaces/Ruicheng/MoGe">	MoGe demo	</a>, 
	<a href="https://www.microsoft.com/en-us/research/articles/magma-a-foundation-model-for-multimodal-ai-agents/">	Magma multimodal agents </a>,
	<a href="https://microsoft.github.io/Magma/">	GitHub - Magma </a>,
	<a href="https://www.microsoft.com/en-us/research/publication/peace-empowering-geologic-map-holistic-understanding-with-mllms/">	[GIS-LLM] PEACE: Empowering Geologic Map Holistic Understanding with MLLMs	</a>,
	<a href="https://github.com/The-AI-Alliance/GEO-Bench-VLM">	[GIS-LLM] GEO-Bench-VLM	</a>,
	<a href="https://huggingface.co/microsoft/bitnet-b1.58-2B-4T">	1-bit LLM	</a>,

<li/> MiniMaxi <a href="https://www.minimaxi.com/en">	MiniMaxi	</a>,
	<a href="https://huggingface.co/MiniMaxAI">	MiniMaxAI	</a>,
	<a href="https://hailuoai.video/">		HailuoAI	</a>,

<li/>	Mistral  <a href="https://huggingface.co/mistralai">	Mistral  </a>,
	<a href="https://mistral.ai/news/mistral-small-3/">	Mistral small-3  </a>

<li/> PixelWave <a href="https://huggingface.co/mikeyandfriends/PixelWave_FLUX.1-dev_03/tree/main">		mikeyandfriends - PixelWave	</a>,
	<a href="https://civitai.com/user/humblemikey/models">		CivitAI - user - humblemikey (Art Style, PixelWave)	</a>

<li/> MoonshotAI 	<a href="https://github.com/MoonshotAI">	MoonshotAI </a>,
	<a href="https://github.com/MoonshotAI/Kimi-k1.5">	Kimi k1.5: Scaling Reinforcement Learning with LLMs	</a>

<li/> NetEase Fuxi Lab	<a href="https://github.com/NetEase-FuXi?tab=repositories">	GitHub </a>

<li/> NuMind <a href="https://huggingface.co/numind">	NuMind</a>,
	<a href="https://huggingface.co/spaces/numind/NuExtract-1.5">	NuMind NuExtract-1.5</a>,

<li/> Nvidia  <a href="https://huggingface.co/nvidia">	Nvidia </a>,
	<a href="https://github.com/NVlabs">		Nvidia Labs (NVlabs)	</a>,
	<a href="https://github.com/NVlabs/Sana">		GitHub - NVlabs/Sana	</a>,
	<a href="https://github.com/Efficient-Large-Model/ComfyUI_ExtraModels">	GitHub - ComfyUI_ExtraModels	</a>,
	<a href="https://research.nvidia.com/labs/toronto-ai/DiffusionRenderer/">	DiffusionRenderer: Video Diffusion Models	</a>, 
	<a href="https://gaussiantracer.github.io/">	3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes	</a>, 
	<a href="https://github.com/nv-tlabs/3dgrut">	3D Gaussian Ray Tracing (3DGRT)	</a>, 

<li/> NTU S-Lab 
	<a href="https://github.com/yuvraj108c/ComfyUI_InvSR">	Inverse Super Resolution (InvSR)	</a>, 
	<a href="https://github.com/facebookresearch/EdgeTAM">	EdgeTAM: On-Device Track Anything Model </a>

<li/> NUS <a href="https://github.com/Yuanshi9815/OminiControl">	OminiControl: Minimal and Universal Control for Diffusion Transformer	</a>, 
	<a href="https://github.com/Xiaojiu-z/EasyControl">	EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer (Liblib AI)	</a>,
	<a href="https://github.com/showlab/OmniConsistency">	OmniConsistency: Learning Style-Agnostic Consistency from Paired Stylization Data </a>

<li/> OpenAI  <a href="https://huggingface.co/openai">	OpenAI </a>,
	<a href="https://huggingface.co/spaces/openai/whisper">	OpenAI Whisper (transcribe / translate) </a>,

<li/> OpenBMB  
	<a href="https://huggingface.co/openbmb/MiniCPM4-8B">	MiniCPM4-8B	</a>, 
	<a href="https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9"> MiniCPM-o </a>, 
	<a href="https://minicpm-omni-webdemo.internetofagents.net/">	MiniCPM-o demo </a>,

<li/> OpenXLab  <a href="https://openxlab.org.cn/models?lang=en-US">	OpenXLab </a>,
	<a href="https://openxlab.org.cn/models">		Models </a>,

<li/> Ostris <a href="https://huggingface.co/ostris">	Ostris </a>,
	<a href="https://huggingface.co/ostris/OpenFLUX.1">	ostris/OpenFLUX.1 </a>,

<li/> 
	<mark>
	Rednote XiaoHongShu 
	<a href="https://github.com/rednote-hilab">	rednote-hilab </a>,
	<a href="https://huggingface.co/rednote-hilab">	 rednote-hilab  </a>,	<a href="https://github.com/rednote-hilab/dots.llm1">	 Dots LLM </a>,
	</mark>

<li/> Resemble-ai
	<a href="https://huggingface.co/ResembleAI">	ResembleAI	</a>, 
	<a href="https://github.com/resemble-ai/chatterbox">	resemble-ai/chatterbox	</a>,
	<a href="https://github.com/wildminder/ComfyUI-Chatterbox">	ComfyUI-Chatterbox	</a>,
	<a href="https://github.com/resemble-ai/perth">	Perth is a comprehensive Python library for audio watermarking and detection </a>

<li/> <a href="https://github.com/salesforce">		SalesForce	</a>,
	<a href="https://github.com/salesforce/LAVIS/tree/xgen-mm">		xGen-MM (BLIP-3): A Family of Open Large Multimodal Models	</a>

<li/> Shakker-Labs <a href="https://huggingface.co/Shakker-Labs">		Shakker-Labs	</a>,
	<a href="https://openart.ai/workflows/reverentelusarca/flux-filmportrait-lora-by-shakkerlabs/Wp7nyj9LHcogIaHfMvAc">	flux-filmportrait-lora	</a>,
	<a href="https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0">	FLUX.1-dev-ControlNet-Union-Pro 2	</a>,
	<a href="https://huggingface.co/spaces/Shakker-Labs/FLUX-LoRA-Gallery">	FLUX-LoRA-Gallery	</a>


<li/> SkyworkAI <a href="https://github.com/SkyworkAI/SkyReels-V1">	SkyworkAI/SkyReels-V1 </a>,
	<a href="https://huggingface.co/Kijai/SkyReels-V1-Hunyuan_comfy">	Kijai/SkyReels-V1-Hunyuan_comfy </a>,
	<a href="https://skyworkai.github.io/skyreels-audio.github.io/">	SkyReels-Audio	</a>

<li/> StabilityAI	<a href="https://huggingface.co/stabilityai">	StabilityAI	</a>

<li/> StableDiffusion	<a href="https://huggingface.co/stablediffusionapi">	StableDiffusionAPI	</a>


<li/> StepFun.AI	<a href="https://arxiv.org/abs/2504.17761">	Step1X-Edit: A Practical Framework for General Image Editing	</a>, 
	<a href="https://github.com/stepfun-ai/Step1X-Edit">	stepfun-ai/Step1X-Edit	</a>, 
	<a href="https://ace-step.github.io/">	ACE-Step: A Step Towards Music Generation Foundation Model </a>, 
	<a href="https://github.com/stepfun-ai/Step1X-3D">	Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets </a>, 


<li/>	
	<mark>
	Tencent <a href="https://github.com/Tencent/">	Tencent	</a>,
	<a href="https://github.com/orgs/Tencent-Hunyuan/repositories">	Tencent-Hunyuan	</a>,
 	<a href="https://huggingface.co/spaces/TencentARC/PhotoMaker-V2">	PhotoMaker	</a>,
	<a href="https://github.com/Tencent/MimicMotion">	MimicMotion	</a>,
	<a href="https://github.com/Tencent/HunyuanDiT/tree/main/comfyui-hydit">	ComfyUI - HunYuan	</a>,
	<a href="https://github.com/kijai/ComfyUI-HunyuanVideoWrapper">	HunYuan	</a>,
	<a href="https://dit.hunyuan.tencent.com/">	HunYuan	</a>,
	<a href="https://openart.ai/workflows/datou/hunyuan-video-720p/FCYqXugi8pVi3aqgeNZA">		OpenArt - datou/hunyuan-video-720p	</a>,
	<a href="https://openart.ai/workflows/cat_untimely_42/huanyuanvideo-video-to-video/25oGJWr3RYWRrtmaHkRV">		OpenArt - cat_untimely_42/huanyuanvideo-video-to-video	</a>,
	<a href="https://github.com/tencent-ailab/persona-hub">	tencent-ailab/persona-hub	</a>,
	<a href="https://huggingface.co/datasets/proj-persona/PersonaHub">	PersonaHub	</a>,
	<a href="https://github.com/Tencent/InstantCharacter">	InstantCharacter	</a>,
	<a href="https://github.com/jax-explorer/ComfyUI-InstantCharacter">	ComfyUI-InstantCharacter	</a>,
	<a href="https://openart.ai/workflows/t8star/instantcharacterid-/MwQZJ37vMqZqFjIKOWJP">	t8star/instantcharacterid	</a>,
	<a href="https://huggingface.co/tencent/HunyuanCustom">	HunyuanCustom, a multi-modal, conditional, and controllable generation model centered on subject consistency	</a>, 
	<a href="https://github.com/Tencent/HunyuanCustom">	Tencent/HunyuanCustom	(80gb) </a>,
	<a href="https://hunyuanvideo-avatar.github.io/">	HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters	</a>,
	<a href="https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1">		Hunyuan3D-2.1 (10Gb=Shape, 21Gb=Texture, 29Gb=Shape + Texture)		</a>
	</mark>

<li/>	Tsinghua <a href="https://huggingface.co/THUDM">	Tsinghua University Knowledge Engineering Group (KEG) & Data Mining 	</a>,
	<a href="https://huggingface.co/THUDM/CogVideoX-5b">	CogVideoX-5b	</a>,
	<a href="https://docs.google.com/spreadsheets/d/16eA6mSL8XkTcu9fSWkPSHfRIqyAKJbR1O99xnuGdCKY/edit?gid=0#gid=0">	CogVideoX models	</a>,

<li/>	VAST-AI-Research 
	<a href="https://www.tripo3d.ai/">	tripo3d.AI	</a>,
	<a href="https://huggingface.co/VAST-AI">	HuggingFace	</a>, 
	<a href="https://huggingface.co/spaces/VAST-AI/TripoSG">	TripoSG - Image to 3D	</a>, 
	<a href="https://github.com/VAST-AI-Research/TripoSG">	Github - TripoSG	</a>, 
	<a href="https://huggingface.co/spaces/VAST-AI/MV-Adapter-I2MV-SDXL">	MV-Adapter [Image-to-Multi-View]	</a>,
	<a href="https://github.com/VAST-AI-Research">	Github	</a>, 
	<a href="https://medium.com/@thegodtripo">	Medium	</a>,
	<a href="https://github.com/flowtyone/ComfyUI-Flowty-TripoSR">	ComfyUI-Flowty-TripoSR	</a>,
	<a href="https://detailgen3d.github.io/DetailGen3D/">		DetailGen3D: Generative 3D Geometry Enhancement via Data-Dependent Flow	</a>, 
	<a href="https://huggingface.co/VAST-AI/DetailGen3D">		VAST-AI/DetailGen3D	</a>, 
	<a href="https://huggingface.co/spaces/VAST-AI/DetailGen3D">	DetailGen3D	demo </a>,
	<a href="https://zjp-shadow.github.io/works/UniRig/">	One Model to Rig Them All: Diverse Skeleton Rigging with UniRig </a> 

<li/>	ViVago
	<a href="https://vivago.ai/home">	ViVago	</a>, 
	<a href="https://github.com/HiDream-ai/HiDream-E1">	HiDream t2i	</a>, 
	<a href="https://modelscope.cn/models/AI-ModelScope/HiDream-E1-Full">	modelscope/HiDream-E1-Full	</a>, 
	<a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/">	ComfyUI_examples/hidream </a>, 
	<a href="https://openart.ai/workflows/datou/hidream-e1/oMjrZlGRAtZMP41jxpBk">	datou/hidream-e1	</a>, 

<li/>	VisionATrix <a href="https://visionatrix.github.io/">	VisionATrix	</a>,
	<a href="https://github.com/Visionatrix/ComfyUI-PhotoMaker-Plus">	PhotoMaker-Plus	</a>

<li/>	vision-xl <a href="https://vision-xl.github.io/supple/">		vision-xl	</a>,

<li/>	XiaoHongShu  <a href="https://github.com/instantX-research">	Xiaohongshu Instant ID Research	</a>,
	<a href="https://github.com/instantX-research/Regional-Prompting-FLUX">	Regional-Prompting-FLUX		</a>

<li/>	XLabs-AI <a href="https://huggingface.co/XLabs-AI">	XLabs-AI	</a>,
	<a href="https://huggingface.co/XLabs-AI/flux-controlnet-collections">	flux-controlnet-collections	</a>,
	<a href="https://github.com/XLabs-AI/x-flux-comfyui">	GitHub - XLabs-AI/x-flux-comfyui	</a>

<li/>	Zyphra <a href="https://www.zyphra.com/post/beta-release-of-zonos-v0-1">	Zyphra	</a>, 
	<a href="https://playground.zyphra.com/">	Zyphra playground	</a>, 
	<a href="https://www.zyphra.com/post/beta-release-of-zonos-v0-1/">	Zonos v01 Speech TTS	</a>, 


</ol>
<hr><!----------------------------------------------------------->
<h3>	GenAI - ComfyUI - Platforms	</h3><ol start=1 type=1>

<li/>	<a href="https://wan.video/">	Alibaba GenAI Platform	</a>
<li/>	<a href="https://dreamina.capcut.com/ai-tool/explore">	ByteDance GenAI Platform	</a>
<li/>	<a href="https://aivideo.hunyuan.tencent.com/">	Tencent GenAI Platform	</a>

<li/>	<a href="https://medium.com/@dminhk/3-easy-steps-to-run-comfyui-on-amazon-sagemaker-notebook-c9bdb226c15e">	Amazon SageMaker	</a>
<li/>	<a href="https://animemaker.graydient.ai/concepts">	animemaker	</a>
<li/>	<a href="https://www.baseten.co/blog/how-to-serve-your-comfyui-model-behind-an-api-endpoint/">	baseten	</a>
<li/>	<a href="https://www.mage.space/">	mage.space	</a>

<li/>	<a href="https://nexa.ai/models">	nexa.ai (on-device models & inference	</a>
	<a href="https://nexa.ai/models">	Supported Models	</a>

<li/>	<a href="https://openart.ai/pricing">	openart	</a>
<li/>	<a href="https://rendernet.ai/">	RenderNet.ai	</a>

<li/>	Perplexity <a href="https://huggingface.co/perplexity-ai/r1-1776">	DeepSeek R1 1776	</a>

<li/>	<a href="https://replicate.com/explore">	Replicate	</a>
	<a href="https://github.com/replicate/comfyui-replicate/blob/main/supported_models.json">	supported models	</a>
	<a href="https://github.com/replicate/comfyui-replicate/tree/main">	comfyui-replicate	</a>

<li/>	<a href="https://www.runcomfy.com/">	RunComfy	</a>
<li/>	<a href="https://www.runninghub.ai/workflows">	RunningHub	</a>
<li/>	<a href="https://blog.runpod.io/how-to-get-stable-diffusion-set-up-with-comfyui-on-runpod/">	runpod.ai	</a>

<li/>	<a href="https://www.segmind.com/pixelflow/templates">	SegMind - PixelFlow	</a>
<li/>	<a href="https://www.shakker.ai/online-comfyui">	Shakker.ai	</a>
<li/>	<a href="https://sinkin.ai/?models=1">	sinkin	</a>

<li/>	<a href="https://tensor.art/">	tensor.art	</a>
<li/>	<a href="https://www.thinkdiffusion.com/#apps-comfyui">	ThinkDiffusion	</a>,
	<a href="https://civitai.com/articles/3244/comfyui-workflows-and-what-you-need-to-know-by-thinkdiffusion">	thinkdiffusion	</a>
<li/>	<a href="https://vast.ai/article/getting-started-with-comfy-UI">	vast.ai	</a>

<li/>	<a href="https://magicquill.art/demo/">	MagicQuill	</a>,
	<a href="https://github.com/magic-quill">	GitHub - MagicQuill	</a>,
	<a href="https://huggingface.co/spaces/AI4Editing/MagicQuill">	Demo	</a>,
	<a href="https://github.com/magic-quill/ComfyUI_MagicQuill/">	ComfyUI_MagicQuill	</a>

<li/>	<a href="https://github.com/showlab/PhotoDoodle">	PhotoDoodle	</a>, 
	<a href="https://github.com/smthemex/ComfyUI_PhotoDoodle">	smthemex/ComfyUI_PhotoDoodle	</a>, 
	<a href="https://huggingface.co/spaces/ameerazam08/PhotoDoodle-Image-Edit-GPU">	HuggingFace - PhotoDoodle-Image-Edit-GPU </a>

<li/> <mark><a href="https://civitai.com/user/UmeAiRT/models">		CivitAI - user - UmeAiRT (ComfyUI workflow 101)	</a></mark>

</ol>
</td><td width="30%"><!----------------------------------------------------------->
<h3>	Benchmarks & Leaderboards		</h3>
<li/>	Keywords: <a href="https://www.inoreader.com/stream/user/1005506540/tag/Benchmarking%20%E2%9C%85/view/html?cs=m">	InoReader - Benchmarking	</a>,
	<br/><ol start=1 type=1>

<li/>	<a href="https://www.confident-ai.com/blog/the-current-state-of-benchmarking-llms">	Intro to LLM Benchmarking	</a>

<li/>	AllenAI
	<a href="https://huggingface.co/spaces/allenai/reward-bench">	RewardBench: Evaluating Reward Models	</a>, 
	<a href="https://huggingface.co/spaces/allenai/ZebraLogic">	ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning	</a>, 
	<a href="https://huggingface.co/spaces/allenai/super_leaderboard">	SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories	</a>, 
	<a href="https://huggingface.co/spaces/allenai/ZeroEval">	ZeroEval: Benchmarking LLMs for Reasoning	</a>, 
	<a href="https://huggingface.co/spaces/allenai/WildBench">	WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild	</a>, 

<li/>	Audio	<a href="https://github.com/MoonshotAI/Kimi-Audio-Evalkit/blob/master/LEADERBOARD.md">	Audio - Kimi-Audio-Evalkit	</a>

<li/>	<a href="https://huggingface.co/spaces/ArtificialAnalysis/LLM-Performance-Leaderboard">	LLM-Performance-Leaderboard	</a>
<li/>	<a href="https://huggingface.co/spaces/ArtificialAnalysis/Text-to-Image-Leaderboard">	Text-to-Image-Leaderboard	</a>
<li/>	<a href="https://huggingface.co/spaces/ArtificialAnalysis/Video-Generation-Arena-Leaderboard">		Video-Generation-Arena-Leaderboard	</a>

<li/>	<a href="https://huggingface.co/spaces/galileo-ai/agent-leaderboard">	Agent Leaderboard	</a>
	
<li/>	<a href="https://huggingface.co/spaces/gaia-benchmark/leaderboard">	GAIA General Agent Leaderboard (e.g. Manus)	</a>
	<a href="https://openreview.net/forum?id=fibxvahvs3">	GAIA: a benchmark for General AI Assistants	</a>

<li/>	<a href="https://huggingface.co/spaces/philschmid/llm-pricing">	LLM Pricing	</a>

<li/>	<a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena">	Text-To-Speech - TTS-AGI/TTS-Arena	</a>
<li/>	<a href="https://huggingface.co/spaces/Vchitect/VBench_Leaderboard">	Text-To-Video - Vchitect/VBench_Leaderboard	</a>
<li/>	<a href="https://huggingface.co/datasets/saiyan-world/Goku-MovieGenBench">	Video - MovieGenBench	</a>

<li/>	<a href="https://arena.hume.ai/">	Speech - Hume - Expressive TTS Arena	</a>

<li/>	<a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena">	Speech - TTS-Arena	</a>
<li/>	<a href="https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/summarization-eval">		Text - Arize Phoenix </a>
<li/>	<a href="https://klu.ai/glossary/llm-evaluation">	Text – klu Evaluation Guide	</a>
<li/>	<a href="https://github.com/confident-ai/deepeval">	Text - DeepEval	</a>	
 
<li/>	<a href="https://ossinsight.io/collections/stable-diffusion-ecosystem/">	Stable Diffusion Ecosystem	</a>
 

<li/>	<a href="https://mmsearch.github.io/">	MMSearch	</a>
<li/>	<a href="https://huggingface.co/spaces/Vchitect/VBench_Leaderboard">	VBench : Comprehensive Benchmark Suite for Video Generative Models	</a>


<li/>	<a href="https://matrix.tencent.com/ai-detect/ai_gen">	Tencent ai-detect	</a>
<li/>	<a href="https://hivemoderation.com/ai-generated-content-detection">	Hive ai-generated-content-detection	</a>


</ol>
<!----------------------------------------------------------->
<h3>	Face Restoration / Realistic Faces			</h3>
<li/>	Keywords: <a href="https://openart.ai/workflows/all?keyword=FaceSwap">	OpenArt - FaceSwap	</a>,
	<a href="https://openart.ai/workflows/all?keyword=Realistic">	OpenArt - Realistic	</a>,
	<a href="https://www.1ai.net/en/tag/ai%e6%8d%a2%e8%84%b8">	1ai	</a>
	<br/><ol start=1 type=1>

<li/>	<a href="https://medium.com/design-bootcamp/ai-face-swap-battle-pulid-vs-instantid-vs-faceid-2f08db230509">	Comapre PuLID vs InstantID vs FaceID	</a>

<li/> <a href="https://github.com/Gourieff/comfyui-reactor-node">		GitHub - Gourieff (ReActor Node for ComfyUI)	</a>,
	<a href="https://github.com/somanchiu/ReSwapper">	somanchiu/ReSwapper	</a>,
	<a href="https://civitai.com/models/256399/face-swap-for-2-people-with-faceid-and-reactor">		faceswap	</a>

<li/>	<a href="https://openart.ai/workflows/grinlau/change-face-v23/4RJFsFp9oXdlP9rty2cI">	OpenArt - grinlau/change-face-v23	</a>

<li/>	<a href="https://huggingface.co/GuijiAI/ReHiFace-S">	HuggingFace - GuijiAI/ReHiFace-S	</a>

<li/>	<a href="https://openart.ai/workflows/myaiforce/face-swapping-ecomid-vs-flux-pulid-vs-instantid/2ATyK62dutoPVCevX8o5">	OpenArt - myaiforce/face-swapping-ecomid-vs-flux-pulid-vs-instantid	</a>

<li/>	<a href="https://github.com/sipie800/ComfyUI-PuLID-Flux-Enhanced">	GitHub - sipie800/ComfyUI-PuLID-Flux-Enhanced	</a>

<li/>	<a href="https://openart.ai/workflows/JakazCfWF9UWi2mOZTlf">	OpenArt - Andrea Baioni - Plastic Skin Solver 	</a>,
	<a href="https://www.youtube.com/watch?v=Ta8GpvgmJMo">	YouTube	</a>

<li/>	<a href="https://github.com/cubiq/ComfyUI_FaceAnalysis">	GitHub - cubiq/ComfyUI_FaceAnalysis	</a>,
	<a href="https://github.com/jordoh/ComfyUI-Deepface/">	GitHub - jordoh/ComfyUI-Deepface/	</a>

<li/>	<a href="https://github.com/djbielejeski/a-person-mask-generator">	GitHub - Person Mask Generator	</a>

<li/> <a href="https://huggingface.co/alexgenovese/facerestore/tree/main">	alexgenovese/facerestore	</a>
<li/>	<a href="https://github.com/modelscope/facechain">	modelscope/facechain	</a>

<li/>	Face Restoration <a href="https://github.com/Hillobar/Rope">	Pearl Rope	</a>,
	<a href="https://github.com/s0md3v/roop">	Roop </a>,
	<a href="https://github.com/iperov/DeepFaceLive">	DeepFaceLive	</a>,
	<a href="https://github.com/neuralchen/SimSwap">	SimSwap	</a>,
	<a href="https://github.com/deepfakes/faceswap">	deepfakes/faceswap	</a>

<li/>	<a href="https://huggingface.co/facefusion">	FaceFusion	</a>,
	<a href="https://huggingface.co/datasets/dimanchkek/Deepfacelive-DFM-Models">	Deepfacelive-DFM-Models	</a>

<li/>	<a href="https://github.com/Gourieff/comfyui-reactor-node">	Gourieff - comfyui-reactor-node	</a>

<li/>	<a href="https://github.com/djbielejeski/a-person-mask-generator">	GitHub - Person Mask Generator	</a>

<li/>	<a href="https://superhero-7.github.io/DreamID/">	DreamID - A Fast and High-Fidelity diffusion-based Face Swapping via Triplet ID Group Learning	</a>


</ol>
<!----------------------------------------------------------->
<h3>	Image-To-Text (i2t) Captioning		</h3>
<li/>	Keywords: <a href="https://openart.ai/workflows/all?keyword=Caption">	OpenArt - Caption 	</a><br/><ol start=1 type=1>

<li/>	<a href="https://cychenyue.com/28204.html">	AllenAI Molmo 7B D		</a>
<li/>	<a href="https://github.com/StartHua/Comfyui_CXH_joy_caption">	Joy Caption	</a>
	<a href="https://openart.ai/workflows/leeguandong/joy-caption-batch-caption/6btLPOJKoSBFmuM2kfRW">	Joy caption batch caption	</a>

<li/>	<a href="https://huggingface.co/microsoft">	Microsoft 	</a>,
	<a href="https://github.com/kijai/ComfyUI-Florence2">	Microsoft Florence2	</a>,
	<a href="https://github.com/spacepxl/ComfyUI-Florence-2">	Florence-2	</a>,
	<a href="https://huggingface.co/MiaoshouAI">	MiaoshouAI	</a>,
	<a href="https://github.com/miaoshouai/ComfyUI-Miaoshouai-Tagger">	ComfyUI-Miaoshouai-Tagger	</a>,
	<a href="https://openart.ai/workflows/toucan_chilly_4/florence-run-batch-capture-prompt-maker/tmkxPTDT7sL2EuZgaQJe">	OpenArt  - Florence Run Batch Capture Prompt Maker	</a>

<li/> <a href="https://github.com/alexisrolland/ComfyUI-Phi">	 Microsoft Phi - alexisrolland/ComfyUI-Phi (Phi-3.5-mini-instruct, Phi-3.5-vision-instruct)	</a>,
	<a href="https://github.com/StartHua/Comfyui_CXH_Phi_3.5">	Phi 3.5	</a>,

<li/>	<a href="https://github.com/CY-CHENYUE/ComfyUI-MiniCPM-Plus">	MiniCPM-Plus	</a>,
			<a href="https://github.com/pzc163/Comfyui_MiniCPMv2_6-prompt-generator">	MiniCPM v2.6 Prompt Generator	</a>

<li/> <a href="https://moondream.ai/playground">	Moondream (Visual Q&A, Caption, Object Detection) </a>,
	<a href="https://moondream.ai/blog/">		Moondream blog	</a>,
	<a href="https://huggingface.co/vikhyatk/moondream2">		vikhyatk/moondream2	</a>,
	<a href="https://github.com/vikhyat/moondream">		vikhyat/moondream	</a>,
	<a href="https://github.com/kijai/ComfyUI-moondream">	kijai/ComfyUI-moondream </a>,
	<a href="https://github.com/Hangover3832/ComfyUI-Hangover-Moondream">	Hangover3832/ComfyUI-Hangover-Moondream </a>

<li/>	<a href="https://nexa.ai/blogs/OmniVLM">	OmniVLM-968M (no ComfyUI)	</a>
<li/>	<a href="https://github.com/SeanScripts/ComfyUI-PixtralLlamaMolmoVision">	Pixtral Llama Molmo Vision	</a>

<li/>	<a href="https://github.com/ZHO-ZHO-ZHO/ComfyUI-Qwen">	Qwen 2.5	</a>,

<li/> <a href="https://github.com/IuvenisSapiens/ComfyUI_Qwen2-VL-Instruct">		Qwen2-VL-Instruct	</a>,
	<a href="https://openart.ai/workflows/leeguandong/qwen2-vlchat_with_multiple_images/qTFHeJbzMRcwYJKK4Qwb">	QWEN multiple images	</a>,
	<a href="https://openart.ai/workflows/leeguandong/qwen2-vl-chat_with_single_image/AEgiUMjzw9H580CXgrXD">	QWEN single_image 	</a>,
	<a href="https://openart.ai/workflows/comfyuiblog/convert-video-and-images-to-text-using-qwen2-vl-model/OdlbuCnxVfjgD4MVsaMR">	vi2t	</a>

<li/>	<a href="https://github.com/SeargeDP/ComfyUI_Searge_LLM">	Searge-LLM </a>
<li/>	<a href="https://github.com/pythongosssss/ComfyUI-WD14-Tagger">	WD14-Tagger	</a>

<li/>	<a href="https://huggingface.co/spaces/gokaygokay/FLUX-Prompt-Generator">	gokaygokay/Flux Prompt Generator	 </a>,
	<a href="https://huggingface.co/spaces/gokaygokay/Flux-Florence-2">	Flux-Florence-2	 </a>,
	<a href="https://github.com/fairy-root/Flux-Prompt-Generator/tree/main">	fairy-root	</a>,

<li/> <a href="https://github.com/IuvenisSapiens">		IuvenisSapiens (miniCPM, QWEN, QWEN Audio)	</a>

<li/>	<a href="https://open.bigmodel.cn/">	Zhipu GLM	</a>,
	<a href="https://github.com/JcandZero/ComfyUI_GLM4Node">	GitHub - JcandZero/ComfyUI_GLM4Node </a>,
	<a href="https://github.com/Nojahhh/ComfyUI_GLM4_Wrapper">	GitHub - Nojahhh/ComfyUI_GLM4_Wrapper </a>,

<li/>	Workflows : <a href="https://openart.ai/workflows/cxh/recommended-based-on-comfyui-node-picturesjoy_caption-minicpmv2_6-prompt-generator-florence2/rUVWkZB5zGOkjXGhsrON">	joy_caption-minicpmv2_6-prompt-generator-florence2	</a>
	<a href="https://openart.ai/workflows/owl_glaring_95/flux_img2img-flux-florence-auto-prompt-generator/3aLDgY2oScWNSDZUvNVn">	florence	</a>


<li/>	<a href="https://openart.ai/workflows/comfyui_llm_party/llm_party-for-local-models/UkynnzaQxhuSwPDTFub7">	llm_party-for-local-models (QWEN)	</a>



</ol>

<!----------------------------------------------------------->
<h3>	Models			</h3>
<li/>	Keywords: <a href="https://huggingface.co/models?pipeline_tag=text-generation&sort=trending">	HuggingFace - text-generation	</a>,
	<a href="https://www.inoreader.com/stream/user/1005506540/tag/GenAI%20-%20Algorithm%20%2B%20Model%20%E2%9C%85/view/html?cs=m">	InoReader - Algorithm	</a>,
	<br/><ol start=1 type=1>

<li/> <a href="https://www.aimodels.fyi/models">	AIModels.fyi	</a>

<li/> <a href="https://huggingface.co/Comfy-Org/models">	Comfy-Org	</a>

<li/> <a href="https://huggingface.co/models">	HuggingFace	</a>
	
<li/> <a href="https://modelscope.cn/models">	ModelScope	</a>

<li/>	AlexGeNovese <a href="https://huggingface.co/alexgenovese/checkpoint/tree/main">	checkpoint	</a>, 
	<a href="https://huggingface.co/alexgenovese/clip/tree/main">		clip	</a>, 
	<a href="https://huggingface.co/alexgenovese/clip_vision/tree/main">	clip_vision	</a>, 
	<a href="https://huggingface.co/alexgenovese/controlnet/tree/main">	controlnet	</a>,
	<a href="https://huggingface.co/alexgenovese/facerestore/tree/main">	facerestore	</a>, 
	<a href="https://huggingface.co/alexgenovese/ipadapters/tree/main">	ipadapters	</a>, 
	<a href="https://huggingface.co/alexgenovese/loras/tree/main">		loras	</a>, 
	<a href="https://huggingface.co/alexgenovese/sams/tree/main">		sams	</a>, 
	<a href="https://huggingface.co/alexgenovese/vae/tree/main">			vae	</a>,
	<a href="https://huggingface.co/alexgenovese/ultralytics/tree/main">	ultralytics	</a>

<li/>	<a href="https://huggingface.co/models?pipeline_tag=image-to-video&p=0&sort=trending">	image-to-video	</a>
<li/>	<a href="https://huggingface.co/impactframes">	impactframes	</a>
<li/>	<a href="https://huggingface.co/jasperai">	jasperai	</a>

<li/>	Phips<a href="https://huggingface.co/Phips">	Phips/upscalers </a>,
	<a href="https://huggingface.co/spaces/Phips/Upscaler">	demo	</a>

<li/>	<a href="https://github.com/IamCreateAI/Ruyi-Models">	IamCreateAI/Ruyi	</a>,
	<a href="https://openart.ai/workflows/leeguandong/ruyi-text2img/unHmvTTdC0QQoGUyWImz">	OpenArt - leeguandong/ruyi-text2img	</a>

<li/>	<a href="https://chenglin-yang.github.io/1.58bit.flux.github.io/">	ByteDance - 1.58-bit FLUX	</a>

<li/>	<a href="https://huggingface.co/HFforLegal">	Hugging Face for Legal	</a>,
	<a href="https://huggingface.co/datasets/HFforLegal/laws">	HFforLegal/datasets	</a>,

<li/>	<a href="https://github.com/cubiq/ComfyUI_IPAdapter_plus">	IPAdapter (FaceID, clip-vision, LORA)	</a>



<li/>	<a href="https://huggingface.co/TheBloke">	TheBloke (>4K)	</a>

<li/>	Reaslim <a href="https://tensor.art/models/808406540089546518/Extra-Realistic-Flux-v1">	TensorArt - Extra-Realistic-Flux	</a>,
	<a href="https://tensor.art/u/614170213247779888/models">	TensorArt - kg_09	</a>

<li/>	StrangerZone <a href="https://huggingface.co/strangerzonehf">	StrangerZone LORA (Flux-Super-Realism-LoRA, Super 3D - Engine) 	</a>

<li/>	Unsloth 
	<a href="https://unsloth.ai/blog">	Unsloth.ai </a>, 
	<a href="https://huggingface.co/unsloth">	UnSloth (>300)	</a>, 
	<a href="https://github.com/unslothai/unsloth">	GitHub - UnSloth AI	</a>, 
	<a href="https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c">	unsloth/deepseek-v3	</a>,
	<a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa">	phi-4-all-versions	</a>, 

<li/>	<a href="https://huggingface.co/collections/mit-han-lab/svdquant-67493c2c2e62a1fc6e93f45c">	SVDQuant	</a>, 
	<a href="https://github.com/mit-han-lab/ComfyUI-nunchaku">	mit-han-lab/ComfyUI-nunchaku	</a>

<li/>	<a href="https://github.com/River-Zhang/ICEdit">	In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer	</a>,
	<a href="https://huggingface.co/spaces/RiverZ/ICEdit">	spaces/RiverZ/ICEdit	</a>,

</ol>
<!----------------------------------------------------------->
<h3>	Motion - Animation / LipSync			</h3>
<li/>	Keywords: <a href="https://openart.ai/workflows/all?keyword=FaceSwap">	OpenArt - FaceSwap	</a>,
	<br/><ol start=1 type=1>

<li/> <a href="https://github.com/kijai/ComfyUI-LivePortraitKJ">	GitHub - ComfyUI-LivePortraitKJ	</a>,
	<a href="https://openart.ai/workflows/datou/live-portrait/PMM4NSLnbhU08xvRqN2e">	OpenArt - datou/live-portrait	</a>,
	<a href="https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait">	GitHub - ComfyUI-AdvancedLivePortrait	</a>,
	<a href="https://www.youtube.com/watch?v=xGJ82MVOTcU">	YouTube	</a>

<li/>	MimicMotion <a href="https://github.com/kijai/ComfyUI-MimicMotionWrapper">	GitHub - ComfyUI-MimicMotionWrapper	</a>
	<a href="https://github.com/Tencent/MimicMotion">	MimicMotion	</a>,
	<a href="https://github.com/TMElyralab/MusePose">	MusePose	</a>,

<li/>	UniAnimate / Animate-X	<a href="https://lucaria-academy.github.io/Animate-X/">	Animate-X	</a>,"
	<a href="https://unianimate.github.io/">		UniAnimate	</a>, 
	<a href="https://github.com/ali-vilab/UniAnimate">		ali-vilab/UniAnimate	</a>, 
	<a href="https://github.com/Isi-dev/ComfyUI-UniAnimate-W">	Isi-dev/ComfyUI-UniAnimate-W (UniAnimate=humans, Animate-X=animals/cartoons)	</a>,
	<a href="https://huggingface.co/Isi99999/UniAnimate_and_Animate-X_Models/tree/main">		UniAnimate/Animate-X models	</a>, 

<li/>	<a href="https://github.com/SamKhoze/ComfyUI-DeepFuze">	GitHub - DeepFuze	</a>,
	<a href="https://www.youtube.com/watch?v=elQzQo__kWI">	YouTube 	</a>, Facial transformations, lipsyncing, video generation, voice cloning, face swapping, and lipsync translation

<li/>	<a href="https://jixiaozhong.github.io/Sonic/">	Sonic: Shifting Focus to Global Audio Perception in Portrait Animation	</a>, 
	<a href="https://huggingface.co/spaces/xiaozhongji/Sonic">	xiaozhongji/Sonic Demo	</a>
	<a href="https://github.com/smthemex/ComfyUI_Sonic">	smthemex/ComfyUI_Sonic	</a>


<li/>	<a href="https://humanaigc.github.io/animate-anyone-2/">	animate-anyone-2 - High-Fidelity Character Image Animation with Environment Affordance	</a>, 
	<a href="https://github.com/HumanAIGC/AnimateAnyone">	AnimateAnyone </a>

<li/>	<mark><a href="https://lingtengqiu.github.io/LHM/">	LHM: Large Animatable Human Reconstruction Model for Single Image to 3D in Seconds	</a></mark>, 
	<a href="https://huggingface.co/spaces/DyrusQZ/LHM">	spaces/DyrusQZ/LHM demo	</a>, 
	<a href="https://github.com/aigc3d/LHM/tree/feat/comfyui">	LHM ComfyUI	</a>

<li/>	<a href="https://github.com/menyifang/MIMO">	MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling	</a>

<li/>	KwaiVGI <a href="https://github.com/KwaiVGI/ReCamMaster">	ReCamMaster: Camera-Controlled Generative Rendering from A Single Video 	</a>, 
	<a href="https://jianhongbai.github.io/ReCamMaster/">	jianhongbai.github.io/ReCamMaster	</a>, 
	<a href="https://arxiv.org/abs/2503.11647">	Paper	</a>, 
	<mark><a href="https://github.com/kijai/ComfyUI-WanVideoWrapper">	kijai/ComfyUI-WanVideoWrapper	</a>, </mark>

<li/>	<a href="https://bytedance.github.io/InfiniteYou/">	InfiniteYou - Flexible Photo Recrafting While Preserving Your Identity	</a>,
	<a href="https://arxiv.org/html/2503.16418v1">	Paper </a>,
	<a href="https://github.com/bytedance/InfiniteYou">	bytedance/InfiniteYou	</a>,
	<a href="https://huggingface.co/spaces/ByteDance/InfiniteYou-FLUX">	InfiniteYou-FLUX demo	</a>, 
	<a href="https://openart.ai/workflows/t8star/infiniteyou/0BGujVa1yfZzispihLjU">	t8star/infiniteyou 	</a>, 
	<a href="https://openart.ai/workflows/bulldog_fruitful_46/flux_infiniteyou/OXicXXXSB1V4KIF0wXdz">	bulldog_fruitful_46/flux_infiniteyou	</a>, 
	<a href="https://github.com/ZenAI-Vietnam/ComfyUI_InfiniteYou">	ZenAI-Vietnam/ComfyUI_InfiniteYou 	</a>



<li/>	<a href="https://ewrfcas.github.io/Uni3C/">	Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation(Camera + Video Motion) 	</a>


</ol>
</td><td width="30%"><!----------------------------------------------------------->
<h3>	Object Background Remover / Segmentation / InPaint / OutPaint		</h3>
<li/>	Keywords: <a href="https://openart.ai/workflows/all?keyword=Segment">	OpenArt - Segment </a>,
	<a href="https://openart.ai/workflows/all?keyword=Background">	OpenArt - Background</a><br/><ol start=1 type=1>

<li/>	<a href="https://github.com/john-mnz/ComfyUI-Inspyrenet-Rembg">	GitHub - Inspyrenet-Rembg	</a>
<li/>	<a href="https://github.com/1038lab/ComfyUI-RMBG">	GitHub - RMBG (BEN2, mask feather, dino object segmentation)	</a>,
	<a href="https://openart.ai/workflows/ailab/comfyui-rmbg-v120-rmbg-20-inspyrenet-and-ben-precision-background-removal/FyG6ZL1q3Z8zjSB3GPxY">	OpenArt - rmbg-v120-rmbg-20-inspyrenet	</a>, 
	<a href="https://github.com/PramaLLC/BEN2_ComfyUI">	PramaLLC/BEN2_ComfyUI	</a>

<li/> <a href="https://huggingface.co/briaai/RMBG-2.0">	BRIA Background Removal v2.0	</a>

<li/> <a href="https://github.com/plemeri/InSPyReNet">	Image Pyramid Structure for High Resolution Salient Object Detection (InSPyReNet)	</a>

<li/> <a href="https://github.com/ZhengPeng7/BiRefNet">	Bilateral Reference for High-Resolution Dichotomous Image Segmentation (BiRefNet)	</a>,
	<a href="https://github.com/Visionatrix/ComfyUI-BiRefNet">	ComfyUI-BiRefNet	</a>,
	<a href="https://github.com/MoonHugo/ComfyUI-BiRefNet-Hugo">	MoonHugo/ComfyUI-BiRefNet-Hugo	</a>

<li/> <a href="https://huggingface.co/PramaLLC/BEN">		Background Erase Network (BEN)	</a>
	<a href="https://civitai.com/articles/8845/ben-background-erase-network-in-comfyui">		CivitAI ComfyUI </a>

<li/>	<a href="https://github.com/Layer-norm/comfyui-lama-remover">	lama-remover	</a>,
	<a href="https://civitai.com/articles/6255/using-comfy-to-batch-process-images-with-lama-cleaner">	batch-process-images-with-lama-cleaner	</a>

<li/>	<a href="https://openart.ai/workflows/fish_intent_33/removeanything/guex9gtyTcSQMslwjvl8">	OpenArt - fish_intent_33/removeanything	</a>
	<a href="https://huggingface.co/blog/OzzyGT/diffusers-image-fill">	Diffusers Image Fill Guide	</a>

<li/>	<a href="https://openart.ai/workflows/emperor_rare_28/object-removal-workflow/346g2A7esW3lde8KZH5F">	OpenArt - emperor_rare_28/object-removal-workflow	</a>


<li/>	<a href="https://github.com/scraed/LanPaint">	Lanpaint: Training-Free Diffusion Inpainting with Exact and Fast Conditional Inference	</a>,
	<a href="https://openart.ai/workflows/makisekurisu/flux-lanpaint-training-free-inpainting/Tpdv6Wor3IOGIs1jPfXu">	makisekurisu/flux-lanpaint-training-free-inpainting	</a>

<li/> SAM2(Segment Anything 2) <a href="https://github.com/kijai/ComfyUI-segment-anything-2">	ComfyUI-segment-anything-2	</a>,
	<a href="https://github.com/neverbiasu/ComfyUI-SAM2">	ComfyUI-SAM2	</a>,
	<a href="https://openart.ai/workflows/cgtips/comfyui---segment-anything-2-sam2---method-2/GTbSSKbVl6KpGu6uh4Lj">	OpenArt - SAM2	</a>,
	<a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">	IDEA-Research/Grounded-Segment-Anything	</a>

<li/>	<a href="https://mega-sam.github.io/#demo">	MegaSAM - Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos	</a>

<li/>	<a href="https://github.com/magic-research/Sa2VA">	magic-research/Sa2VA	</a>, 
	<a href="https://huggingface.co/spaces/fffiloni/Sa2VA-simple-demo">	Sa2VA-simple-demo	</a>

<li/>	<a href="https://pq-yang.github.io/projects/MatAnyone/">	MatAnyone (NTU, SenseTime)	</a>,
	<a href="https://huggingface.co/spaces/PeiqingYang/MatAnyone">	HuggingFace demo	</a>

<li/>	<a href="https://github.com/roboflow/rf-detr">	RF-DETR: SOTA Real-Time Object Detection Model	</a>, 
	<a href="https://huggingface.co/spaces/SkalskiP/RF-DETR">	RF-DETR demo	</a>

<li/>	<a href="https://teleport.varjo.com/">	Vargo Teleport - 3D from iPhone Video	</a>


</ol>
<!----------------------------------------------------------->
<h3>	Speech / Music - Platform 			</h3>
<ol start=1 type=1>

<li/> Platform - ElevenLabs  <a href="https://elevenlabs.io/voice-library/angry-voices">	voice-library/angry-voices </a>
<li/> Platform - Hume.AI	<a href="https://www.hume.ai/text-to-speech">		LLM for text-to-speech	</a>,
<li/> Platform - Play.HT	<a href="https://play.ht/text-to-speech/singaporean-english/#samples">		Play.HT - singaporean-english	</a>,
	<a href="https://app.play.ht/api/sandbox">	play.ht sandbox	</a>
<li/>	Platform - Resemble <a href="https://www.resemble.ai/">	resemble.ai (Fake Audio Detection)	</a>

<li/>	<a href="https://github.com/abdozmantar/ComfyUI-DeepExtract">	ComfyUI-DeepExtract - separate vocals and sounds from audio files	</a>

<li/>	<a href="https://www.riffusion.com/?filter=staff-picks">	Riffusion (platform)	</a>

<li/>	<a href="https://github.com/ASLP-lab/DiffRhythm">	DiRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion	</a>


</ol>
<!----------------------------------------------------------->
<h3>	Speech - Music 			</h3>
<li/>	Keywords: 
	<a href="https://openart.ai/workflows/all?keyword=Audio">	OpenArt - Audio	</a>,
	<a href="https://gist.github.com/0xdevalias/359f4265adf03b0142e4d0543c156a3e">	github - Awesome Audio	</a>
	<br/><ol start=1 type=1>

<li/>	<a href="https://github.com/hkchengrex/MMAudio">	hkchengrex/MMAudio	</a>,
	<a href="https://github.com/kijai/ComfyUI-MMAudio">	kijai/ComfyUI-MMAudio	</a>,
	<a href="https://openart.ai/workflows/sneakyrobot/mmaudio-video-to-sound-audio-visual-workflow/zHLVMpwqHzqR9yHLzW06">		OpenArt - sneakyrobot/mmaudio-video-to-sound-audio-visual-workflow	</a>

<li/>	Music <a href="https://map-yue.github.io/">	YuE: Open Music Foundation Models for Full-Song Generation	</a>


<li/>	<a href="https://github.com/billwuhao/ComfyUI_DiffRhythm">	billwuhao/ComfyUI_DiffRhythm	</a>
<li/>	<a href="https://ace-step.github.io/">	ACE-Step: A Step Towards Music Generation Foundation Model	</a>, 
	<a href="https://github.com/ace-step/ACE-Step">	ace-step/ACE-Step	</a>


</ol>
<!----------------------------------------------------------->
<h3>	Speech - Text-2-Speech (TTS) 			</h3>
<li/>	Keywords: 
	<a href="https://huggingface.co/papers?q=Speech%20Token">	HuggingFace - Speech	</a>,
	<a href="https://openart.ai/workflows/all?keyword=Audio">	OpenArt - Audio	</a>,
	<a href="https://gist.github.com/0xdevalias/359f4265adf03b0142e4d0543c156a3e">	github - Awesome Audio	</a>
	<br/><ol start=1 type=1>



<li/>	CanopyLabs <a href="https://huggingface.co/canopylabs">	Llama-based Speech-LLM designed for high-quality, empathetic text-to-speech generation	</a>,
	<a href="https://huggingface.co/canopylabs/orpheus-3b-0.1-ft">	canopylabs/orpheus TTS (emotion, training mesopolitica)	</a>, 
	<a href="https://github.com/ShmuelRonen/ComfyUI-Orpheus-TTS">	ShmuelRonen/ComfyUI-Orpheus-TTS	</a> 

<li/>	CosyVoice <a href="https://funaudiollm.github.io/cosyvoice2/">	CosyVoice 2	</a>,
	<a href="https://openart.ai/workflows/t8star/cosyvoice2latentsync/sW87AX7TU9Lq6t4uHySn">	t8star/cosyvoice2latentsync 	</a>
	<a href="https://github.com/muxueChen/ComfyUI_NTCosyVoice">	muxueChen/ComfyUI_NTCosyVoice	</a>
	<a href="https://github.com/touge/ComfyUI-NCE_CosyVoice">	touge/ComfyUI-NCE_CosyVoice	</a>

<li/>	Data 
	<a href="AI Audio Datasets (AI-ADS)">	AI Audio Datasets (AI-ADS)	</a>, 
	<a href="https://www.openslr.org/82/">		CN-Celeb1, CN-Celeb2	</a>, 

<li/> FishAudio 	<a href="https://github.com/fishaudio/fish-speech">	FishAudio (No ComfyUI)	</a>
	<a href="https://huggingface.co/fishaudio">	huggingface.co/fishaudio	</a>

<li/>	<a href="https://github.com/IuvenisSapiens/ComfyUI_Qwen2-Audio-7B-Instruct-Int4">	Qwen2-Audio-7B-Instruct-Int4	</a>

<li/> TTS - DeepGram <a href="https://playground.deepgram.com/?endpoint=listen&smart_format=true&language=en&model=nova-3">	TTS playground </a>, 
	<a href="https://developers.deepgram.com/docs/text-to-speech-prompting">	text-to-speech-prompting </a>

<li/>	TTS - F5-TTS <a href="https://huggingface.co/SWivid/F5-TTS">		HuggingFace - SWivid/F5-TTS	</a>,
	<a href="https://github.com/niknah/ComfyUI-F5-TTS">	niknah/ComfyUI-F5-TTS	</a>,
	<a href="https://huggingface.co/erax-ai/EraX-Smile-Female-F5-V1.0">	 erax-ai (vietnamese)	</a>,

<li/>	TTS - FreeVC <a href="https://olawod.github.io/FreeVC-demo/">	Github - FreeVC - One-Shot Voice Conversion 	</a>, 
	<a href="https://github.com/ShmuelRonen/ComfyUI-FreeVC_wrapper">	ShmuelRonen/ComfyUI-FreeVC_wrapper	</a>

<li/>	TTS - Kokoro <a href="https://huggingface.co/spaces/ysharma/Make_Custom_Voices_With_KokoroTTS">	Voice Mixer Studio	</a>, 
	<a href="https://github.com/MushroomFleet/DJZ-KokoroTTS">	MushroomFleet/DJZ-KokoroTTS	</a>, 
	<a href="https://huggingface.co/spaces/ysharma/Make_Custom_Voices_With_KokoroTTS"> Make_Custom_Voices_With_KokoroTTS demo	</a>

<li/> TTS = Llasa-3B <a href="https://huggingface.co/spaces/srinivasbilla/llasa-3b-tts">	Llasa-3B	</a>, 
	<a href="https://huggingface.co/HKUSTAudio/Llasa-3B">	HKUSTAudio/Llasa-3B	</a>,
	<a href="https://replicate.com/kjjk10/llasa-3b-long">	Replicate - kjjk10/llasa-3b-long	</a>

<li/> TTS - MegaTTS3 	<a href="https://github.com/bytedance/MegaTTS3">	bytedance - MegaTTS3 (voice cloning)	</a>, 
	<a href="https://huggingface.co/spaces/ByteDance/MegaTTS3">	MegaTTS3 Demo	</a>, 
	<a href="https://openart.ai/workflows/ailab/comfyui-megatts/K0urCR510JCn2FAcYxoW">	ailab/comfyui-megatts	</a>

<li/>	TTS - Microsoft EdgeTTS 
	<a href="https://huggingface.co/spaces/innoai/Edge-TTS-Text-to-Speech">	Microsoft Edge-TTS-Text-to-Speech	</a>
	<a href="https://github.com/1038lab/ComfyUI-EdgeTTS">	1038lab/ComfyUI-EdgeTTS	</a>,

<li/>	TTS - Nari-Labs <a href="https://github.com/nari-labs/dia">	DIA	</a>

<li/>	TTS - Sesame <a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice">	Sesame - Crossing the uncanny valley of conversational voice	</a>, 
	<a href="https://levelup.gitconnected.com/sesame-csm-1b-for-multi-speaker-ai-conversations-complete-guide-to-installing-and-running-e76b202e5b91">	Sesame CSM 1B for Multi-Speaker AI Conversations	</a>,
	<a href="https://github.com/SesameAILabs/csm">	SesameAILabs/csm	</a>, 
	<a href="https://huggingface.co/sesame/csm-1b">	Sesame - CSM (Conversational Speech Model) 	</a>,
	<a href="https://github.com/billwuhao/ComfyUI_CSM">	billwuhao/ComfyUI_CSM	</a>, 
	<a href="https://blog.speechmatics.com/sesame-finetune">	SpeechMatics - How to Finetune Sesame AI's Speech Model on New Languages and Voices	</a>, 
	<a href="https://github.com/knottwill/sesame-finetune">	SpeechMatics - knottwill/sesame-finetune </a>


<li/>	TTS - SparkTTS 
	<a href="https://sparkaudio.github.io/spark-tts/">	An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens	</a>
	<mark><a href="https://github.com/billwuhao/ComfyUI_SparkTTS">	billwuhao/ComfyUI_SparkTTS	</a> </mark>, 
	<a href="https://huggingface.co/spaces/Mobvoi/Offical-Spark-TTS">	SparkTTS Demo </a>, 
	<a href="https://github.com/1038lab/ComfyUI-SparkTTS">	1038lab/ComfyUI-SparkTTS	</a>, 
	<a href="https://openart.ai/workflows/ailab/comfyui-sparktts-advanced-text-to-speech-for-comfyui/3WhEqE9ejVinv1RLG3DI">	comfyui-sparktts-advanced-text-to-speech-for-comfyui/3WhEqE9ejVinv1RLG3DI	</a>, 
	<a href="https://github.com/tuanh123789/Spark-TTS-finetune">	Spark-TTS-finetune	</a>, 
	<a href="https://github.com/SparkAudio/Spark-TTS">	SparkAudio/Spark-TTS (NTU)	</a>

<li/>	TTS - Seed-VC	
	<a href="https://plachtaa.github.io/seed-vc/">	Zero Shot Voice Conversion	</a>, 
	<a href="https://github.com/Plachtaa/seed-vc">	Plachtaa/seed-vc	</a>

<li/>	TTS - StepAudioTTS  <a href="https://github.com/billwuhao/ComfyUI_StepAudioTTS ">	billwuhao/ComfyUI_StepAudioTTS 	</a> 

<li/>	TTS - Zonos <a href="https://www.zyphra.com/post/beta-release-of-zonos-v0-1">	Zyphra	</a>, 
	<a href="https://playground.zyphra.com/">	Zyphra playground	</a>, 
	<a href="https://www.zyphra.com/post/beta-release-of-zonos-v0-1/">	Zonos v01 Speech TTS	</a>, 

</ol>
<!----------------------------------------------------------->
<h3>	Talking Head			</h3>
<li/>	Keywords: 
	<a href="https://openart.ai/workflows/all?keyword=talking">	OpenArt - Talking	</a>,
	<a href="https://github.com/harlanhong/awesome-talking-head-generation">	harlanhong/awesome-talking-head-generation	</a>,
	<a href="https://github.com/JosephPai/Awesome-Talking-Face">	JosephPai/Awesome-Talking-Face	</a>,
	<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis">	Kedreamix/Awesome-Talking-Head-Synthesis	</a>,
	<a href="https://github.com/Curated-Awesome-Lists/awesome-ai-talking-heads">	awesome-ai-talking-heads	</a>,
	<a href="https://github.com/weihaox/awesome-digital-human">	awesome-digital-human	</a>,

	<a href="https://cvpr.thecvf.com/">	IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)	</a>,
	<a href="https://eccv.ecva.ne">	European Conference on Computer Vision (ECCV)	</a>,
	<a href="https://iccv.thecvf.com/">	International Conference on Computer Vision (ICCV)		</a>
	<a href="https://iclr.cc/">	International Conference on Learning Representations (ICLR)	</a>

	<br/>
	<ol start=1 type=1>

<li/>	<a href="https://github.com/harlanhong/ACTalker">	ACTalker	</a>, 
	<a href="https://harlanhong.github.io/publications/actalker/index.html">	Paper 	</a>

<li/>	<a href="https://github.com/oneThousand1000/AnimPortrait3D">	AnimPortrait3D - text to 3D animation	</a>

<li/>	<a href="https://arxiv.org/abs/2502.20220?ref=uploadvr.com">	Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars	</a>

<li/> <a href="https://character-ai.github.io/avatar-fx/">	Character.AI avatar-fx	</a>

<li/>	<a href="https://github.com/toto222/DICE-Talk">	DICE-Talk - Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation	</a>, 
	<a href="https://github.com/smthemex/ComfyUI_DICE_Talk">	smthemex/ComfyUI_DICE_Talk	</a>

<li/>	<a href="https://arxiv.org/abs/2503.07027">	EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer	</a>, 
	<a href="https://github.com/jax-explorer/ComfyUI-easycontrol">		jax-explorer/ComfyUI-easycontrol </a>, 
	<a href="https://arxiv.org/abs/2503.07027">	Paper	</a>

<li/>	<a href="https://github.com/antgroup/echomimic_v2">		EchoMimic	</a>

<li/>	<a href="https://github.com/deepbrainai-research/float">	FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait	</a>, 
	<a href="https://github.com/yuvraj108c/ComfyUI-FLOAT">	yuvraj108c/ComfyUI-FLOAT	</a>, 
	<a href="https://deepbrainai-research.github.io/float/">	deepbrainai-research.github.io/float	</a>, 

<li/>	<a href="https://fantasy-amap.github.io/fantasy-talking/">		Alibaba - Fantasy Talking	</a>, 
	<a href="https://github.com/kijai/ComfyUI-WanVideoWrapper">	kijai/ComfyUI-WanVideoWrapper	</a>, 
	<a href="https://openart.ai/workflows/pix_studio/fantasy-talking/dy5O3i4qKYaTns7ZgUVF">	pix_studio/fantasy-talking	</a>, 
	<a href="https://www.youtube.com/watch?v=bSssQdqXy9A&t=538s">	YouTube	</a> 

<li/>	<a href="https://lllyasviel.github.io/frame_pack_gitpage/">	FramePack - Packing Input Frame Context in Next-Frame Prediction Models for Video Generation </a>, 
	<a href="https://github.com/lllyasviel/FramePack">	lllyasviel/FramePack	</a>, 
	<a href="https://openart.ai/workflows/pix_studio/framepack6g/kZCYax19o8PgNs25y1Xr">	pix_studio/framepack6g	</a>

<li/>	<a href="https://github.com/HelloVision/ComfyUI_HelloMeme">	HelloMeme	</a>

<li/>	<a href="https://github.com/bytedance/LatentSync">	ByteDance - LatentSync </a>

<li/>	<a href="https://github.com/KwaiVGI/LivePortrait">		LivePortrait	</a>

<li/>	<a href="https://memoavatar.github.io/">	MemoAvatar - MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation	</a>,
	<a href="https://github.com/if-ai/ComfyUI-IF_MemoAvatar">	ComfyUI-IF_MemoAvatar	</a>,
	<a href="https://openart.ai/workflows/dashen/memoavatar/2tsyggrYyZtWeHiyYagV">	OpenArt - dashen/memoavatar	</a>,
	<a href="https://openart.ai/workflows/t8star/memoavatar/yH1rroH1nLA6O0Os4lh3">	OpenArt - t8star/memoavatar	</a>,
	<a href="https://openart.ai/workflows/cat_untimely_42/memoavatar_photo-photo_talk/Dw9ZIG27X017M2DHwkSW">	cat_untimely_42/memoavatar_photo-photo_talk	</a>

<li/>	<a href="https://humanaigc.github.io/omnitalker/">	Alibaba - OmniTalker	</a>

<li/>	<a href="https://real3dportrait.github.io/">	Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis	</a>

<li/>	<a href="https://skyworkai.github.io/skyreels-audio.github.io/">	SkyReels-Audio	</a>

<li/> TaoAvatar <a href="https://pixelai-team.github.io/TaoAvatar/">	pixelai-team.github.io/TaoAvatar	</a>, 
	<a href="https://arxiv.org/abs/2503.17032">	TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting	</a>, 
	<a href="https://medium.com/@nimritakoul01/taoavatar-a-full-body-talking-avatar-for-mobile-devices-using-3d-gaussian-splatting-and-smplx-247e48c42933">	Medium	</a>

<li/>	X-Portrait <a href="https://github.com/akatz-ai/ComfyUI-X-Portrait-Nodes">	GitHub - akatz-ai/ComfyUI-X-Portrait	</a>





</ol>
<!----------------------------------------------------------->
<h3>	Tool / Training / Utility			</h3>
<ol start=1 type=1>

<li/>	<a href="https://education.civitai.com/using-civitai-the-on-site-lora-trainer/">	CivitAI LORA Trainer	</a>,
	<a href="https://education.civitai.com/quickstart-guide-to-flux-1/#rapid-flux-training">	CivitAI FLUX Trainer	</a>, 
	<a href="https://openart.ai/workflows/tenofas/flux-lora-trainer-20/VmxcKxjxRoN2Lrs9ESU7">	tenofas/flux-lora-trainer-20	</a>
	<a href="https://learn.thinkdiffusion.com/building-better-models-flux-loras-in-comfyui/">	ThinkDiffusion - building-better-models-flux-loras-in-comfyui	</a>

<li/> <a href="https://github.com/crystian/ComfyUI-Crystools">		  Crystools (CPU, GPU, RAM, VRAM, GPU Temp and space) 	</a>

<li/>	PixelPruner <a href="https://github.com/theallyprompts/PixelPruner">	theallyprompts	</a>,
	<a href="https://civitai.com/models/465684/pixelpruner-crop-tool-for-data-set-prep">	civitai	</a>

<li/> <a href="https://github.com/liusida/ComfyUI-AutoCropFaces">	GitHub - liusida/ComfyUI-AutoCropFaces	</a>

<li/> <a href="https://github.com/daxcay/ComfyUI-JDCN">		GitHub - JDCN - Directory Path	</a>

<li/>	<a href="https://github.com/MushroomFleet/DJZ-Workflows/tree/main/Foda_Flux/Captioning%20Tools">	Captioning	</a>
<li/>	<a href="https://openart.ai/workflows/toucan_chilly_4/florence-run-batch-capture-prompt-maker/tmkxPTDT7sL2EuZgaQJe">	OpenArt  - Florence Run Batch Capture Prompt Maker	</a>

<li/>	<a href="https://img-comparison-slider.sneas.io/examples.html">	HTML img-comparison-slider	</a>,
	<a href="https://github.com/sneas/img-comparison-slider">	GitHub 	</a>,
	<a href="https://demo.photo.gallery/examples/features/captions/">	demo.photo.gallery	</a>


<li/>	<a href="https://github.com/choey/Comfy-Topaz">	choey/Comfy-Topaz	</a>,
	<a href="https://openart.ai/workflows/t8star/gigapixel/e7l2FXI0D8qn0Bxa71Nb">	t8star/gigapixel	</a>

<li/>	Prompt <a href="https://github.com/marduk191/ComfyUI-Fluxpromptenhancer">	marduk191/ComfyUI-Fluxpromptenhancer	</a>

<li/>	Prompt Lists <a href="https://prompter.fofr.ai/explore">	fofr	</a>,
	<a href="https://github.com/ai-prompts/prompt-lists/">	ai-prompts/prompt-lists	</a>

<li/>	<a href="https://github.com/chflame163/ComfyUI_LayerStyle">	LayerStyle	</a>
<li/>	<a href="https://github.com/digitaljohn/comfyui-propost/tree/master">	comfyui-propost	</a>

<li/>	<a href="https://github.com/chflame163/ComfyUI_LayerStyle_Advance">	chflame163/ComfyUI_LayerStyle_Advance (ZhiPu / SegmentAnything)	</a>

<li/>	Detectors - NSFW <a href="https://huggingface.co/Falconsai/nsfw_image_detection">	Falconsai/nsfw_image_detection	</a>,
	<a href="https://github.com/trumanwong/ComfyUI-NSFW-Detection">	ComfyUI-NSFW-Detection	</a>

<li/>	<a href="https://omnisvg.github.io/">	OmniSVG: A Unified Scalable Vector Graphics Generation Model	</a>

<li/>	<a href="https://github.com/pydn/ComfyUI-to-Python-Extension ">	ComfyUI-to-Python-Extension 	</a>

<li/>	Performance <a href="https://liewfeng.github.io/TeaCache/">	TeaCache </a>, 
	<a href="https://github.com/aejion/AccVideo">	AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset	</a>
	<a href="https://github.com/linkedin/Liger-Kernel">	Liger Kernel: Efficient Triton Kernels for LLM Training	</a>

<li/>	<a href="https://www.bentoml.com/blog/comfy-pack-serving-comfyui-workflows-as-apis">	comfy-pack: Serving ComfyUI Workflows as APIs	</a>, 
	<a href="https://github.com/bentoml/comfy-pack">	bentoml/comfy-pack	</a>

<li/>	<a href="https://docs.comfy.org/tutorials/api-nodes/overview">	ComfyUI api-nodes	</a>

<li/>	3D Tools <a href="https://www.autodesk.com/solutions/wonder-dynamics">	AutoDesk - Wonder Dynamics	</a>, 
	<a href="https://www.deepmotion.com/animate-3d">	DeepMotion	</a>,
	<a href="https://www.rokoko.com/products/studio">	Rokoko	</a>,
	<a href="https://odyssey.systems/introducing-explorer">	Odyssey (3D scene generation)	</a>




</ol>

<!----------------------------------------------------------->
<h3>	Tool - Prompt Engineering			</h3><ol start=1 type=1>

<li/>	<a href="https://github.com/adieyal/comfyui-dynamicprompts">	adieyal/comfyui-dynamicprompts	</a>
<li/>	<a href="https://github.com/MushroomFleet/LLM-Base-Prompts">	MushroomFleet/LLM-Base-Prompts (mixed)	 </a>
<li/>	<a href="https://github.com/AIrjen/OneButtonPrompt">	AIrjen/OneButtonPrompt 	</a>
<li/>	<a href="https://prompthero.com/portraits-prompts">	PromptHero - portraits-prompts	</a>



</ol>
<!----------------------------------------------------------->
<h3>	Tool - General / TechScan / Research			</h3><ol start=1 type=1>

<li/>	<mark>
	<a href="https://huggingface.co/papers">	HuggingFace - Daily Papers	</a>, 
	<a href="https://blog.comfy.org/">	blog.comfy.org	</a>, 
	<a href="https://docs.comfy.org/">	docs.comfy.org	</a>, 
	<a href="https://github.com/ComfyUI-Workflow/awesome-comfyui">	awesome-comfyui </a>
	</mark>

<li/>	TopazLabs <a href="https://openart.ai/workflows/t8star/-topaz-aigigapixel/nfK3ydWg8NYZIoYn9wwc">	PhotoAI GigaPixel	</a>, 
	<a href="https://openart.ai/workflows/t8star/-topaz-video/XsQRrLpnfNfLqlg7emVz">	Video AI	</a>

<li/> Research Assistant 	<a href="https://github.com/bytedance/pasa">	ByteDance PaSa: An LLM Agent for Comprehensive Academic Paper Search </a>,
	<a href="https://storm.genie.stanford.edu/">	Stanford STORM </a>,
	<a href="https://github.com/dzhng/deep-research">	Github - Deep Research 	</a>,
	<a href="https://github.com/masterFoad/NanoSage">	NanoSage - Advanced Recursive Search & Report Generation	</a>
	<a href="https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research">	Perplexity - Deep Research 	</a>,
	<a href="https://foremost-beechnut-8ed.notion.site/WebThinker-Empowering-Large-Reasoning-Models-with-Deep-Research-Capability-d13158a27d924a4b9df7f9ab94066b64">	WebThinker 	</a>

<li/>	<a href="https://dify.ai/blog/dify-deepseek-deploy-a-private-ai-assistant">	dify-deepseek-deploy-a-private-ai-assistant	</a>

<li/>	<a href="https://arxiv.org/abs/2504.08066">	AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search	</a>, 
	<a href="https://github.com/SakanaAI/AI-Scientist-v2">	SakanaAI/AI-Scientist-v2	</a>

<li/>	<a href="https://arxiv.org/abs/2504.21776">	WebThinker: Empowering Large Reasoning Models with Deep Research Capability	</a>, 
	<a href="https://github.com/RUC-NLPIR/WebThinker">	RUC-NLPIR/WebThinker	</a>, 


</ol>

<!----------------------------------------------------------->
<h3>	Tool - Training			</h3><ol start=1 type=1>

<li/>	Data <a href="https://blog.eleuther.ai/common-pile/">	Common Pile v0.1	</a>


<li/>	<a href="https://github.com/kijai/ComfyUI-FluxTrainer">	ComfyUI-FluxTrainer	</a>,
	<a href="https://openart.ai/workflows/-/flux-lora-trainer-on-comfyui-v11/XQuqTMSGQCzsgWgkQ1MN">	flux-lora-trainer-on-comfyui-v11	</a>,
	<a href="https://openart.ai/workflows/leeguandong/flux-trainer-lora/dERqhXJbsHtDDvxzQPxl">	leeguandong/flux-trainer-lora	</a>

<li/>	<a href="https://www.youtube.com/watch?v=bf8_mEvs9lE">	YouTube - Custom AI Digital Human with HeyGen's Lora Training	</a>

<li/>	<a href="https://www.youtube.com/watch?v=ksUEu6c-ouc">	Training FLUX LORA 	</a>
<li/>	<a href="https://civitai.com/articles/7131/flux-lora-trainer-on-comfyui-v10">	Training FLUX LORA	</a>
<li/>	<a href="https://civitai.com/models/713258/flux-lora-trainer-on-comfyui">	Training FLUX LORA	</a>

<li/>	<a href="https://www.youtube.com/watch?v=1DyfPhIkX78">	FluxGym 	</a>,
	<a href="https://www.youtube.com/watch?v=gdEeceGYBu0">	Training FLUX LORA	</a>,
	<a href="https://www.youtube.com/watch?v=nySGu12Y05k">	Training FLUX LORA	</a>

<li/>	<a href="https://civitai.com/articles/9360/flux-guide-part-i-lora-training">	CivitAI - flux-guide-part-i-lora-training	</a>

<li/>	<a href="https://github.com/LarryJane491/Lora-Training-in-Comfy">	Training LORA	</a>
<li/>	<a href="https://civitai.com/articles/3406/lora-training-dataset-creation-comfyui-one-click-dataset">	Training lora-training-dataset-creation-comfyui-one-click-dataset	</a>

<li/>	<a href="https://github.com/modelscope/data-juicer">	ModelScope/data-juicer	</a>

<li/>	SpeechMatics <a href="https://blog.speechmatics.com/sesame-finetune">	How to Finetune Sesame AI's Speech Model on New Languages and Voices	</a>, 
	<a href="https://github.com/knottwill/sesame-finetune">	knottwill/sesame-finetune </a>

<li/>	Unsloth.ai <a href="https://unsloth.ai/blog/qwen3"> Fine-tune & Run Qwen3	</a>, 
	<a href="https://unsloth.ai/blog/tts">	Fine-tuning TTS models (Sesame's CSM, Orpheus)	</a>

</ol>

</td><td width="30%"><!----------------------------------------------------------->
<!----------------------------------------------------------->
<h3>	Upscale SUPIR			</h3>
<li/>	Keywords: <a href="https://openart.ai/workflows/all?keyword=upscale">	OpenArt - Upscale	</a>,
	<a href="https://openart.ai/workflows/all?keyword=SUPIR">	OpenArt - SUPIR	</a>, 
	<a href="https://openmodeldb.info/">		OpenModelDB	</a>,
	<a href="https://huggingface.co/Phips">	HuggingFace - Phips </a>,
	<a href="https://openmodeldb.info/?q=Skin">		realistic skin	</a>
	<br/><ol start=1 type=1>

<li/>	<a href="https://huggingface.co/ac-pill/upscale_models/tree/main">	ac-pill/upscale_models (e.g. RealESRGAN_x4plus_anime_6B.pth) 	</a>

<li/>	<a href="https://bryanswkim.github.io/chain-of-zoom/">	Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment	 (No COmfyUI)	</a>

<li/>	<a href="https://github.com/zsyOAOA/InvSR">	InvSR - Arbitrary-steps Image Super-resolution via Diffusion Inversion (No ComfyUI)	</a>,
	<a href="https://huggingface.co/spaces/OAOA/InvSR">	OAOA/InvSR demo	</a>

<li/> <a href="https://huggingface.co/camenduru/SUPIR/tree/main">		camenduru/SUPIR	</a>,
	<a href="https://openart.ai/workflows/crocodile_ruddy_19/supir-upscale/WdeLcIKRRaPNDnayUy49">	SUPIR	</a>,
	<a href="https://openart.ai/workflows/meerkat_elliptical_71/supir-image-scale/elXpUhL9AN0UzHS5zRYh">		supir-image-scale	</a>,
	<a href="https://openart.ai/workflows/jerrydavos/ultimate-flux-upscaler---2k---4k---8k---16k---32k/viXz5ezmbAEcqORoN9iW">	jerrydavos/ultimate-flux-upscaler 2k, 4k, 8k, 16k	</a>

<li/> <a href="https://huggingface.co/uwg/upscaler/tree/main/SwinIR">		HuggingFace - upscaler	</a>

<li/>	<a href="https://openart.ai/workflows/comfyuiblog/upscale-flux1-dev-controlnet-union-pro/hY1q5HqEY8jDu2KrHUE3">	Flux ControlNet Upscale	</a>

<li/>	<a href="https://github.com/shiimizu/ComfyUI-TiledDiffusion">	GitHub - shiimizu/ComfyUI-TiledDiffusion	</a>
<li/>	<a href="https://github.com/ssitu/ComfyUI_UltimateSDUpscale">	GitHub - ssitu/ComfyUI_UltimateSDUpscale	</a>



<li/>	<a href="https://therasr.github.io/">	Thera: Aliasing-Free Arbitrary-Scale Super-Resolution with Neural Heat Fields (No COmfyUI)	</a>



</ol>
<!----------------------------------------------------------->
<h3>	Video			</h3>
<li/>	Keywords: <a href="https://github.com/showlab/Awesome-Video-Diffusion">	Github - Awesome Video Diffusion	</a>,
	<a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding">	Github - Awesome-LLMs-for-Video-Understanding	</a><br/><ol start=1 type=1>


<li/> Subject-to-Video (s2v) 
	<a href="https://github.com/SkyworkAI/SkyReels-V1">	SkyworkAI/SkyReels-V1 </a>,
	<a href="https://huggingface.co/Kijai/SkyReels-V1-Hunyuan_comfy">	Kijai/SkyReels-V1-Hunyuan_comfy </a>,

<li/>	
	<mark>
	<a href="https://github.com/lllyasviel/FramePack">	FramePack - generate 1-minute video (60 seconds)	</a>, 
	<a href="https://openart.ai/workflows/pix_studio/framepack6g/kZCYax19o8PgNs25y1Xr">	pix_studio/framepack6g	</a>, 
	<a href="https://openart.ai/workflows/t8star/framepack-v1/5RCvSnlYsoe94w6NknBl">	t8star/framepack-v1	</a>
	</mark>

<li/>	<a href="https://www.genmo.ai/">	Genmo	</a>
	<a href="https://github.com/genmoai/mochi">	Mochi	</a>,
	<a href="https://github.com/kijai/ComfyUI-MochiWrapper">	ComfyUI-MochiWrapper	</a>
	<a href="https://github.com/logtd/ComfyUI-MochiEdit">		GitHub - logtd/ComfyUI-MochiEdit	</a>,

<li/>	<a href="https://github.com/logtd/ComfyUI-LTXTricks">		GitHub - logtd/ComfyUI-LTXTricks	</a>,
	<a href="https://openart.ai/workflows/pika_creamy_57/ltx091-i2vgen-long-video/b8ftrZGQ76AN2JDl6chW">		pika_creamy_57/ltx091-i2vgen-long-video	</a>,
	<a href="https://openart.ai/workflows/neofuturist/ltx-91-llm-movie-directors/YvlF3w9ndoUFYVchFc1x">	neofuturist/ltx-91-llm-movie-directors	</a>

<li/>	<a href="https://xiaoyushi97.github.io/Motion-I2V/">	Motion-I2V (No ComfyUI)	</a>
<li/>	<a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">	Google Genie-2 (No ComfyUI)	</a>
<li/>	<a href="https://huggingface.co/THUDM">	Tsinghua University Knowledge Engineering Group (KEG) & Data Mining 	</a>
	<a href="https://huggingface.co/THUDM/CogVideoX-5b">	CogVideoX-5b	</a>
	<a href="https://docs.google.com/spreadsheets/d/16eA6mSL8XkTcu9fSWkPSHfRIqyAKJbR1O99xnuGdCKY/edit?gid=0#gid=0">	CogVideoX models	</a>

<li/>	<a href="https://openart.ai/workflows/datou/cogvideo-tora/pfHf1wJRmxItzrqx6nHJ">	Alibaba TORA	</a>

<li/>	<a href="https://openart.ai/workflows/cat_untimely_42/ltx-video-enhance-stg/wMIwpeYhc372cOtpiiNo">		OpenArt - cat_untimely_42/ltx-video-enhance-stg	</a>

<li/>	<a href="https://openart.ai/workflows/cat_untimely_42/gimmvfi-video/Z4OcbafEIPpppewJLn7V">		OpenArt - cat_untimely_42/gimmvfi-video (frame smoothen)	</a>

<li/>	<a href="https://openart.ai/workflows/llama_gifted_48/basic-video-face-swap/brRQtSZeVrb4W62TL4Ln">	OpenArt - llama_gifted_48/basic-video-face-swap	</a>
<li/>	<a href="https://openart.ai/workflows/cat_untimely_42/gimmvfi-video/Z4OcbafEIPpppewJLn7V">	OpenArt - cat_untimely_42/gimmvfi-video	</a>

<li/>	<a href="https://magref-video.github.io/magref.github.io/">		MAGREF - Masked Guidance for Any-Reference Video Generation	</a>, 
	<a href="https://github.com/MAGREF-Video/MAGREF">	MAGREF-Video/MAGREF		</a>, 

<li/>	Phantom (Subject2Video)
	<a href="https://github.com/Phantom-video/Phantom">	Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment	</a>, 
	<a href="https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/dev/example_workflows">	kijai/ComfyUI-WanVideoWrapper	</a>, 
	<a href="https://openart.ai/workflows/pix_studio/phantom/OHTBKYkLglPQCwKmwTxM">	pix_studio/phantom	</a>, 
	<a href="https://www.youtube.com/watch?v=ZYnhSTMa5VQ"> YouTube - Phantom workflow	</a>


<li/>	Remade-AI <a href="https://huggingface.co/Remade-AI">	HuggingFace - Remade-AI (video LORA) 	</a>, 
	<a href="https://huggingface.co/spaces/Remade-AI/remade-effects">	remade-effects	</a>,
	<a href="https://huggingface.co/Remade-AI/Selfie-With-Younger-Self">	Selfie-With-Younger-Self	</a>, 
	<a href="https://huggingface.co/Remade-AI/Rotate">	360 Degree Rotation	</a>, 
	<a href="https://huggingface.co/Remade-AI/Zoom-Call">	Zoom-Call	</a>, 
	<a href="https://github.com/amao2001/ganloss-latent-space/blob/main/workflow/2025-04-08%20%E4%BA%92%E5%8A%A8%E5%9E%8Blora.json">	workflow - Selfie-With-Younger-Self </a>

<li/>	SkyReels (e2v)
	<a href="https://huggingface.co/Skywork/SkyReels-V1-Hunyuan-I2V">	Skyreels V1: Human-Centric Video Foundation Model	</a>,
	<a href="https://github.com/SkyworkAI/SkyReels-V2">	SkyReels V2: Infinite-Length Film Generative Model	</a>, 
	<a href="https://openart.ai/workflows/pix_studio/aiskyreels-v2-dfcomfyui/gFBctY3VkaT8kIpYgPSd">	pix_studio/aiskyreels-v2-dfcomfyui	</a>,

<li/>	Tencent Hunyuan <a href="https://aivideo.hunyuan.tencent.com/">	Tencent Hunyuan	</a>,
	<a href="https://github.com/kijai/ComfyUI-HunyuanVideoWrapper">	ComfyUI-HunyuanVideoWrapper	</a>,
	<a href="https://huggingface.co/Kijai/HunyuanVideo_comfy/tree/main">	HunyuanVideo_comfy models	</a>,
	<a href="https://openart.ai/workflows/odam_ai/hunyuan-video-generation-face-swap/GyIn6sC02Pg0urjVDh7a">	hunyuan-video-generation-face-swap	</a>,
	<a href="https://openart.ai/workflows/latent_dream/hunyuan-vid2vid-txt2vid-fast-test-bench-v1/dSo9BIpAbkaalNZjL5RI">	latent_dream/hunyuan-vid2vid-txt2vid-fast-test-bench-v1	</a>,
	<a href="https://openart.ai/workflows/flounder_bowed_50/hunyuan-video-generation-large-model/VckyU6A9c7up0x2SvdL5">	flounder_bowed_50/hunyuan-video-generation-large-model	</a>


<li/>	Video Frame Interpolation <a href="https://github.com/kijai/ComfyUI-GIMM-VFI">	kijai/ComfyUI-GIMM-VFI 	</a>,
	<a href="https://github.com/Fannovel16/ComfyUI-Frame-Interpolation">	Fannovel16/ComfyUI-Frame-Interpolation	</a>


<li/>	<a href="https://matankleiner.github.io/flowedit/">	FlowEdit 	</a>,
	<a href="https://www.youtube.com/watch?v=qJ21Vf0eHzg">	FlowEdit Image Editing (One-Click Text Modification)	</a>,
	<a href="https://github.com/logtd/ComfyUI-Fluxtapoz">	https://github.com/logtd/ComfyUI-Fluxtapoz	</a>,
	<a href="https://www.youtube.com/watch?v=s04W4tIyyKU">	FlowEdit Video Editing (No Masks, No Noise)	</a>,
	<a href="https://github.com/logtd/ComfyUI-LTXTricks">	logtd/ComfyUI-LTXTricks	</a>,
	<a href="https://github.com/logtd/ComfyUI-HunyuanLoom">	logtd/ComfyUI-HunyuanLoom	</a>,

<li/>	<a href="https://github.com/Fannovel16/ComfyUI-MotionDiff">	GitHub - Fannovel16/ComfyUI-MotionDiff	</a>

<li/>	WAN
	<a href="https://github.com/Wan-Video/Wan2.1">	Wan: Open Large-Scale Video Generative Models </a>,
	<a href="https://anytraj.github.io/">	ATI: Any Trajectory Instruction for Controllable Video Generation </a>, 
	<a href="https://self-forcing.github.io/">	Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion	</a>, 
	<a href="https://causvid.github.io/">	CausVid - From Slow Bidirectional to Fast Autoregressive Video Diffusion Models	</a>, 
	<a href="https://github.com/ali-vilab/VACE">	VACE: All-in-One Video Creation and Editing	</a>, 


</ol>
<hr><!----------------------------------------------------------->
<h3>	3D OpenPose / PoseNet / DepthMap	</h3>
<li/> Keyword:
	<a href="https://github.com/orgs/VAST-AI-Research/repositories">	VAST-AI-Research/repositories	</a>, 
	<a href="https://civitai.com/tag/poses">			CivitAI - poses	</a>,
	<a href="https://civitai.com/tag/openpose">		CivitAI - openpose	</a>,
	<a href="https://openart.ai/workflows/all?keyword=OpenPose">	openart- OpenPose	</a>,
	<a href="https://openart.ai/workflows/all?keyword=Depth">		openart- Depth	</a><br/><ol start=1 type=1>

<li/>	<a href="https://github.com/ZHO-ZHO-ZHO/X-Pose-ZHO"> GitHub - ZHO-ZHO-ZHO/X-Pose-ZHO	</a>

<li/>	<a href="https://openart.ai/workflows/pixeleasel/inpainting-pose-editor-color-match-composite/yqxeK0ENomLbCK8RLuXZ">	Inpainting Pose Editor, Color Match, Composite	</a>,
	<a href="https://github.com/hinablue/ComfyUI_3dPoseEditor"> GitHub - hinablue/ComfyUI_3dPoseEditor	</a>

<li/>	Data <a href="https://www.posemaniacs.com/poses"> PoseManiacs </a>,
	<a href="https://github.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/tree/master"> Bandai-Namco  </a>,
	<a href="https://github.com/a-lgil/pose-depot">	Pose-Depot  </a>,
	<a href="https://civitai.com/models/157187/poses-bundle-aio-collection-over-20000-poses-controlnet">	CivitAI (>5Gb)	    </a>,
	<a href="https://posemy.art/angry-poses/">	PoseMyArt 	</a>,
	<a href="https://app.anything.world/">	AppAnything 1       </a>,
	<a href="https://app.anything.world/gallery/m/pieter_adams%230000">	AppAnything 2       </a>,
	<a href="https://app.anything.world/gallery/m/casual_winter_clothes_man%230000">	AppAnything 3       </a>,
	<a href="https://humandataset.com/">	HumanDataset 1   </a>,
	<a href="https://humandataset.com/individuals/">	HumanDataset 2   </a>,
	<a href="https://www.3dscanstore.com/blog">	3DScanStore   </a>,
	<a href="https://renderpeople.com/3d-people/?_product=rigged-people%2Canimated-people&_type=bundles">	RenderPeople   </a>,
	<a href="https://mocap.cs.cmu.edu/search.php?subjectnumber=%&motion=%">	CMU Graphics Lab Motion Capture Database	</a>, 
	<a href="https://github.com/microsoft/Microsoft-Rocketbox">		Microsoft-Rocketbox </a>,

<li/>	MarketPlace	 <a href="https://www.deviantart.com/tag/poses">	DevianArt	</a>,
	<a href="https://www.proko.com/@stan/tools">	Proko  	</a>,
	<a href="https://mocapcentral.com/collections/mocap-studio-series">`	MocapCentral	</a>

<li/>	Tool - Poses <a href="https://openposeai.com/">	OpenPoseAI (detect pose from image)	</a>

<li/>	<a href="https://github.com/MVIG-SJTU/AlphaPose">	AlphaPose (out-dated)	</a>

<li/> <a href="https://github.com/DepthAnything/Depth-Anything-V2">		GitHub - Depth-Anything-V2	</a>,
	<a href="https://huggingface.co/spaces/depth-anything/Depth-Anything-V2">		HuggingFace - Demo	</a>

<li/> <a href="https://huggingface.co/apple/DepthPro">	Appple DepthPro	</a>,
	<a href="https://github.com/spacepxl/ComfyUI-Depth-Pro">	ComfyUI-Depth-Pro	</a>,
	<a href="https://openart.ai/workflows/ailab/depthflow-with-comfyui-depth-pro/4TQF8VyJgkCY4RuHYA79">	depthflow-with-comfyui-depth-pro	</a>

<li/> <a href="https://github.com/Fannovel16/comfyui_controlnet_aux">	comfyui_controlnet_aux-Midas, Zoe Depth	</a>,
	<a href="https://github.com/kijai/ComfyUI-Marigold">	ComfyUI-Marigold	</a>

<li/> <a href="https://openart.ai/workflows/amr_sha/flux-controlnet-depth-v3-canny-v3/RbbioFRM8vM3lPsH1U6a">	OpenArt - amr_sha/flux-controlnet-depth-canny	</a>

<li/>	<a href="https://github.com/TMElyralab/Comfyui-MusePose">	TMElyralab/Comfyui-MusePose	</a>

<li/>	<a href="https://github.com/akatz-ai/ComfyUI-DepthCrafter-Nodes">	Tencent akatz-ai/ComfyUI-DepthCrafter-Nodes	</a>

<li/>	<a href="https://sotamak1r.github.io/deepverse/">	DeepVerse - 4D Autoregressive Video Generation as a World Model	</a>

<li/>	<a href="https://github.com/fkryan/gazelle">	Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders </a>,
	<a href="https://huggingface.co/spaces/fffiloni/Gaze-LLE">		HuggingFace - fffiloni/Gaze-LLE </a>

<li/>	<a href="https://www.wlyu.me/FaceLift/">	FaceLift: Single Image to 3D Head	</a>

<li/>	Pose Estimation <a href="https://shubham-goel.github.io/4dhumans/">	4DHumans	</a>,
	<a href="https://github.com/shubham-goel/4D-Humans">	shubham-goel/4D-Humans	</a>,
	<a href="https://github.com/open-mmlab/mmpose">	open-mmlab/mmpose	</a>,
	<a href="https://github.com/TMElyralab/Comfyui-MusePose">	TMElyralab/Comfyui-MusePose	</a>,
	<a href="https://github.com/logtd/ComfyUI-4DHumans">	logtd/ComfyUI-4DHumans	</a>

<li/>	<a href="https://articulate-anything.github.io/">	Articulate-Anything - Automatic Modeling of Articulated Objects	</a>

<li/>	GeoWizard <a href="https://huggingface.co/spaces/lemonaddie/geowizard">	GeoWizard 2D->3D	</a>,
	<a href="https://github.com/fuxiao0719/GeoWizard">	GitHub - fuxiao0719/GeoWizard	</a>,
	<a href="https://github.com/kijai/ComfyUI-Geowizard">	kijai/ComfyUI-Geowizard	</a>

<li/>	<a href="https://depth-anything-v2.github.io/">	Depth Anything v2	</a>, 
	<a href="https://huggingface.co/spaces/depth-anything/Depth-Anything-V2">	spaces/depth-anything - demo	</a>

<li/>	<a href="https://github.com/Westlake-AGI-Lab/Distill-Any-Depth">	Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator	</a>

<li/>	<a href="https://github.com/smthemex/ComfyUI_Sapiens">	ComfyUI_Sapiens - (seg,normal,pose,depth,mask maps)	</a>m
	<a href="https://huggingface.co/facebook/sapiens-pose-1b-torchscript/tree/main">	sapiens-pose-1b-torchscript	</a>

<li/>	<a href="https://francis-rings.github.io/StableAnimator/">	StableAnimator: High-Quality Identity-Preserving Human Image Animation	</a>

</ol>
<hr><!----------------------------------------------------------->
<h3>	3D - 2D to 3D Monocular / NERF / Gaussian Splatting / Multi-view	</h3>
<li/> Keyword:
	<a href="https://github.com/longxiang-ai/awesome-gaussians">		Github - awesome-gaussians	</a>,
	<a href="https://github.com/MrNeRF/awesome-3D-gaussian-splatting">	Github - awesome-3D-gaussian-splatting	</a><br/><ol start=1 type=1>

<li/>	<a href="https://sunshinewyc.github.io/BlockGaussian/">	BlockGaussian	</a>

<li/>	CityGaussian <a href="https://github.com/Linketic/CityGaussian">	CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes	</a>,
	<a href="https://dekuliutesla.github.io/citygs/">	GitHub - citygs	</a>, 
	<a href="https://arxiv.org/abs/2411.00771v1">	Paper	</a>

<li/>	DreamTechAI <a href="https://github.com/DreamTechAI/Direct3D-S2">	Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention	</a>

<li/>	<a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">	Google Genie2: Generative Interactive Environments	</a>

<li/>	<a href="https://rogermm14.github.io/eonerf/"> EO-NeRF - Multi-Date Earth Observation NeRF - The Detail Is in the Shadows	</a>, 
	<a href="https://mezzelfo.github.io/EOGS/">	EOGS - Gaussian Splatting for Efficient Satellite Image Photogrammetry	</a>, 
	<a href="https://arxiv.org/abs/2412.13047">	EOGS Paper	</a>

<li/>	<a href="https://3d-models.hunyuan.tencent.com/">	Hunyuan3D-2: High Resolution Textured 3D Assets Generation	</a>, 
	<a href="https://github.com/tencent/Hunyuan3D-2">	tencent/Hunyuan3D-2	</a>
	<a href="https://huggingface.co/spaces/tencent/Hunyuan3D-2">	Hunyuan3D-2 demo	 	</a>, 

<li/>	<a href="https://github.com/microsoft/MoGe">	MoGe - Monocular 2D->3D </a>,
	<a href="https://github.com/kijai/ComfyUI-MoGe">	kijai/ComfyUI-MoGe </a>

<li/>	<a href="https://geo4d.github.io/">	Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction	</a>, 
	<a href="https://github.com/jzr99/Geo4D">	jzr99/Geo4D	</a>

<li/>	<a href="https://vast-ai-research.github.io/HoloPart/">	HoloPart: Generative 3D Part Amodal Segmentation	</a>,
	<a href="https://huggingface.co/spaces/VAST-AI/HoloPart">	HoloPart demo	</a>, 
	<a href="https://github.com/Pointcept/SAMPart3D">	SAMPart3D: Segment Any Part in 3D Objects		</a>

<li/>	<a href="https://github.com/jtydhr88/ComfyUI-InstantMesh">	jtydhr88/ComfyUI-InstantMesh	</a>

<li/>	Meta - Multi-SpatialMLLM
	<a href="https://runsenxu.com/projects/Multi-SpatialMLLM/">	Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models	</a>

<li/>	<a href="https://muelea.github.io/hsfm/">	Humans and Structure from Motion (HSfM) - Reconstructing People, Places, and Cameras	</a>

<li/>	<a href="https://github.com/jasongzy/Make-It-Animatable">	Make-It-Animatable	</a>,
	<a href="https://huggingface.co/spaces/jasongzy/Make-It-Animatable">	Demo	</a>

<li/>	AllenAI	<a href="https://objaverse.allenai.org/">		Objaverse-XL - A Universe of 10M+ 3D Objects	</a>

<li/>	<a href="https://silent-chen.github.io/PartGen/">	PartGen - Part-level 3D Generation and Reconstruction	</a>

<li/>	<a href="https://manycore-research.github.io/SpatialLM/">	SpatialLM: Large Language Model for Spatial Understanding (No COmfyUI)	</a>, 
	<a href="https://github.com/manycore-research/SpatialLM">	manycore-research/SpatialLM	</a>,
	<a href="https://huggingface.co/manycore-research">	manycore research	</a>

<li/>	<a href="https://github.com/huanngzh/MV-Adapter">	MV-Adapter: Multi-view Consistent Image Generation Made Easy	</a>, 
	<a href="https://github.com/huanngzh/ComfyUI-MVAdapter">	ComfyUI-MVAdapter	</a>, 
	<a href="https://huggingface.co/collections/huanngzh/mv-adapter-spaces-677e497578747fd734a1b999">	MVAdapter-demo	</a>, 
	<a href="https://arxiv.org/abs/2412.03632">	Paper 	</a>

<li/>	OccluGaussian <a href="https://occlugaussian.github.io/">	OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering	</a>, 
	<a href="https://arxiv.org/abs/2503.16177v1">	Paper	</a>

<li/>	<a href="https://jamesyjl.github.io/ShapeLLM/">		ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding	</a>

<li/>	Stable-X
	<a href="https://github.com/Stable-X/Hi3DGen">	Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal Bridging (no textures)	</a>,
	<a href="https://huggingface.co/spaces/Stable-X/Hi3DGen">	Stable-X/Hi3DGen demo	</a>, 
	<a href="https://github.com/Stable-X/ComfyUI-Hi3DGen">	Stable-X/ComfyUI-Hi3DGen	</a>,
	<a href="https://openart.ai/workflows/t8star/hi-3dgen3d/FLKrkPWPiwNO5W0RYl8F">	t8star/hi-3dgen3d	</a>

<li/>	<a href="https://immersegen.github.io/">	ImmerseGen - Agent-Guided Immersive World Generation with Alpha-Textured Proxies	</a>

<li/>	<a href="https://playerone-hku.github.io/">	PlayerOne: Egocentric World Simulator	</a>




<li/>	<a href="https://github.com/lizhihao6/Sparc3D">	Sparc3D: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling	</a>, 
	<a href="https://huggingface.co/spaces/ilcve21/Sparc3D">	ilcve21/Sparc3D	</a>

<li/>	SynCity <a href="https://research.paulengstler.com/syncity/">	SynCity: Training-Free Generation of 3D Worlds	</a>,
	<a href="https://arxiv.org/abs/2503.16420">	Paper	</a>



<li/>	<a href="https://github.com/lpiccinelli-eth/UniK3D">	UniK3D: Universal Camera Monocular 3D Estimation	</a>, 
	<a href="https://huggingface.co/spaces/lpiccinelli/UniK3D-demo">	UniK3D-demo	</a>

<li/>	<mark>
	Trellis3D <a href="https://trellis3d.github.io/">	Trellis3d - Structured 3D Latents - for Scalable and Versatile 3D Generation	</a>,
	<a href="https://huggingface.co/spaces/JeffreyXiang/TRELLIS">	Trellis demo	</a>,
	<a href="https://github.com/smthemex/ComfyUI_TRELLIS">	smthemex/ComfyUI_TRELLIS	</a>,
	<a href="https://github.com/if-ai/ComfyUI-IF_Trellis">	if-ai/ComfyUI-IF_Trellis	</a>,
	<a href="https://github.com/microsoft/TRELLIS">	microsoft/TRELLIS	</a>
	<a href="https://www.youtube.com/watch?v=-vEpuYL9I3g">	YouTube	</a>
	</mark>

<li/>	VastGaussian <a href="https://vastgaussian.github.io/">	VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction	</a>, 
	<a href="https://arxiv.org/abs/2402.17427">	Paper	</a>

<li/>	<a href="https://www.worldlabs.ai/blog">	WorldLabs.AI (Li FeiFei)	</a>


<li/>	<mark>
	<a href="https://eric-ai-lab.github.io/3dtown.github.io/">	3DTown: Constructing a 3D Town from a Single Image	</a>
	</mark>

<li/>	<mark>
	HunYuan 3D <a href="https://www.hunyuan-3d.com/">	hunyuan-3d	</a>, 
	<a href="https://github.com/Tencent/Hunyuan3D-2">	Tencent/Hunyuan3D-2	</a>, 
	<a href="https://github.com/MrForExample/ComfyUI-3D-Pack">	MrForExample/ComfyUI-3D-Pack	</a>, 
	<a href="https://github.com/niknah/ComfyUI-Hunyuan-3D-2">	niknah/ComfyUI-Hunyuan-3D-2	</a>
	</mark>



<li/>	<mark>
	Vast.AI <a href="https://github.com/VAST-AI-Research/TripoSG">	Github - TripoSG	</a>
	</mark>


</ol>

<hr><!----------------------------------------------------------->
<h3>	Datasets	</h3>
<markRED>
<ol>
<li/>	<a href="https://c8241998.github.io/HumanRig/">	HumanRig - Learning Automatic Rigging for Humanoid Character in a Large Scale Dataset	</a>

<li/>	<a href="https://huggingface.co/datasets/kubernetes-bad/CivitAI-As-Characters">	CivitAI-As-Characters	</a>

<li/>	<a href="https://huggingface.co/spaces/mesolitica/Malay-VITS ">	Malay Speech Dataset	</a>
<li/>	<a href="https://github.com/Yuan-ManX/ai-audio-datasets">	Yuan-ManX/ai-audio-datasets 	</a>


</ol>
</markRED>
<hr><!----------------------------------------------------------->
<h3>	Lighting	</h3>
<li/> Keyword:
	<a href="https://civitai.com/search/models?sortBy=models_v9&query=lighting">			CivitAI - lighting	</a>,
	<a href="https://openart.ai/workflows/all?keyword=lighting">		openart- lighting	</a><br/><ol start=1 type=1>

<li/>	IC-Light <a href="https://openart.ai/workflows/risunobushi/relight-with-ic-light-and-background-as-lighting-source/UPKc0ak0YJibwbDff85i">	risunobushi/relight-with-ic-light-and-background-as-lighting-source	</a>,
	<a href="https://openart.ai/workflows/quhan/ic-light-custom-lighting-ic-light/6LWfdnkUyoNmkeXdrlcm">	quhan/ic-light-custom-lighting-ic-light	</a>,
	<a href="https://openart.ai/workflows/risunobushi/relight-people-preserve-colors-and-details/W50hRGaBRUlBT1ReD4EF">	risunobushi/relight-people-preserve-colors-and-details	</a>,

<li/>	<a href="https://github.com/LAOGOU-666/Comfyui-LG_Relight">	GitHub - LAOGOU-666/Comfyui-LG_Relight	</a>

<li/>	<a href="https://github.com/kijai/ComfyUI-Geowizard">	GitHub - kijai/ComfyUI-Geowizard	</a>
<li/>	<a href="https://github.com/kijai/ComfyUI-Lotus">	GitHub - kijai/ComfyUI-Lotus	</a>

<li/>	<a href="https://openart.ai/workflows/profile/risunobushi?sort=latest">	risunobushi (IC Lighting workflows)	</a>

<li/>	<a href="https://gojasper.github.io/latent-bridge-matching/">	LBM: Latent Bridge Matching for Fast Image-to-Image Translation	</a>, 
	<a href="https://github.com/gojasper/LBM">	gojasper/LBM	</a>, 
	<a href="https://openart.ai/workflows/t8star/lbmiclight-v23d-relight-ultra-8/LAxqUznhNGKgNSk1YiSB">	t8star/lbmiclight-v23d-relight-ultra-8	</a>




</ol>

</td>

</tr>
</table>


<!---------------------------------------------------------------------------->

</body>
</html>
