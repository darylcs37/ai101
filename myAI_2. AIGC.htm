<html>
<head>
<title>
	AIGC Awesome List - Jan 26
</title>


<style>
body {
	margin-top   : 2pt;
	margin-left  : 2pt;
	margin-right : 2pt;

	scrollbar-face-color       : gray;
	font-size                  : 12pt;
	scrollbar-highlight-color  : white;
	scrollbar-shadow-color     : black;
	color                      : black;
	scrollbar-3dlight-color    : black;
	scrollbar-arrow-color      : black;
	scrollbar-track-color      : silver;
      font-family                : Consolas,monaco,monospace;
	scrollbar-darkshadow-color : white;
	text-align                 : left
}

table {
	font-size      : 12pt;
	border-spacing : 5px;
	table-layout   : fixed;
	margin-left    : auto;
	margin-right   : auto;
}

th {
	color  : green;
	border : 1px solid black;
}

td {
	word-wrap : break-word;
}

pre {
    background  : #f4f4f4;
    border      : 1px solid #ddd;
    border-left : 3px solid #f36d33;
    color       : #666;
    page-break-inside: avoid;
    font-family   : monospace;
    font-size     : 15px;
    line-height   : 1.6;
    margin-bottom : 1.6em;
    max-width     : 95%;
    overflow      : auto;
    padding       : 1em 1.5em;
    display       : block;
//    word-wrap     : break-word;
    white-space: pre-wrap;       /* Since CSS 2.1 */
    word-wrap: break-word;       /* Internet Explorer 5.5+ */
}

/*---Title-------------------------------------------------------------------*/
H1 {
	border-left-width:  "20pt";
	border-left-style:  solid;
	border-left-color : black;
	border	: sunken;
	background-color : #00FFC3;
	color	           : #111111;
	font-size        : "30px";
	text-align       : left;

	padding-top      : 5pt;
	padding-left     : 5pt;
	padding-bottom   : 5pt;
	padding-right    : 5pt;
	margin           : 0pt;
}

A:link    { color: #2F4F4F; } /* unvisited link daa520 */
A:visited { color: #405888; } /* visited links */

/*---Header------------------------------------------------------------*/
/* FF5733  FFC300 */
H2 {
	background-color : #0070C0;
	color            : #ffffff;
	font-size        : 30px;

	padding-top    : 5pt;
	padding-left   : 5pt;
	padding-bottom : 5pt;
	padding-right  : 5pt;
	margin         : 0pt;
}

H3 {
	background-color : #C70039;
	color        : #FFFFFF;
	font-size    : 20px;

	padding-top    : 5pt;
	padding-left   : 5pt;
	padding-bottom : 5pt;
	padding-right  : 5pt;
	margin         : 0pt;
}

H4 {
	background-color : #CD7F32;
	color          : #FFFFFF;
	font-size      : 15px;

	padding-top    :  8pt;
	padding-left   :  8pt;
	padding-bottom :  8pt;
	padding-right  :  8pt;
}

/*---3D boxes------------------------------------------------------------------*/
.GreyList {
	background-color : #dddddd;
	border-width   : 0px;
	padding-top    : 20px;
	padding-bottom : 5px;
	border-bottom-width : 2px;
}

.LightGreyList {
	background     : #eeeeee;
	border-width   : 0px;
	padding-top    : 20px;
	padding-bottom : 5px;
	border-bottom-width : 2px;
	padding-left        : 10px;
	padding-right       : 5px;
}

.CodeExample{
	background  : #eeeeee;
	font-family : Courier, Helvetica, Arial;
}

/*---Now Tags------------------------------------------------------------------*/
.tagBlue {
	background  : #3377ff;
	color       : #FFFFFF;
	href        : #FFFFFF;
	anchor      : #FFFFFF;
}

/*---------------------------------------------------------------------------*/
/* menu header */
  .tblBlueHeader {font-size: 12px; font-weight: bold;
                  background: #333399; border-top: solid 1px #394CB8; border-left: solid 1px #394CB8;}
  .tblBlueBody {font-size: 12px; color: #ffffff; background: #333366;
                border-top: solid 1px #434376; border-left: solid 1px #434376; border-bottom: solid 1px #232356; border-right: solid 1px #232356;}

  .tblGreenHeader {font-size: 12px; font-weight: bold; background: #4D9933;
                   border-top: solid 1px #437643; border-left: solid 1px #437643; border-bottom: solid 1px #235623; border-right: solid 1px #235623;}
  .tblGreenBody {font-size: 12px; background: #1B3D29;
                 border-top: solid 1px #437643; border-left: solid 1px #437643; border-bottom: solid 1px #235623; border-right: solid 1px #235623;}
  a.tblGreenLink {text-decoration: none; color: #000000; linkColor:#000000; alinkColor:#000000; vlinkColor :#000000;}
  a.tblGreenLink.link {color:#000000; }
  a.tblGreenLink.visited {color:#000000; }


/*---------------------------------------------------------------------------*/
/* Styling using ID tag */
#mark {
	background-color : #DAF7A6;
	color            : black;
}

#markBLUE {
	background-color : #A6DAF7;
	color            : black;
}

#markGRAY {
	background-color : #C0C0C0;
	color            : black;
}

#markORANGE {
	background-color : #F7C610;
	color            : black;
}

#markRED {
	background-color : #F7A6DA;
	color            : black;
}
/*---------------------------------------------------------------------------*/
</style>


</head>
<body>

<p/>

<h2>	TechScan - Artificial Intelligence Generated Content (AIGC) - Jan 26	</h2>

&#183	<a href="#tag3D">	3D	</a>
&#183	<a href="#tagAgent">	Agent	</a>	
&#183	<a href="#tagBenchmark">	Benchmark	</a>
&#183	<a href="#tagCompany">	Company	</a>
&#183	<a href="#tagData">	Datasets	</a>
&#183	<a href="#tagDepthMap">	DepthMap	</a>
&#183	<a href="#tagFaces">	FaceSwap	</a>
&#183	<a href="#tagCaption">	Image Captioning	</a>
&#183	<a href="#tagLighting">	Lighting	</a>
&#183	<a href="#tagModel">	Models	</a>
&#183	<a href="#tagMusic">	Music	</a>
&#183	<a href="#tagPlatform">	Platforms	</a>
&#183	<a href="#tagPerformance">	Performance	</a>
&#183	<a href="#tagRead">	Read	</a>
&#183	<a href="#tagRemove">	Segmentation	</a>	
&#183	<a href="#tagSimulation">	Simulation	</a>	
&#183	<a href="#tagTalkingHead">	TalkingHead	</a>

&#183	<a href="#tagTraining">	Training	</a>	
&#183	<a href="#tagTechScan">	TechScan	</a>
&#183	<a href="#tagTTS">	TTS	</a>
&#183	<a href="#tagUpscale">	Upscale	</a>
&#183	<a href="#tagVideo">	Video	</a>


<table width="100%">
<tr valign="top">

<!----------------------------------------------------------------------------->
<!------------------------------------COLUMN 1--------------------------------->
<!----------------------------------------------------------------------------->
<td width="30%"><!------------------------------------------------------------->
<!----------------------------------------------------------------------------->
<h3 id="tagCompany">	Companies			</h3><ol start=1 type=1>

<li/> AIMageLab <a href="https://github.com/orgs/aimagelab/repositories">	aimagelab/repositories	</a>

<li/> AixonLab <a href="https://www.aixonlab.com/">		AixonLab	</a>,
	<a href="https://huggingface.co/aixonlab">		HuggingFace - AixonLab	</a>,
	<a href="https://huggingface.co/aixonlab/flux.1-lumiere-alpha/blob/main/comfy/lumiere_alpha_workflow.json">		lumiere_alpha	</a>

<li id="mark"/>
	Alibaba  <a href="https://github.com/alibaba/Tora">	Alibaba 	</a>,	
	<a href="https://huggingface.co/alibaba-pai">		Alibaba PAI (Platform for AI)	</a>, 
	<a href="https://github.com/ali-vilab/">		Alibaba TongYi Vision Intelligence Lab	</a>,
	<a href="https://qwenlm.github.io">		Qwen	</a>,
	<a href="https://github.com/QwenLM/Qwen2-VL">		Qwen2-VL	</a>,
	<a href="https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct/tree/main">		Qwen2-VL-7B-Instruct	</a>,
	<a href="https://github.com/ali-vilab/ACE_plus">		ACE++	</a>,
	<a href="https://ali-vilab.github.io/In-Context-LoRA-Page/">		In-Context-LoRA	</a>,
	<a href="https://lucaria-academy.github.io/Animate-X/">	Animate-X	</a>,
	<a href="https://huggingface.co/ali-vilab/ACE_Plus/">	ACE Plus ++	</a>,

	<a href="https://github.com/Wan-Video/Wan2.1">	Wan: Open Large-Scale Video Generative Models </a>
	<a href="https://huggingface.co/Kijai/WanVideo_comfy/tree/main">	Models - Kijai/WanVideo_comfy </a>, 
	<a href="https://github.com/ali-vilab/UniAnimate-DiT">	UniAnimate-DiT	</a>,
	<a href="https://humanaigc.github.io/SwapAnyHead/">	SwapAnyHead (no code)	</a>, 
	<a href="https://github.com/QwenLM/Qwen-Image">	GitHub - Qwen-Image (Chinese text)	</a>, 
	<a href="https://github.com/PKU-YuanGroup/Edit-R1">	Edit-R1: Reinforce Image Editing with Diffusion Negative-Aware Finetuning and MLLM Implicit Feedback	</a>, 
	<a href="https://qwenlm.github.io/blog/qwen-tts/">	Qwen-TTS - Dialects </a>
	<a href="https://humanaigc.github.io/omnitalker/">	OmniTalker	</a>, 
	<a href="https://humanaigc.github.io/wan-animate/">	Wan-Animate: Unified Character Animation and Replacement with Holistic Replication </a>,
	<a href="https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo"	>	Spaces/Qwen3-ASR-Demo (Speech-To-Text)	</a>,	
	<a href="https://qwen.ai/blog?id=4266edf7f3718f2d3fda098b3f4c48f3573215d0&from=home.latest-research-list">	Qwen3-LiveTranslate: Real-Time Multimodal Interpretation - See It, Hear It, Speak It!	</a>,
	<a href="https://github.com/alibaba-damo-academy/Lumos-Custom">		UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback	</a>,
	<a href="https://liveavatar.github.io/">		Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length	</a>,
	<a href="https://github.com/Alibaba-Quark/LiveAvatar">	Alibaba-Quark/LiveAvatar	</a>
	<a href="https://tongyi-mai.github.io/Z-Image-blog/">		Tongyi-MAI/Z-Image	</a>

<li/> Alimama  <a href="https://github.com/orgs/alimama-creative/repositories">		Github- Alimama Creative	</a>,
	<a href="https://huggingface.co/alimama-creative">		HuggingFace - alimama-creative	</a>

<li/> AllenAI <a href="https://huggingface.co/allenai">		AllenAI	</a>,
	<a href="https://allenai.org/open-data">	Open Data	</a>, 
	<a href="https://allenai.org/olmo">	Molml Olmo </a>,
	<a href="https://molmo.allenai.org/">	Molml Demo </a>,
	<a href="https://cychenyue.com/28204.html">	AllenAI Molmo 7B D		</a>,
	<a href="https://github.com/SeanScripts/ComfyUI-PixtralLlamaMolmoVision">	Pixtral Llama Molmo Vision	</a>, 
	<a href="https://github.com/aigc3d">	Applied Vision Lab, Institute for Intelligent Computing	</a>,
	<a href="https://huggingface.co/datasets/allenai/tulu-3-sft-personas-instruction-following">	allenai/tulu-3-sft-personas-instruction-following	</a>
	<a href="https://objaverse.allenai.org/">		Objaverse-XL - A Universe of 10M+ 3D Objects	</a>,
	<a href="https://huggingface.co/spaces/allenai/reward-bench">	RewardBench: Evaluating Reward Models	</a>, 
	<a href="https://huggingface.co/spaces/allenai/ZebraLogic">	ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning	</a>, 
	<a href="https://huggingface.co/spaces/allenai/super_leaderboard">	SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories	</a>, 
	<a href="https://huggingface.co/spaces/allenai/ZeroEval">	ZeroEval: Benchmarking LLMs for Reasoning	</a>, 
	<a href="https://huggingface.co/spaces/allenai/WildBench">	WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild	</a>, 

<li/> Alpha-VLLM 
	<a href="https://github.com/Alpha-VLLM/Lumina-mGPT-2.0">	Lumina-mGPT 2.0: Stand-alone Autoregressive Image Modeling	</a>, 
	<a href="https://github.com/Alpha-VLLM/Lumina-DiMOO">		Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding	</a>, 
	<a href="https://github.com/NewBieAI-Lab/NewBie-image-Exp0.1">	NewBie image Exp0.1: Efficient Image Generation Base Model Based on Next-DiT	</a>, 
	<a href="https://zhenglin-cheng.com/twinflow/">	TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows	</a>

<li/> ANT 
	<a href="https://github.com/antgroup/">		ANT	</a>,
	<a href="https://github.com/antgroup/echomimic_v2">		EchoMimic	</a>, 
	<a href="https://huggingface.co/spaces/BadToBest/EchoMimic">		EchoMimic - Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditioning	</a>

<li/> Anthropic 
	<a href="https://www.anthropic.com/engineering/multi-agent-research-system">	How we built our multi-agent research system	</a>

<li/> Apple  <a href="https://huggingface.co/apple/DepthPro">	Apple DepthPro	</a>,
	<a href="https://github.com/spacepxl/ComfyUI-Depth-Pro">	ComfyUI-Depth-Pro	</a>, 
	<a href="https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e">	FastVLM: Efficient Vision Encoding for Vision Language Models	</a>, 
	<a href="https://github.com/apple/pico-banana-400k">		pico-banana-400k	</a>, 
	<a href="https://starflow-v.github.io/">		STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flows	</a>

<li/> Baidu - BAAI  <a href="https://emu.world/pages/web/landingPage">	Emu3.5: Native Multimodal Models are World Learners	</a>


<li id="mark"/>
	Bilibili 
	<a href="https://index-tts.github.io/index-tts2.github.io/">	IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech	</a>,
	<a href="https://github.com/index-tts">	IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System (based on Tortise/XTTS)	</a>,
	<a href="https://arxiv.org/abs/2502.05512">	IndexTTS Paper	</a>,
	<a href="https://index-tts.github.io/">	IndexTTS Samples	</a>,
	<a href="https://huggingface.co/spaces/IndexTeam/IndexTTS">	IndexTTS demo	</a>,

	<a href="https://github.com/billwuhao/ComfyUI_IndexTTS">	billwuhao/ComfyUI_IndexTTS	</a>, 

<li id="mark"/>
	Black Forest Labs (BFL)
	<a href="https://huggingface.co/black-forest-labs">	BlackForestLabs	</a>,
	<a href="https://arxiv.org/html/2506.15742v2">	FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space	</a>, 
	<a href="https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev">	black-forest-labs/FLUX.1-Krea-dev	</a>,
	<a href="https://www.krea.ai/apps/image/flux-krea">	Krea online tool	</a>, 
	<a href="https://blog.comfy.org/p/flux1-kontext-dev-day-0-support">	ComfyUI - flux1-kontext-dev-day-0-support	</a>,
	<a href="https://huggingface.co/bullerwins/FLUX.1-Kontext-dev-GGUF/tree/main">	bullerwins/FLUX.1-Kontext-dev-GGUF	</a>,
	<a href="https://github.com/ZenAI-Vietnam/ComfyUI-Kontext-Inpainting">	ZenAI-Vietnam/ComfyUI-Kontext-Inpainting </a>, 
	<a href="https://blog.comfy.org/p/flux2-state-of-the-art-visual-intelligence">	Flux2	</a>

<li/>	Boson.AI	<a href="https://www.boson.ai/blog/higgs-audio-v2">	Higgs Audio	</a>, 
	<a href="https://github.com/boson-ai/emergenttts-eval-public">	EmergentTTS-Eval ("Emotions" and "Questions")	</a>, 
	<a href="https://huggingface.co/spaces/smola/higgs_audio_v2">	spaces/smola/higgs_audio_v2	</a>

<li/> BRIA <a href="https://huggingface.co/briaai/RMBG-2.0">		BRIA Background Removal v2.0	</a>

<li id="mark"/>
	ByteDance <a href="https://huggingface.co/ByteDance">	ByteDance	</a>,
	<a href="https://opensource.bytedance.com/project">	ByteDance Research </a>,
	<a href="https://chenglin-yang.github.io/1.58bit.flux.github.io/">	1.58-bit FLUX	</a>,
	<a href="https://github.com/bytedance/LatentSync">	GitHub - LipSync - LatentSync </a>,
	<a href="https://byteaigc.github.io/X-Portrait2/">	X-Portrait2	</a>,
	<a href="https://omnihuman-lab.github.io/v1_5/">	OmniHuman-1.5 </a>,
	<a href="https://grisoon.github.io/DreamActor-M1/">	DreamActor-M1	</a>,
	<a href="https://github.com/Saiyan-World/goku">	GitHub - goku - Flow Based Video Generative Foundation Models </a>,
	<a href="https://github.com/bytedance/MegaTTS3">	bytedance - MegaTTS3 (voice cloning)	</a>, 
	<a href="https://huggingface.co/spaces/ByteDance/MegaTTS3">	MegaTTS3  Demo	</a>, 
	<a href="https://github.com/1038lab/ComfyUI-MegaTTS">	1038lab/ComfyUI-MegaTTS	</a>, 
	<a href="https://openart.ai/workflows/ailab/comfyui-megatts/K0urCR510JCn2FAcYxoW">	ailab/comfyui-megatts	</a>, 
	<a href="https://github.com/bytedance/UNO">	UNO (e2i)	</a>, 
	<a href="https://github.com/bytedance/RealCustom">	RealCustom	</a>, 
	<a href="https://github.com/bytedance/InfiniteYou">	InfiniteYou	</a>, 
	<a href="https://github.com/bytedance/DreamFit">	DreamFit		</a>, 
	<a href="https://github.com/bytedance-seed/BAGEL">	BAGEL (ByteDance Adaptive Generative Language Model) 		</a>, 
	<a href="https://bytedance.github.io/USO/">	USO: Unified Style and Subject Driven Generation via Disentangled and Reward Learning		</a>, 
	<a href="https://arxiv.org/html/2508.21066v1">	OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning		</a>,
	<a href="https://huggingface.co/bytedance-research/OneReward/tree/main">	bytedance-research/OneReward		</a>,
	<a href="https://phantom-video.github.io/HuMo/">	HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning	</a>,
	<a href="https://pbihao.github.io/projects/DreamOmni2/index.html">	DreamOmni2: Multimodal Instruction-based Editing and Generation	</a>, 
	<a href="https://github.com/dvlab-research/DreamOmni2">	dvlab-research/DreamOmni2		</a>, 
	<a href="https://lzy-dot.github.io/BindWeave/">	BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration	</a>, 
	<a href="https">	DreamID-V: Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer	</a>

<li/> CanopyLabs.ai <a href="https://canopylabs.ai/model-releases">	Voice Cloning - Natural intonation, emotion, and rhythm that is superior to SOTA closed source models (giggle, gasp, angry, happy) 	</a>,
	<a href="https://github.com/canopyai/Orpheus-TTS">	Orpheus-TTS	</a>, 
	<a href="https://huggingface.co/spaces/MohamedRashad/Orpheus-TTS">	Orpheus-TTS demo	</a>, 
	<a href="https://github.com/canopyai/Orpheus-TTS/tree/main/additional_inference_options/watermark_audio">	watermark_audio	</a>, 
	<a href="https://github.com/ShmuelRonen/ComfyUI-Orpheus-TTS">	ShmuelRonen/ComfyUI-Orpheus-TTS	</a>,
	<a href="https://byteaigc.github.io/Lynx/">	Lynx: Towards High-Fidelity Personalized Video Generation	</a>,


<li/> Character.AI	<a href="https://aaxwaz.github.io/Ovi/">	Ovi - Twin backbone cross-modal fusion for audio-video generation	</a>,
	<a href="https://github.com/character-ai/Ovi">	character-ai/Ovi	</a>,
	<a href="https://github.com/snicolast/ComfyUI-Ovi">	snicolast/ComfyUI-Ovi	</a>
		
<li/> Cohere  <a href="https://cohere.com/blog/command-r7b-arabic">	r7b-arabic	</a>,
	<a href="https://huggingface.co/CohereForAI">	HuggingFace - CohereForAI		</a>,
	<a href="https://huggingface.co/CohereForAI/c4ai-command-r7b-arabic-02-2025">	HuggingFace - r7b-arabic		</a>,
	<a href="https://arxiv.org/html/2506.10766v1">	One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers	</a>

<li/>	ComfyUI 
	<a href="https://blog.comfy.org/">	Official Blog	</a>, 


<li/>	DeepSeek 
	<a href="https://huggingface.co/deepseek-ai">	HuggingFace - DeepSeek 	</a>,
	<a href="https://huggingface.co/blog/open-r1">	HuggingFace - Open R1	</a>,
	<a href="https://github.com/huggingface/open-r1">	GitHub - Open R1	</a>, 

<li/> Facebook META  <
	a href="https://ai.meta.com/blog/segment-anything-model-3/">	META SAM3 (Segment Anything 3) </a>,

	<a href="https://openart.ai/workflows/cgtips/comfyui---segment-anything-2-sam2---method-2/GTbSSKbVl6KpGu6uh4Lj">	OpenArt - SAM2	</a>,
	<a href="https://huggingface.co/city96/DiT/tree/main">	DiT	</a>,
	<a href="https://github.com/facebookresearch/DiT">	Facebook Research Scalable Diffusion Models with Transformers (DiT)	</a>,
	<a href="https://github.com/facebookresearch/seamless_communication">	Seamless4MT	</a>,
	<a href="https://seamless.metademolab.com/demo">	Seamless4MT Demo	</a>,
	<a href="https://congwei1230.github.io/MoCha/">	MoCha - Towards Movie-Grade Talking Character Synthesis	</a>,
	<a href="https://developers.meta.com/horizon/blog/AssetGen2">	AssetGen2 - Animated 3D game assets	</a>, 
	<a href="https://runsenxu.com/projects/Multi-SpatialMLLM/">	Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models	</a>,
	<a href="https://github.com/facebookresearch/omnilingual-asr">	Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages	</a>, 
	<a href="https://franciszzj.github.io/Saber/">	SABER: Scaling Zero-Shot Reference-to-Video Generation	</a>, 
	<a href="https://zhaochongan.github.io/projects/OneStory/">	OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory </a>, 
	<a href="https://ai.meta.com/samaudio/">	Meta Segment Anything Model Audio (SAM Audio)	</a>, 
	<a href="http://haonanqiu.com/projects/HiStream.html">	HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming(no code)	</a>

<li/>	FAL <a href="https://huggingface.co/fal">	fal	</a>

<li/> FreePik <a href="https://huggingface.co/Freepik">		HuggingFace - FreePik 	</a>, 
	<a href="https://www.freepik.com/blog/f-lite-freepik-and-fal-ai-unveil-open-source-image-model-trained-on-licensed-data/">	Freepik/F-Lite Blog </a>,
	<a href="https://github.com/fal-ai/f-lite">	fal-ai/f-lite	</a>, 
	<a href="https://huggingface.co/Freepik/F-Lite">	Freepik/F-Lite	</a>

<li/> Genmo	<a href="https://www.genmo.ai/">	Genmo	</a>
	<a href="https://github.com/genmoai/mochi">	Mochi	</a>,
	<a href="https://github.com/kijai/ComfyUI-MochiWrapper">	ComfyUI-MochiWrapper	</a>

<li/> Google <a href="https://github.com/google-deepmind/graphcast">		Google-deepmind	</a>,
	<a href="https://deepbrainai-research.github.io/float/">	Google DeepBrain - Generative Motion Latent FlOw MAtching for Audio-driven Talking Portrait (FLOAT)	</a>,
	<a href="https://deepmind.google/discover/blog/gencast-predicts-weather-and-the-risks-of-extreme-conditions-with-sota-accuracy/">	GraphCast / GenCast	</a>, 
 	<a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">	Google Genie2: Generative Interactive Environments	</a>, 
 	<a href="https://deepmind.google/blog/sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds/">	SIMA 2: An Agent that Plays, Reasons, and Learns With You in Virtual 3D Worlds	</a>

<li/> HelloVision <a href="https://github.com/HelloVision/">		HelloVision	</a>,
	<a href="https://github.com/HelloVision/ComfyUI_HelloMeme">	HelloVision/ComfyUI_HelloMeme	</a>

<li/> HongKong University of Science & Technology (HKUST) <a href="https://huggingface.co/spaces/srinivasbilla/llasa-3b-tts">	Llasa-3B	</a>,
	<a href="https://huggingface.co/HKUSTAudio/Llasa-3B">	HKUSTAudio/Llasa-3B	</a>,
	<a href="https://huggingface.co/HKUSTAudio/AudioX">	HKUSTAudio/AudioX	</a>,
	<a href="https://github.com/ZeyueT/AudioX">	ZeyueT/AudioX	</a>, 
	<a href="https://replicate.com/kjjk10/llasa-3b-long">	Replicate - kjjk10/llasa-3b-long	</a>,
	<a href="https://meigen-ai.github.io/multi-talk/">	MultiTalk: Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation	</a>, 
	<a href="https://github.com/MeiGen-AI/MultiTalk">	MeiGen-AI/MultiTalk	</a>, 
	<a href="https://github.com/MeiGen-AI/InfiniteTalk">	InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing	</a>, 
	<a href="https://zcai0612.github.io/UP2You/">	UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections	</a>, 
	<a href="https://github.com/W2GenAI-Lab/LucidFlux">	LucidFlux: Caption-Free Universal Image Restoration with a Large-Scale Diffusion Transformer	</a>

<li/> HuaWei - Computing Systems Lab (CSL)
	<a href="https://github.com/huawei-csl/SINQ">	SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLMs	</a>, 
	<a href="https://huggingface.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web">		Reflection Removal through Efficient Adaptation of Diffusion Transformers	</a>, 
	<a href="https://animotionlab.github.io/MoCapAnything/">	MoCapAnything: Unified 3D Motion Capture for Arbitrary Skeletons from Monocular Videos	</a>


<li/> HuggingFace 
	<a href="https://huggingface.co/HuggingFaceTB">		HuggingFace	</a>,
	<a href="https://huggingface.co/spaces">		HuggingFace	Spaces </a>,
	<a href="https://huggingface.co/latent-consistency">		Latent Consistency (LCM)	</a>, 
	<a href="https://huggingface.co/spaces/HuggingFaceTB/SmolVLM-500M-Instruct-WebGPU">		Vision Language Model - SmolVLM-500M-Instruct-WebGPU	</a>,

<li/> Intel
	<a href="https://github.com/NYU-ICL/image-gs">		Image-GS: Content-Adaptive Image Representation via 2D Gaussians	</a>,
	<a href="https://www.immersivecomputinglab.org/publication/image-gs-content-adaptive-image-representation-via-2d-gaussians/">	Paper	</a>, 
	<a href="https://www.youtube.com/watch?v=_WjU5d26Cc4">	2-minute paper	</a>,


<li/> International Digital Economy Academy (IDEA-Research) <a href="https://github.com/IDEA-Research">	International Digital Economy Academy (IDEA-Research)	</a>,
	<a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">	IDEA-Research/Grounded-Segment-Anything	</a>,
	<a href="https://github.com/IDEA-Research/DINO-X-API">	DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding	</a>,
	<a href="https://deepdataspace.com/en/blog/dino-xseek/">	Dino-X blog	</a>,
	<a href="https://deepdataspace.com/playground/ivp">	TREX-2 - object counting </a>,
	<a href="https://cloud.deepdataspace.com/playground/dino-x">	dino-x - Detection - Segmentation -  Keypoints - Generative Understanding </a>,
	<a href="https://deepdataspace.com/playground/ivp_video">	Video - Object Tracking </a>,
	<a href="https://huggingface.co/blog/smollm">	SmolLM (135M, 360M, 1.7B)	</a>,
	<a href="https://rex-omni.github.io/">	Rex-Omni: Detect Anything via Next Point Prediction	</a>, 
	<a href="https://huggingface.co/spaces/Mountchicken/Rex-Omni">	Rex-Omni demo	</a>

<li/>	Korea	<a href="https://cgrhyu.github.io/4-showcase.html">	CGR Lab HanYang	</a>

<li/> Kuaishou  <a href="https://github.com/KwaiVGI">		Kuaishou Visual Generation and Interaction Center </a>,
	<a href="https://huggingface.co/Kwai-Kolors">		Kwai-Kolors	</a>,
	<a href="https://github.com/orgs/Kwai-Kolors/repositories">		Kwai-Kolors	</a>,
	<a href="https://github.com/kijai/ComfyUI-KwaiKolorsWrapper">	kijai/ComfyUI-KwaiKolorsWrapper	</a>,
	<a href="https://github.com/KwaiVGI/LivePortrait">		LivePortrait	</a>,
	<a href="https://github.com/KwaiVGI/3DTrajMaster">		3DTrajMaster	</a>,
	<a href="https://little-misfit.github.io/GRAG-Image-Editing/">	GRAG: Group-Relative Attention Guidance for Image Editing		</a>, 
	<a href="https://howlin-wang.github.io/svg/">	SVG: Latent Diffusion Model without Variational Autoencoder	</a>

<li/> Kyutai	<a href="https://kyutai.org/next/tts">	Kyutai-TTS	</a>, 
	<a href="https://huggingface.co/kyutai/tts-1.6b-en_fr">	kyutai/tts-1.6b-en_fr	</a>,

<li/> Liblib AI <a href="https://github.com/Xiaojiu-z/EasyControl">		EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer 	</a>,
	<a href="https://github.com/jax-explorer/ComfyUI-easycontrol">		ComfyUI-easycontrol (40Gb VRAM)	</a>

<li id="mark"/>
	Lightricks 
	<a href="https://www.lightricks.com/">	Lightricks 	</a>, 
	<a href="https://github.com/Lightricks/LTX-Video">	Lightricks/LTX-Video </a>, 
	<a href="https://huggingface.co/Lightricks/LTX-Video">		Lightricks - LTX-Video	</a>,
	<a href="https://github.com/Lightricks/ComfyUI-LTXVideo">		ComfyUI-LTXVideo	</a>,
	<a href="https://openart.ai/workflows/cat_untimely_42/ltx-video-enhance-stg/wMIwpeYhc372cOtpiiNo">	OpenArt - cat_untimely_42/ltx-video-enhance-stg	</a>,
	<a href="https://openart.ai/workflows/cat_untimely_42/ltx-video-video-to-video/kqyFkoCU8s16hsjdqZRu">	OpenArt - cat_untimely_42/ltx-video-video-to-video	</a>,
	<a href="https://openart.ai/workflows/monkey_hard-to-find_14/comfyui-ltx-video/NNBGp1H777pvivzDjFwa">		OpenArt - monkey_hard-to-find_14/comfyui-ltx-video	</a>,
	<a href="https://openart.ai/workflows/toucan_chilly_4/ltx/CvRMMGcKETGECx59xZFX">		OpenArt - toucan_chilly_4/ltx	</a>,
	<a href="https://civitai.com/models/995093?modelVersionId=1191358">	CivitAI - LTX IMAGE to VIDEO with STG, CAPTION & CLIP EXTEND workflow	</a>
	<a href="https://github.com/logtd/ComfyUI-LTXTricks">		GitHub - logtd/ComfyUI-LTXTricks	</a>,
	<a href="https://civitai.com/models/995093/ltx-image-to-video-with-stg-caption-and-clip-extend-workflow">	LTX IMAGE to VIDEO with STG, CAPTION & CLIP EXTEND workflow	</a>

<li/> Liquid.AI	
	<a href="https://huggingface.co/LiquidAI/LFM2-VL-3B">	LFM2: On-Device Models	</a>, 
	<a href="https://www.liquid.ai/models">	Edge Models	</a>, 
	<a href="https://www.liquid.ai/blog/introducing-lfm2-5-the-next-generation-of-on-device-ai">	LFM2.5 Models	</a>

<li/> LG <a href="https://felixtaubner.github.io/cap4d/">		LG - CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models	</a>
	<a href="https://github.com/felixtaubner/cap4d">	felixtaubner/cap4d		</a>

<li/> Marvis-AI	
	<a href="https://huggingface.co/Marvis-AI">	Marvis-TTS-250m (Sesame CSM-1B, Kyutai mimi codec)	</a>, 
	<a href="https://github.com/Marvis-Labs/marvis-tts">		Marvis-Labs/marvis-tts	</a>

<li/> Meituan	
	<a href="https://tech.meituan.com/">	Meituan Technology Team	</a>, 
	<a href="https://github.com/MeiGen-AI/MultiTalk">	MeiGen-AI/MultiTalk	</a>, 
	<a href="https://github.com/MeiGen-AI/InfiniteTalk">	InfiniteTalk: Audio-driven Video Generation for Sparse-Frame Video Dubbing	</a>, 
	<a href="https://wanghao9610.github.io/X-SAM/">	X-SAM: From Segment Anything to Any Segmentation	</a>, 
	<a href="https://vitabench.github.io/">		VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications	</a>,
	<a href="https://meituan-longcat.github.io/LongCat-Video/">	LongCat-Video: A Unified Foundational Video Generation Model	</a>,
	<a href="https://meigen-ai.github.io/LongCat-Video-Avatar/">	LongCat-Video-Avatar	</a>, 
	<a href="https://github.com/meituan-longcat/LongCat-Image">	LongCat-Image-6B (40Gb)	</a>, 

<li/>	MiaoshouAI <a href="https://www.miaoshouai.com/">	MiaoshouAI	</a>,
	<a href="https://huggingface.co/MiaoshouAI">	HuggingFace - Florence-2-base-PromptGen-v2.0 / Florence-2-large-PromptGen-v2.0	</a>
	<a href="https://huggingface.co/MiaoshouAI/Florence-2-large-PromptGen-v1.5">	Florence-2-large-PromptGen-v1.5	</a>
	<a href="https://github.com/miaoshouai/ComfyUI-Miaoshouai-Tagger">	ComfyUI-Miaoshouai-Tagger	</a>

<li/>	Microsoft <a href="https://huggingface.co/microsoft">	Microsoft (Florence2, Phi 3.5, Phi 4) </a>,
	<a href="https://huggingface.co/microsoft/Phi-4-multimodal-instruct">	Phi-4-multimodal-instruct	</a>, 
	<a href="https://wangrc.site/MoGePage/">	MoGe - Monocular 2D->3D </a>,
	<a href="https://github.com/microsoft/MoGe">	MoGe (Monocular 2D->3D) </a>,
	<a href="https://github.com/kijai/ComfyUI-MoGe">	kijai/ComfyUI-MoGe </a>,
	<a href="https://huggingface.co/spaces/Ruicheng/MoGe">	MoGe demo	</a>, 
	<a href="https://www.microsoft.com/en-us/research/articles/magma-a-foundation-model-for-multimodal-ai-agents/">	Magma multimodal agents </a>,
	<a href="https://microsoft.github.io/Magma/">	GitHub - Magma </a>,
	<a href="https://www.microsoft.com/en-us/research/publication/peace-empowering-geologic-map-holistic-understanding-with-mllms/">	[GIS-LLM] PEACE: Empowering Geologic Map Holistic Understanding with MLLMs	</a>,
	<a href="https://github.com/The-AI-Alliance/GEO-Bench-VLM">	[GIS-LLM] GEO-Bench-VLM	</a>,
	<a href="https://huggingface.co/microsoft/bitnet-b1.58-2B-4T">	1-bit LLM	</a>,

<li/> MiniMaxi 
	<a href="https://www.minimaxi.com/en">	MiniMaxi	</a>,
	<a href="https://huggingface.co/MiniMaxAI">	MiniMaxAI	</a>,
	<a href="https://hailuoai.video/">		HailuoAI	</a>,
	<a href="https://github.com/MiniMax-AI/MiniMax-M2">	MiniMax-M2	</a>

<li/>	Mistral  <a href="https://huggingface.co/mistralai">	Mistral  </a>,
	<a href="https://mistral.ai/news/mistral-small-3/">	Mistral small-3  </a>
	<a href="https://mistral.ai/news/mistral-3">	Mistral Large 3 (instruct)	</a>

<li/> MoonshotAI 	<a href="https://github.com/MoonshotAI">	MoonshotAI </a>,
	<a href="https://moonshotai.github.io/Kimi-K2/">	Kimi K2: Open Agentic Intelligence	</a>

<li/> MoonValley	
	<a href="https://www.moonvalley.com/">	moonvalley	</a>,
	<a href="https://blog.comfy.org/p/marey-realism-v15-in-comfyui-built">	Marey Realism v1.5 in ComfyUI API	</a>,

<li/> NetEase Fuxi Lab	<a href="https://github.com/NetEase-FuXi?tab=repositories">	GitHub </a>

<li/> NTU
	<a href="https://github.com/yuvraj108c/ComfyUI_InvSR">	Inverse Super Resolution (InvSR)	</a>, 
	<a href="https://github.com/facebookresearch/EdgeTAM">	EdgeTAM: On-Device Track Anything Model </a>,
	<a href="https://github.com/lizhihao6/Sparc3D">	Sparc3D: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling (Math Magic)	</a>, 
	<a href="https://huggingface.co/spaces/ilcve21/Sparc3D">	ilcve21/Sparc3D (Math Magic)	</a>,
	<a href="https://www.hitem3d.ai/">	Hitem3D (use Sparc3D) </a>, 
	<a href="https://buaacyw.github.io/ultra3d/">	 Ultra3D: Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention (Math Magic)	</a>,
	<a href="https://github.com/zjx0101/ObjectClear">	ObjectClear: Complete Object Removal via Object-Effect Attention	</a>,
	<a href="https://github.com/Eyeline-Labs/CineScale">	CineScale: High-Resolution Cinematic Visual Generation </a>, 
	<a href="https://amap-ml.github.io/FE2E/">	FE2E: From Editor to Dense Geometry Estimator	</a>,
	<a href="https://fenghora.github.io/DiT360-Page/">	DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training	</a>, 
	<a href="https://kangliao929.github.io/projects/puffin/">	Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation	</a>, 
	<a href="https://holo-cine.github.io/">	HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives 	</a>, 
	<a href="https://physx-anything.github.io/">	PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image	</a>, 
	<a href="https://lightx-ai.github.io/">	Light-X : Generative 4D Video Rendering with Camera and Illumination Control	</a>, 
	<a href="https://vchitect.github.io/LongVie2-project/">	LongVie 2: Multimodal Controllable Ultra-Long Video World Model	</a>
	<a href="https://kevin-thu.github.io/StoryMem/">	StoryMem: Multi-shot Long Video Storytelling with Memory	</a>

<li/> NUS <a href="https://github.com/Yuanshi9815/OminiControl">	OminiControl: Minimal and Universal Control for Diffusion Transformer	</a>, 
	<a href="https://github.com/Xiaojiu-z/EasyControl">	EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer (Liblib AI)	</a>,
	<a href="https://github.com/showlab/OmniConsistency">	OmniConsistency: Learning Style-Agnostic Consistency from Paired Stylization Data </a>, 
	<a href="https://arxiv.org/html/2507.23268v2">	PixNerd: Pixel Neural Field Diffusion (pixel-space diffusion transformer for image generation without VAE)	</a>, 
	<a href="https://huggingface.co/lodestones/Chroma1-Radiance">		Chroma1-Radiance (based on PixNerd)	</a>, 
	<a href="https://biangbiang0321.github.io/SpotEdit.github.io/">	SpotEdit: Selective Region Editing in Diffusion Transformers	</a>

<li/> NuMind <a href="https://huggingface.co/numind">	NuMind</a>,
	<a href="https://huggingface.co/spaces/numind/NuExtract-1.5">	NuMind NuExtract-1.5</a>,

<li/> Nvidia  <a href="https://huggingface.co/nvidia">	Nvidia </a>,
	<a href="https://github.com/NVlabs">		Nvidia Labs (NVlabs)	</a>,
	<a href="https://github.com/NVlabs/Sana">		GitHub - NVlabs/Sana	</a>,
	<a href="https://github.com/Efficient-Large-Model/ComfyUI_ExtraModels">	GitHub - ComfyUI_ExtraModels	</a>,
	<a href="https://research.nvidia.com/labs/toronto-ai/DiffusionRenderer/">	DiffusionRenderer: Video Diffusion Models	</a>, 
	<a href="https://gaussiantracer.github.io/">	3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes	</a>, 
	<a href="https://github.com/nv-tlabs/3dgrut">	3D Gaussian Ray Tracing (3DGRT)	</a>, 
	<a href="https://huggingface.co/nvidia/GEN3C-Cosmos-7B">	GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control	(Monocular 2D->3D)	</a>,
	<a href="https://research.nvidia.com/labs/toronto-ai/vipe/">	ViPE: Video Pose Engine for 3D Geometric Perception	</a>, 
	<a href="https://huggingface.co/spaces/nvidia/audio-flamingo-3">	Audio Flamingo: Series of Advanced Audio Understanding Language Models	</a>


<li/> OpenAI  <a href="https://huggingface.co/openai">	OpenAI </a>,
	<a href="https://huggingface.co/spaces/openai/whisper">	OpenAI Whisper (transcribe / translate) </a>,

<li/> OpenBMB  
	<a href="https://github.com/OpenBMB/MiniCPM-V">	MiniCPM-V: GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone	</a>, 
	<a href="https://github.com/1038lab/ComfyUI-MiniCPM">	1038lab/ComfyUI-MiniCPM	</a>, 
	<a href="https://huggingface.co/openbmb/MiniCPM4-8B">	MiniCPM4-8B	</a>, 
	<a href="https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9"> 
MiniCPM-o </a>, 
	<a href="https://minicpm-omni-webdemo.internetofagents.net/">	MiniCPM-o demo </a>,
	<a href="https://github.com/OpenBMB/VoxCPM">	VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning	</a>, 
	<a href="https://github.com/OpenBMB/UltraEval-Audio">		UltraEval-Audio: A Unified Framework for Comprehensive Evaluation of Audio Foundation Models	</a>, 



<li/> OpenXLab  <a href="https://openxlab.org.cn/models?lang=en-US">	OpenXLab </a>,
	<a href="https://openxlab.org.cn/models">		Models </a>,

<li/> Ostris <a href="https://huggingface.co/ostris">	Ostris </a>,
	<a href="https://huggingface.co/ostris/OpenFLUX.1">	ostris/OpenFLUX.1 </a>,

<li id="mark"/>
	Rednote XiaoHongShu 
	<a href="https://github.com/rednote-hilab">	rednote-hilab </a>,
	<a href="https://huggingface.co/rednote-hilab">	 rednote-hilab  </a>,
	<a href="https://github.com/rednote-hilab/dots.llm1">	 Dots LLM </a>,
	<a href="https://fireredteam.github.io/demos/firered_tts_2/">	FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot	</a>, 
	<a href="https://github.com/FireRedTeam/FireRedTTS2">	FireRedTeam/FireRedTTS2 (multilingual)		</a>, 

<li/> PixelWave <a href="https://huggingface.co/mikeyandfriends/PixelWave_FLUX.1-dev_03/tree/main">		mikeyandfriends - PixelWave	</a>,
	<a href="https://civitai.com/user/humblemikey/models">		CivitAI - user - humblemikey (Art Style, PixelWave)	</a>

<li/>	RedNote - XiaoHongShu  
	<a href="https://github.com/rednote-hilab/dots.ocr">	dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model 	</a>, 
	<a href="https://github.com/instantX-research">	Xiaohongshu Instant ID Research	</a>,
	<a href="https://github.com/instantX-research/Regional-Prompting-FLUX">	Regional-Prompting-FLUX		</a>,
	<a href="https://arxiv.org/abs/2507.10605v2">	RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services	</a>, 


<li/> Resemble-ai
	<a href="https://huggingface.co/ResembleAI">	ResembleAI	</a>, 
	<a href="https://github.com/wildminder/ComfyUI-Chatterbox">	ComfyUI-Chatterbox	</a>,
	<a href="https://github.com/resemble-ai/perth">	Perth is a comprehensive Python library for audio watermarking and detection </a>, 
	<a href="https://huggingface.co/ResembleAI/chatterbox-turbo">	ResembleAI/chatterbox-turbo	</a>

<li/> SalesForce
	<a href="https://github.com/salesforce/LAVIS/tree/xgen-mm">		xGen-MM (BLIP-3): A Family of Open Large Multimodal Models	</a>

<li/> ServiceNow
	<a href="https://github.com/ServiceNow/drbench">		DrBench Enterprise Research Benchmark	</a>

<li/> Shakker-Labs <a href="https://huggingface.co/Shakker-Labs">		Shakker-Labs	</a>,
	<a href="https://openart.ai/workflows/reverentelusarca/flux-filmportrait-lora-by-shakkerlabs/Wp7nyj9LHcogIaHfMvAc">	flux-filmportrait-lora	</a>,
	<a href="https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro-2.0">	FLUX.1-dev-ControlNet-Union-Pro 2	</a>,
	<a href="https://huggingface.co/spaces/Shakker-Labs/FLUX-LoRA-Gallery">	FLUX-LoRA-Gallery	</a>, 
	<a href="https://huggingface.co/Shakker-Labs/AWPortrait-Z">	AWPortrait-Z	</a>, 

<li/> SkyworkAI <a href="https://github.com/SkyworkAI/SkyReels-V1">	SkyworkAI/SkyReels-V1 </a>,
	<a href="https://huggingface.co/Kijai/SkyReels-V1-Hunyuan_comfy">	Kijai/SkyReels-V1-Hunyuan_comfy </a>,
	<a href="https://skyworkai.github.io/skyreels-audio.github.io/">	SkyReels-Audio	</a>, 
	<a href="https://matrix-game-v2.github.io/">	Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model	</a>

<li/> Snap
	<a href="https://snap-research.github.io/EgoEdit/">	EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing	</a>

<li/> StabilityAI	<a href="https://huggingface.co/stabilityai">	StabilityAI	</a>

<li/> StableDiffusion	<a href="https://huggingface.co/stablediffusionapi">	StableDiffusionAPI	</a>


<li/> StepFun.AI	<a href="https://arxiv.org/abs/2504.17761">	Step1X-Edit: A Practical Framework for General Image Editing	</a>, 
	<a href="https://stepaudiollm.github.io/step-audio-editx/">	Step-Audio-EditX		</a>, 
	<a href="https://github.com/stepfun-ai/Step1X-Edit">	stepfun-ai/Step1X-Edit	</a>, 
	<a href="https://ace-step.github.io/">	ACE-Step: A Step Towards Music Generation Foundation Model </a>, 
	<a href="https://github.com/stepfun-ai/Step1X-3D">	Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets </a>, 
	<a href="https://github.com/stepfun-ai/Step-Audio2">	StepAudio2: Speech and Audio Understanding & Conversation </a>, 
	<a href="https://stepfun.ai/chats/new">	StepAudio Demo </a>, 
	<a href="https://huggingface.co/stepfun-ai/Step-Audio-2-mini">	Step-Audio-2-mini </a>, 

<li id="mark"/>
	Tencent <a href="https://github.com/Tencent/">	Tencent	</a>,
	<a href="https://github.com/orgs/Tencent-Hunyuan/repositories">	Tencent-Hunyuan	</a>,
 	<a href="https://huggingface.co/spaces/TencentARC/PhotoMaker-V2">	PhotoMaker	</a>,
	<a href="https://github.com/Tencent/MimicMotion">	MimicMotion	</a>,
	<a href="https://github.com/Tencent/HunyuanDiT/tree/main/comfyui-hydit">	ComfyUI - HunYuan	</a>,
	<a href="https://github.com/kijai/ComfyUI-HunyuanVideoWrapper">	HunYuan	</a>,
	<a href="https://dit.hunyuan.tencent.com/">	HunYuan	</a>,
	<a href="https://openart.ai/workflows/datou/hunyuan-video-720p/FCYqXugi8pVi3aqgeNZA">		OpenArt - datou/hunyuan-video-720p	</a>,
	<a href="https://openart.ai/workflows/cat_untimely_42/huanyuanvideo-video-to-video/25oGJWr3RYWRrtmaHkRV">		OpenArt - cat_untimely_42/huanyuanvideo-video-to-video	</a>,
	<a href="https://github.com/tencent-ailab/persona-hub">	tencent-ailab/persona-hub	</a>,
	<a href="https://huggingface.co/datasets/proj-persona/PersonaHub">	PersonaHub	</a>,
	<a href="https://github.com/Tencent/InstantCharacter">	InstantCharacter	</a>,
	<a href="https://github.com/jax-explorer/ComfyUI-InstantCharacter">	ComfyUI-InstantCharacter	</a>,
	<a href="https://openart.ai/workflows/t8star/instantcharacterid-/MwQZJ37vMqZqFjIKOWJP">	t8star/instantcharacterid	</a>,
	<a href="https://huggingface.co/tencent/HunyuanCustom">	HunyuanCustom, a multi-modal, conditional, and controllable generation model centered on subject consistency	</a>, 
	<a href="https://github.com/Tencent/HunyuanCustom">	Tencent/HunyuanCustom	(80gb) </a>,
	<a href="https://hunyuanvideo-avatar.github.io/">	HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters	</a>,
	<a href="https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1">		Hunyuan3D-2.1 (10Gb=Shape, 21Gb=Texture, 29Gb=Shape + Texture)		</a>
	<a href="https://arxiv.org/html/2506.15442v1">	Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material		</a>
	<a href="https://x-omni-team.github.io/">	X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again (==GPT4o)	</a>, 
	<a href="https://github.com/Francis-Rings/StableAvatar">	StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation	</a>, 
	<a href="https://github.com/smthemex/ComfyUI_StableAvatar">	smthemex/ComfyUI_StableAvatar		</a>, 
	<a href="https://github.com/Tencent-Hunyuan/SRPO">	SRPO: Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference (realism)	</a>,
	<a href="https://huggingface.co/wikeeyang/SRPO-for-ComfyUI">		wikeeyang/SRPO-for-ComfyUI	</a>,
	<a href="https://open-bee.github.io/">	Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs	</a>,
	<a href="https://imlixinyang.github.io/FlashWorld-Project-Page/">	FlashWorld: High-quality 3D Scene Generation within Seconds	</a>,
	<a href="https://huggingface.co/tencent/HunyuanOCR">		HunyuanOCR-1B	</a>,
	<a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0">	HY-Motion 1.0: Scaling Flow Matching Models for 3D Motion Generation	</a>, 

<li/>	Tsinghua <a href="https://huggingface.co/THUDM">	Tsinghua University Knowledge Engineering Group (KEG) & Data Mining 	</a>,
	<a href="https://huggingface.co/THUDM/CogVideoX-5b">	CogVideoX-5b	</a>,
	<a href="https://docs.google.com/spreadsheets/d/16eA6mSL8XkTcu9fSWkPSHfRIqyAKJbR1O99xnuGdCKY/edit?gid=0#gid=0">	CogVideoX models	</a>,
	<a href="https://www.stand-in.tech/">	Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation	</a>,
	<a href="https://zjp-shadow.github.io/works/UniRig/">	Unirig: Diverse Skeleton Rigging - One Model to Rig Them All		</a>,
 
<li/>	VAST-AI-Research 
	<a href="https://www.tripo3d.ai/">	tripo3d.AI	</a>,
	<a href="https://huggingface.co/VAST-AI">	HuggingFace	</a>, 
	<a href="https://huggingface.co/spaces/VAST-AI/TripoSG">	TripoSG - Image to 3D	</a>, 
	<a href="https://github.com/VAST-AI-Research/TripoSG">	Github - TripoSG	</a>, 
	<a href="https://huggingface.co/spaces/VAST-AI/MV-Adapter-I2MV-SDXL">	MV-Adapter [Image-to-Multi-View]	</a>,
	<a href="https://github.com/VAST-AI-Research">	Github	</a>, 
	<a href="https://medium.com/@thegodtripo">	Medium	</a>,
	<a href="https://github.com/flowtyone/ComfyUI-Flowty-TripoSR">	ComfyUI-Flowty-TripoSR	</a>,
	<a href="https://detailgen3d.github.io/DetailGen3D/">		DetailGen3D: Generative 3D Geometry Enhancement via Data-Dependent Flow	</a>, 
	<a href="https://huggingface.co/VAST-AI/DetailGen3D">		VAST-AI/DetailGen3D	</a>, 
	<a href="https://huggingface.co/spaces/VAST-AI/DetailGen3D">	DetailGen3D	demo </a>,
	<a href="https://zjp-shadow.github.io/works/UniRig/">	One Model to Rig Them All: Diverse Skeleton Rigging with UniRig </a> 

<li/>	ViVago
	<a href="https://vivago.ai/home">	ViVago	</a>, 
	<a href="https://github.com/HiDream-ai/HiDream-E1">	HiDream t2i	</a>, 
	<a href="https://modelscope.cn/models/AI-ModelScope/HiDream-E1-Full">	modelscope/HiDream-E1-Full	</a>, 
	<a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/">	ComfyUI_examples/hidream </a>, 
	<a href="https://openart.ai/workflows/datou/hidream-e1/oMjrZlGRAtZMP41jxpBk">	datou/hidream-e1	</a>, 
	<a href="https://docs.comfy.org/tutorials/advanced/hidream">	HiDream t2i	</a>, 
	<a href="https://docs.comfy.org/tutorials/image/hidream/hidream-e1">	HiDream e1	</a>, 
	<a href="https://docs.comfy.org/tutorials/video/wan/wan-video">	Wan t2v	</a>, 
	<a href="https://comfyui.org/en/text-to-video-wanvideo-controlnet">	text-to-video wanvideo-controlnet	</a>, 
	<a href="https://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1">	text-to-music ace-step-v1	</a>, 
	<a href="https://diffusiondoodles.substack.com/p/quick-thoughts-on-hidream-i1-and">	Quick Thoughts on HiDream-I1 & E1	</a>

<li/>	VisionATrix <a href="https://visionatrix.github.io/">	VisionATrix	</a>,
	<a href="https://github.com/Visionatrix/ComfyUI-PhotoMaker-Plus">	PhotoMaker-Plus	</a>

<li/>	Vision-xl <a href="https://vision-xl.github.io/supple/">		vision-xl	</a>

<li/>	Sina weibo <a href="https://github.com/WeiboAI/VibeThinker">		VibeThinker: Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B	</a>

<li/>	XiaoMi <a href="https://huggingface.co/XiaomiMiMo">	MiMo Audio: Audio Language Models are Few-Shot Learners	</a>, 
	<a href="https://github.com/XiaomiMiMo/MiMo-Audio">	XiaomiMiMo/MiMo-Audio	</a>, 
	<a href="https://mimo.xiaomi.com/blog/mimo-v2-flash">		MiMo-V2-Flash	</a>

<li/>	XLabs-AI <a href="https://huggingface.co/XLabs-AI">	XLabs-AI	</a>,
	<a href="https://huggingface.co/XLabs-AI/flux-controlnet-collections">	flux-controlnet-collections	</a>,
	<a href="https://github.com/XLabs-AI/x-flux-comfyui">	GitHub - XLabs-AI/x-flux-comfyui	</a>

<li/>	Zyphra <a href="https://www.zyphra.com/post/beta-release-of-zonos-v0-1">	Zyphra	</a>, 
	<a href="https://playground.zyphra.com/">	Zyphra playground	</a>, 
	<a href="https://www.zyphra.com/post/beta-release-of-zonos-v0-1/">	Zonos v01 Speech TTS	</a>, 


<li/>	ZhiPu Z.AI <a href="https://z.ai/blog/glm-4.5">	GLM-4.5: Reasoning, Coding, and Agentic Abililties	</a>, 
	<a href="https://chat.z.ai/s/2a9a1a90-545b-4f29-b6ac-854539dcc323">	Flappy Bird	</a>
	<a href="https://chat.z.ai/s/f8c2f383-51d4-40b8-82e5-63529eaa00db">	Pokemon webapp - Pokedex	</a>,
	<a href="https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b">	zai-org	</a>,
	<a href="https://teal024.github.io/SCAIL/">	SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations	</a>
	<a href="https://xiao9905.github.io/AutoGLM/blog.html">	AutoGLM	</a>, 
	<a href="https://github.com/zai-org/RealVideo">		RealVideo: A Real-Time Streaming Conversational System Powered by Autoregressive Diffusion Video Generation	</a>

<li/>	<a href="https://werewolf.foaster.ai/">	Probing LLM Social Intelligence via Werewolf	</a>, 
	<a href="https://arxiv.org/html/2407.13943v1">	Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction	</a>

</ol>
<!----------------------------------------------------------------------------->
<!------------------------------------COLUMN 2--------------------------------->
<!----------------------------------------------------------------------------->
</td><td width="30%"><!-------------------------------------------------------->
<h3 id="tagBenchmark">	Benchmarks & Leaderboards		</h3>
<li/>	Keywords: <a href="https://www.inoreader.com/stream/user/1005506540/tag/Benchmarking%20%E2%9C%85/view/html?cs=m">	InoReader - Benchmarking	</a>,
	<br/><ol start=1 type=1>

<li/>	<a href="https://www.confident-ai.com/blog/the-current-state-of-benchmarking-llms">	Intro to LLM Benchmarking	</a>

<li/>	<a href="https://arxiv.org/html/2506.01061v1">	AceVFI: A Comprehensive Survey of Advances in Video Frame Interpolation	 (VFI)	</a>,
	<a href="https://github.com/CMLab-Korea/Awesome-Video-Frame-Interpolation">	Awesome-Video-Frame-Interpolation	</a>

<li/>	<a href="https://huggingface.co/spaces/galileo-ai/agent-leaderboard">	Agent Leaderboard	</a>

<li/>	<a href="https://github.com/THUDM/AgentBench">	AgentBench: Evaluating LLMs as Agents 	</a>

<li/>	<a href="https://github.com/ljcleo/agent_sense">	AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios	</a>, 
	<a href="https://arxiv.org/abs/2410.19346">	Paper	</a>

<li/>	AllenAI
	<a href="https://huggingface.co/spaces/allenai/reward-bench">	RewardBench: Evaluating Reward Models	</a>, 
	<a href="https://huggingface.co/spaces/allenai/ZebraLogic">	ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning	</a>, 
	<a href="https://huggingface.co/spaces/allenai/super_leaderboard">	SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories	</a>, 
	<a href="https://huggingface.co/spaces/allenai/ZeroEval">	ZeroEval: Benchmarking LLMs for Reasoning	</a>, 
	<a href="https://huggingface.co/spaces/allenai/WildBench">	WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild	</a>, 

<li/>	Artificial Analysis 
	<a href="https://artificialanalysis.ai/text-to-image/arena/leaderboard-text">	Text to Image Leaderboard	</a>, 
	<a href="https://huggingface.co/spaces/ArtificialAnalysis/LLM-Performance-Leaderboard">	LLM-Performance-Leaderboard	</a>

<li/>	Arize <a href="https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/summarization-eval">		Text - Arize Phoenix </a>

<li/>	Audio	<a href="https://github.com/MoonshotAI/Kimi-Audio-Evalkit/blob/master/LEADERBOARD.md">	Audio - Kimi-Audio-Evalkit	</a>

<li/>	<a href="https://github.com/and-mill/Awesome-GenAI-Watermarking">	Awesome-GenAI-Watermarking	</a>

<li/>	Coding 
	<a href="https://www.sonarsource.com/the-coding-personalities-of-leading-llms/leaderboard/">	LLM Leaderboard for Code Quality & Security	</a>, 
	<a href="https://www.vellum.ai/best-llm-for-coding">	Coding LLM Leaderboard (e.g. Kimi K2)	</a>, 

<li/>	<a href="https://github.com/xxyQwQ/ComfyBench">	ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems	</a>

<li/>	<a href="https://arxiv.org/abs/2512.02038">	Deep Research: A Systematic Survey	</a>

<li/>	<a href="https://huggingface.co/spaces/muset-ai/DeepResearch-Bench-Leaderboard">	DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents	</a>

<li/>	<a href="https://deep-research-survey.github.io/">	A Systematic Survey of Deep Research	</a>
	
<li/>	<a href="https://huggingface.co/spaces/gaia-benchmark/leaderboard">	GAIA General Agent Leaderboard (e.g. Manus)	</a>
	<a href="https://openreview.net/forum?id=fibxvahvs3">	GAIA: a benchmark for General AI Assistants	</a>

<li id="mark"/>	
	ImagenWorld
	<a href="https://tiger-ai-lab.github.io/ImagenWorld/">	ImagenWorld: Stress-Testing Image Generation Models with Explainable Human Evaluation on Open-ended Real-World Tasks		</a>,
	<a href="https://blog.comfy.org/p/introducing-imagenworld">		Introducing ImagenWorld: A Real World Benchmark for Image Generation and Editing	</a>, 
	<a href="https://huggingface.co/spaces/TIGER-Lab/ImagenWorld-Visualizer">	TIGER-Lab/ImagenWorld-Visualizer	</a>, 
	<a href="https://huggingface.co/datasets/TIGER-Lab/ImagenWorld">	TIGER-Lab/ImagenWorld	</a>

<li/>	Inferless <a href="https://huggingface.co/spaces/Inferless/Open-Source-TTS-Gallary">	Open-Source Text-to-Speech Model Gallery	</a>

<li/>	<a href="https://lifearchitect.ai/models-table/">	LifeArchitect.ai	</a>

<li/>	LMArena	<a href="https://lmarena.ai/leaderboard/search ">	LMArena.ai/leaderboard	</a>

<li id="mark"/>	Meituan	<a href="https://vitabench.github.io/">		VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications	</a>,

<li/>	OpenLLM	<a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">	Open LLM Leaderboard	</a>

<li/>	OpenCompass <a href=https://opencompass.org.cn/leaderboard-llm">	OpenCompass LLM Leaderboard	</a>, 
	<a href="https://opencompass.org.cn/leaderboard-multimodal">	OpenCompass multi-modal Leaderboard	</a>

<li/>	<a href="https://huggingface.co/spaces/philschmid/llm-pricing">	philschmid/LLM Pricing	</a>

<li/>	<a href="https://arxiv.org/html/2410.21311v1">	MMDocBench: benchmarking large vision-language models for fine-grained visual document understanding	</a>

<li/>	<a href="https://os-agent-survey.github.io/">	OS Agents: A Survey on MLLM-based Agents for Computer, Phone and Browser Use	</a>

<li/>	Princeton <a href="https://hal.cs.princeton.edu/">	HAL: Holistic Agent Leaderboard	</a>,
	<a href="https://arxiv.org/abs/2510.11977">	Holistic Agent Leaderboard: The Missing Infrastructure For Ai Agent Evaluation	</a>

<li/>	<a href="https://arena.hume.ai/">	Speech - Hume - Expressive TTS Arena	</a>

<li/>	<a href="https://au-harness.github.io/">	Speech - ServiceNow: AU-Harness: An Open-Source Toolkit for Holistic Evaluation of Audio LLMs	</a>

<li/>	<a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena-V2">	Speech - TTS-Arena	</a>

<li/>	<a href="https://github.com/OpenBMB/UltraEval-Audio">		Speech - UltraEval-Audio: A Unified Framework for Comprehensive Evaluation of Audio Foundation Models (OpenBMB)	</a>


<li/>	Stanford	<a href="https://crfm.stanford.edu/fmti/">	Foundation Model Transparency Index	</a>, 
		<a href="https://crfm.stanford.edu/helm/latest/?models=1">	Stanford Holistic Evaluation of Language Models (HELM)	</a>

<li/>	<a href="https://huggingface.co/spaces/ArtificialAnalysis/Text-to-Image-Leaderboard">	Text-to-Image-Leaderboard	</a>
<li/>	<a href="https://huggingface.co/spaces/ArtificialAnalysis/Video-Generation-Arena-Leaderboard">		Video-Generation-Arena-Leaderboard	</a>

<li/>	<a href="https://huggingface.co/spaces/TTS-AGI/TTS-Arena">	Text-To-Speech - TTS-AGI/TTS-Arena	</a>

<li/>	<a href="https://huggingface.co/spaces/Vchitect/VBench_Leaderboard">	Text-To-Video - Vchitect/VBench_Leaderboard	</a>
<li/>	<a href="https://huggingface.co/datasets/saiyan-world/Goku-MovieGenBench">	Video - MovieGenBench	</a>

<li/>	<a href="https://klu.ai/glossary/llm-evaluation">	Text - klu Evaluation Guide	</a>
<li/>	<a href="https://github.com/confident-ai/deepeval">	Text - DeepEval	</a>	
 
<li/>	<a href="https://ossinsight.io/collections/stable-diffusion-ecosystem/">	Stable Diffusion Ecosystem	</a>

<li/>	<a href="https://mmsearch.github.io/">	MMSearch	</a>


<li/>	<a href="https://github.com/CLUEbenchmark/SuperCLUE">	SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark	</a>

<li/>	<a href="https://github.com/Tencent-Hunyuan/ArtifactsBenchmark">		Tencent-Hunyuan/ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation	</a>

<li/>	<a href="https://huggingface.co/spaces/Vchitect/VBench_Leaderboard">	VBench : Comprehensive Benchmark Suite for Video Generative Models	</a>

<li/>	Detection <a href="https://matrix.tencent.com/ai-detect/ai_gen">	Tencent ai-detect	</a>,
	<a href="https://hivemoderation.com/ai-generated-content-detection">	Hive ai-generated-content-detection	</a>

<li/>	<a href="https://github.com/bytedance/comfyui-lumi-batcher">	ComfyUI-Lumi-Batcher tool like XYZ Plot in WebUI	</a>

</ol>
<!----------------------------------------------------------->
<h3 id="tagFaces">	Face Restoration & Realism			</h3>
<li/>	Keywords: <a href="https://openart.ai/workflows/all?keyword=FaceSwap">	OpenArt - FaceSwap	</a>,
	<a href="https://openart.ai/workflows/all?keyword=Realistic">	OpenArt - Realistic	</a>,
	<a href="https://www.1ai.net/en/tag/ai%e6%8d%a2%e8%84%b8">	1ai	</a>
	<br/><ol start=1 type=1>

<li/>	<a href="https://luoxyhappy.github.io/CanonSwap/">	CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation	</a>

<li/>	<a href="https://medium.com/design-bootcamp/ai-face-swap-battle-pulid-vs-instantid-vs-faceid-2f08db230509">	Compare PuLID vs InstantID vs FaceID	</a>

<li/> <a href="https://github.com/Gourieff/comfyui-reactor-node">		GitHub - Gourieff (ReActor Node for ComfyUI)	</a>,
	<a href="https://github.com/somanchiu/ReSwapper">	somanchiu/ReSwapper	</a>,
	<a href="https://civitai.com/models/256399/face-swap-for-2-people-with-faceid-and-reactor">		faceswap	</a>

<li/>	<a href="https://openart.ai/workflows/grinlau/change-face-v23/4RJFsFp9oXdlP9rty2cI">	OpenArt - grinlau/change-face-v23	</a>

<li/>	<a href="https://huggingface.co/GuijiAI/ReHiFace-S">	HuggingFace - GuijiAI/ReHiFace-S	</a>

<li/>	<a href="https://openart.ai/workflows/myaiforce/face-swapping-ecomid-vs-flux-pulid-vs-instantid/2ATyK62dutoPVCevX8o5">	OpenArt - myaiforce/face-swapping-ecomid-vs-flux-pulid-vs-instantid	</a>

<li/>	<a href="https://github.com/sipie800/ComfyUI-PuLID-Flux-Enhanced">	GitHub - sipie800/ComfyUI-PuLID-Flux-Enhanced	</a>

<li/>	<a href="https://openart.ai/workflows/JakazCfWF9UWi2mOZTlf">	OpenArt - Andrea Baioni - Plastic Skin Solver 	</a>,
	<a href="https://www.youtube.com/watch?v=Ta8GpvgmJMo">	YouTube	</a>

<li/>	<a href="https://github.com/cubiq/ComfyUI_FaceAnalysis">	GitHub - cubiq/ComfyUI_FaceAnalysis	</a>,
	<a href="https://github.com/jordoh/ComfyUI-Deepface/">	GitHub - jordoh/ComfyUI-Deepface/	</a>

<li/>	<a href="https://github.com/djbielejeski/a-person-mask-generator">	GitHub - Person Mask Generator	</a>

<li/> <a href="https://huggingface.co/alexgenovese/facerestore/tree/main">	alexgenovese/facerestore	</a>
<li/>	<a href="https://github.com/modelscope/facechain">	modelscope/facechain	</a>

<li/>	Face Restoration <a href="https://github.com/Hillobar/Rope">	Pearl Rope	</a>,
	<a href="https://github.com/s0md3v/roop">	Roop </a>,
	<a href="https://github.com/iperov/DeepFaceLive">	DeepFaceLive	</a>,
	<a href="https://github.com/neuralchen/SimSwap">	SimSwap	</a>,
	<a href="https://github.com/deepfakes/faceswap">	deepfakes/faceswap	</a>

<li/>	<a href="https://huggingface.co/facefusion">	FaceFusion	</a>,
	<a href="https://huggingface.co/datasets/dimanchkek/Deepfacelive-DFM-Models">	Deepfacelive-DFM-Models	</a>

<li/>	<a href="https://github.com/Gourieff/comfyui-reactor-node">	Gourieff - comfyui-reactor-node	</a>

<li/>	<a href="https://github.com/djbielejeski/a-person-mask-generator">	GitHub - Person Mask Generator	</a>

<li/>	<a href="https://superhero-7.github.io/DreamID/">	DreamID - A Fast and High-Fidelity diffusion-based Face Swapping via Triplet ID Group Learning	</a>

</ol>
<!----------------------------------------------------------->
<h3 id="tagCaption">	Image-To-Text (i2t) Captioning		</h3>
<li/>	Keywords: <a href="https://openart.ai/workflows/all?keyword=Caption">	OpenArt - Caption 	</a><br/><ol start=1 type=1>

<li/>	<a href="https://cychenyue.com/28204.html">	AllenAI Molmo 7B D		</a>
<li/>	<a href="https://github.com/StartHua/Comfyui_CXH_joy_caption">	Joy Caption	</a>
	<a href="https://openart.ai/workflows/leeguandong/joy-caption-batch-caption/6btLPOJKoSBFmuM2kfRW">	Joy caption batch caption	</a>

<li/>	<a href="https://huggingface.co/microsoft">	Microsoft 	</a>,
	<a href="https://github.com/kijai/ComfyUI-Florence2">	Microsoft Florence2	</a>,
	<a href="https://github.com/spacepxl/ComfyUI-Florence-2">	Florence-2	</a>,
	<a href="https://huggingface.co/MiaoshouAI">	MiaoshouAI	</a>,
	<a href="https://github.com/miaoshouai/ComfyUI-Miaoshouai-Tagger">	ComfyUI-Miaoshouai-Tagger	</a>,
	<a href="https://openart.ai/workflows/toucan_chilly_4/florence-run-batch-capture-prompt-maker/tmkxPTDT7sL2EuZgaQJe">	OpenArt  - Florence Run Batch Capture Prompt Maker	</a>

<li/> <a href="https://github.com/alexisrolland/ComfyUI-Phi">	 Microsoft Phi - alexisrolland/ComfyUI-Phi (Phi-3.5-mini-instruct, Phi-3.5-vision-instruct)	</a>,
	<a href="https://github.com/StartHua/Comfyui_CXH_Phi_3.5">	Phi 3.5	</a>,

<li/>	<a href="https://github.com/CY-CHENYUE/ComfyUI-MiniCPM-Plus">	MiniCPM-Plus	</a>,
	<a href="https://github.com/pzc163/Comfyui_MiniCPMv2_6-prompt-generator">	MiniCPM v2.6 Prompt Generator	</a>

<li/> <a href="https://moondream.ai/playground">	Moondream (Visual Q&A, Caption, Object Detection) </a>,
	<a href="https://moondream.ai/blog/">		Moondream blog	</a>,
	<a href="https://huggingface.co/vikhyatk/moondream2">		vikhyatk/moondream2	</a>,
	<a href="https://github.com/vikhyat/moondream">		vikhyat/moondream	</a>,
	<a href="https://github.com/kijai/ComfyUI-moondream">	kijai/ComfyUI-moondream </a>,
	<a href="https://github.com/Hangover3832/ComfyUI-Hangover-Moondream">	Hangover3832/ComfyUI-Hangover-Moondream </a>

<li/>	<a href="https://nexa.ai/blogs/OmniVLM">	OmniVLM-968M (no ComfyUI)	</a>
<li/>	<a href="https://github.com/SeanScripts/ComfyUI-PixtralLlamaMolmoVision">	Pixtral Llama Molmo Vision	</a>

<li/>	<a href="https://github.com/BesianSherifaj-AI/PromptCraft">		PromptCraft	</a>

<li/>	<a href="https://yejy53.github.io/RealGen/">	RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards (qwen-edit-2509 LORA)	</a>

<li/> 
	<a href="https://github.com/1038lab/ComfyUI-QwenVL">		QwenVL for ComfyUI (image & video)		</a>,
	<a href="https://github.com/IuvenisSapiens/ComfyUI_Qwen2-VL-Instruct">		Qwen2-VL-Instruct	</a>,
	<a href="https://openart.ai/workflows/leeguandong/qwen2-vlchat_with_multiple_images/qTFHeJbzMRcwYJKK4Qwb">	QWEN multiple images	</a>,
	<a href="https://openart.ai/workflows/leeguandong/qwen2-vl-chat_with_single_image/AEgiUMjzw9H580CXgrXD">	QWEN single_image 	</a>,
	<a href="https://openart.ai/workflows/comfyuiblog/convert-video-and-images-to-text-using-qwen2-vl-model/OdlbuCnxVfjgD4MVsaMR">	vi2t	</a>

<li/>	<a href="https://github.com/SeargeDP/ComfyUI_Searge_LLM">	Searge-LLM </a>
<li/>	<a href="https://github.com/pythongosssss/ComfyUI-WD14-Tagger">	WD14-Tagger	</a>

<li/>	<a href="https://huggingface.co/spaces/gokaygokay/FLUX-Prompt-Generator">	gokaygokay/Flux Prompt Generator	 </a>,
	<a href="https://huggingface.co/spaces/gokaygokay/Flux-Florence-2">	Flux-Florence-2	 </a>,
	<a href="https://github.com/fairy-root/Flux-Prompt-Generator/tree/main">	fairy-root	</a>,

<li/> <a href="https://github.com/IuvenisSapiens">		IuvenisSapiens (miniCPM, QWEN, QWEN Audio)	</a>

<li/>	<a href="https://open.bigmodel.cn/">	Zhipu GLM	</a>,
	<a href="https://github.com/JcandZero/ComfyUI_GLM4Node">	GitHub - JcandZero/ComfyUI_GLM4Node </a>,
	<a href="https://github.com/Nojahhh/ComfyUI_GLM4_Wrapper">	GitHub - Nojahhh/ComfyUI_GLM4_Wrapper </a>,

<li/>	Workflows : <a href="https://openart.ai/workflows/cxh/recommended-based-on-comfyui-node-picturesjoy_caption-minicpmv2_6-prompt-generator-florence2/rUVWkZB5zGOkjXGhsrON">	joy_caption-minicpmv2_6-prompt-generator-florence2	</a>
	<a href="https://openart.ai/workflows/owl_glaring_95/flux_img2img-flux-florence-auto-prompt-generator/3aLDgY2oScWNSDZUvNVn">	florence	</a>


<li/>	<a href="https://openart.ai/workflows/comfyui_llm_party/llm_party-for-local-models/UkynnzaQxhuSwPDTFub7">	llm_party-for-local-models (QWEN)	</a>

</ol>
<!----------------------------------------------------------->
<h3 id="tagModel">	Models			</h3>
<li/>	Keywords: <a href="https://huggingface.co/models?pipeline_tag=text-generation&sort=trending">	HuggingFace - text-generation	</a>,
	<a href="https://www.inoreader.com/stream/user/1005506540/tag/GenAI%20-%20Algorithm%20%2B%20Model%20%E2%9C%85/view/html?cs=m">	InoReader - Algorithm	</a>,
	<br/><ol start=1 type=1>

<li/> <a href="https://www.aimodels.fyi/models">	AIModels.fyi	</a>

<li/> <a href="https://huggingface.co/Comfy-Org/models">	Comfy-Org	</a>

<li/> <a href="https://huggingface.co/models">	HuggingFace	</a>
	
<li/> <a href="https://modelscope.cn/models">	ModelScope	</a>

<li/>	AlexGeNovese <a href="https://huggingface.co/alexgenovese/checkpoint/tree/main">	checkpoint	</a>, 
	<a href="https://huggingface.co/alexgenovese/clip/tree/main">		clip	</a>, 
	<a href="https://huggingface.co/alexgenovese/clip_vision/tree/main">	clip_vision	</a>, 
	<a href="https://huggingface.co/alexgenovese/controlnet/tree/main">	controlnet	</a>,
	<a href="https://huggingface.co/alexgenovese/facerestore/tree/main">	facerestore	</a>, 
	<a href="https://huggingface.co/alexgenovese/ipadapters/tree/main">	ipadapters	</a>, 
	<a href="https://huggingface.co/alexgenovese/loras/tree/main">		loras	</a>, 
	<a href="https://huggingface.co/alexgenovese/sams/tree/main">		sams	</a>, 
	<a href="https://huggingface.co/alexgenovese/vae/tree/main">		vae	</a>,
	<a href="https://huggingface.co/alexgenovese/ultralytics/tree/main">	ultralytics	</a>

<li/>	city96 GGUF<a href="https://huggingface.co/city96/Qwen-Image-gguf">	Qwen-Image	</a>,
	<a href="https://huggingface.co/city96/LTX-Video-0.9.6-distilled-gguf">	LTX	</a>,
	<a href="https://huggingface.co/city96/HunyuanVideo-I2V-gguf">	HunyuanVideo-I2V	</a>

<li/>	DiffBot <a href="https://github.com/diffbot/diffbot-llm-inference">	diffbot-llm-inference	</a>, 
	<a href="https://diffy.chat/">		diffy.chat	demo </a>


<li/>	HuiHui-ai
	<a href="https://huggingface.co/collections/huihui-ai/qwen3-vl-abliterated">	Huihui-Qwen3-VL-8B-Instruct-abliterated	</a>, 

<li/>	<a href="https://github.com/River-Zhang/ICEdit">	In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer	</a>,
	<a href="https://huggingface.co/spaces/RiverZ/ICEdit">	spaces/RiverZ/ICEdit	</a>

<li/>	<a href="https://huggingface.co/models?pipeline_tag=image-to-video&p=0&sort=trending">	image-to-video	</a>
<li/>	<a href="https://huggingface.co/impactframes">	impactframes	</a>
<li/>	<a href="https://huggingface.co/jasperai">	jasperai	</a>

<li/>	Phips<a href="https://huggingface.co/Phips">	Phips/upscalers </a>,
	<a href="https://huggingface.co/spaces/Phips/Upscaler">	demo	</a>

<li/>	<a href="https://github.com/IamCreateAI/Ruyi-Models">	IamCreateAI/Ruyi	</a>,
	<a href="https://openart.ai/workflows/leeguandong/ruyi-text2img/unHmvTTdC0QQoGUyWImz">	OpenArt - leeguandong/ruyi-text2img	</a>

<li/>	<a href="https://chenglin-yang.github.io/1.58bit.flux.github.io/">	ByteDance - 1.58-bit FLUX	</a>

<li/>	<a href="https://huggingface.co/HFforLegal">	Hugging Face for Legal	</a>,
	<a href="https://huggingface.co/datasets/HFforLegal/laws">	HFforLegal/datasets	</a>,

<li/>	<a href="https://github.com/cubiq/ComfyUI_IPAdapter_plus">	IPAdapter (FaceID, clip-vision, LORA)	</a>

<li/>	Kijai	<a href="https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Skyreels">	Skyreels	</a>,
	<a href="https://huggingface.co/Kijai/LTXV/tree/main">	LTXV	</a>,
	<a href="https://huggingface.co/Kijai/HunyuanVideo_comfy/tree/main">	HunyuanVideo	</a>,

<li/>	MonsterMMORPG	<a href="https://huggingface.co/MonsterMMORPG/Wan_GGUF/tree/main">	Wan - GGUF	</a>,
	<a href="https://huggingface.co/MonsterMMORPG/BestImageUpscalers/tree/main">	Upscale	</a>,
	<a href="https://huggingface.co/MonsterMMORPG/FaceSegments/tree/main">	FaceSegments`			</a>,
	<a href="https://huggingface.co/MonsterMMORPG/YoloModels/tree/main">	Yolo	</a>

<li/>	Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) , UAE	<a href="https://ifm.mbzuai.ac.ae/">		Institute of Foundation Models (IFM)	</a>, 
	<a href="https://huggingface.co/inceptionai/Llama-3.1-Sherkala-8B-Chat">	Sherkala (English, Russian, and Turkish)	</a>, 
	<a href="https://www.k2think.ai/k2think">		K2-Think	</a>, 


<li/>	Ostris	<a href="https://huggingface.co/ostris/qwen_image_edit_inpainting">	qwen_edit_inpainting		</a>

<li/>	PowerInfer	<a href="https://huggingface.co/PowerInfer">	</a>,
	<a href="https://github.com/SJTU-IPADS/SmallThinker">	SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local	</a>, 
	<a href="https://arxiv.org/abs/2507.20984">	Paper	</a>

<li/>	QuantStack GGUF <a href="https://huggingface.co/QuantStack/Wan2.2-I2V-A14B-GGUF">	Wan2.2-I2V-A14B	</a>,
	<a href="https://huggingface.co/QuantStack/Qwen-Image-Distill-GGUF">	Qwen-Image-Distill	</a>,
	<a href="https://huggingface.co/QuantStack/FLUX.1-Kontext-dev-GGUF">	FLUX.1-Kontext-dev	</a>,
	<a href="https://huggingface.co/QuantStack/LTXV-13B-0.9.8-distilled-GGUF">	LTXV-13B-0.9.8-distilled	</a>,
	<a href="https://huggingface.co/QuantStack/Wan2.1_I2V_14B_FusionX-GGUF">	Wan2.1_I2V_14B_FusionX	</a>

<li/>	Reaslim <a href="https://tensor.art/models/808406540089546518/Extra-Realistic-Flux-v1">	TensorArt - Extra-Realistic-Flux	</a>,
	<a href="https://tensor.art/u/614170213247779888/models">	TensorArt - kg_09	</a>

<li/>	StrangerZone <a href="https://huggingface.co/strangerzonehf">	StrangerZone LORA (Flux-Super-Realism-LoRA, Super 3D - Engine) 	</a>

<li/>	<a href="https://huggingface.co/swiss-ai">	Swiss-AI - Apertus	</a>.
	<a href="https://www.swiss-ai.org/about-1">	Swiss-AI - Projects	</a>

<li/>	<a href="https://huggingface.co/collections/mit-han-lab/svdquant-67493c2c2e62a1fc6e93f45c">	SVDQuant	</a>, 
	<a href="https://github.com/mit-han-lab/ComfyUI-nunchaku">	mit-han-lab/ComfyUI-nunchaku	</a>

<li/>	<a href="https://huggingface.co/TheBloke">	TheBloke (>4K)	</a>

<li/>	<a href="https://tilde.ai/tildeopen-llm/">	TildeOpen LLM: Europe's Sovereign Multilingual AITildeOpen LLM: Europe's Sovereign Multilingual AI	</a>, 
	<a href="https://huggingface.co/TildeAI/TildeOpen-30b">	TildeAI/TildeOpen-30b	</a>

<li/>	Unsloth.ai 
	<a href="https://unsloth.ai/blog">	Unsloth.ai </a>, 
	<a href="https://huggingface.co/unsloth">	UnSloth (>300)	</a>, 
	<a href="https://github.com/unslothai/unsloth">	GitHub - UnSloth AI	</a>, 
	<a href="https://huggingface.co/collections/unsloth/deepseek-v3-all-versions-677cf5cfd7df8b7815fc723c">	unsloth/deepseek-v3	</a>,
	<a href="https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa">	phi-4-all-versions	</a>, 
	<a href="https://unsloth.ai/blog/qwen3"> Fine-tune & Run Qwen3	</a>, 
	<a href="https://unsloth.ai/blog/tts">	Fine-tuning TTS models (Sesame's CSM, Orpheus)	</a>

</ol>
<hr><!----------------------------------------------------------->
<h3 id="tagPlatform">	GenAI - Platforms	</h3><ol start=1 type=1>

<li/>	<a href="https://wan.video/">	Alibaba GenAI Platform	</a>


<li/>	<a href="https://www.argil.ai/ai-influencers">	argil.ai: AI influencers	</a>,
	<a href="https://fal.ai/models/argil/avatars/text-to-video">	fal.ai: argil/avatars	</a>

<li/>	<a href="https://medium.com/@dminhk/3-easy-steps-to-run-comfyui-on-amazon-sagemaker-notebook-c9bdb226c15e">	Amazon SageMaker	</a>
<li/>	<a href="https://animemaker.graydient.ai/concepts">	animemaker	</a>

<li/>	<a href="https://www.baseten.co/blog/how-to-serve-your-comfyui-model-behind-an-api-endpoint/">	baseten	</a>

<li/>	<a href="https://dreamina.capcut.com/ai-tool/explore">	ByteDance GenAI Platform	</a>


<li/>	<a href="https://datacrunch.io/managed-endpoints/flux-kontext">	DataCrunch.io	</a>

<li/>	<a href="https://ltx.studio/platform/ai-avatars">	LighTricks - LTX Studio	</a>

<li/>	<a href="https://www.mage.space/">	mage.space	</a>

<li/>	<a href="https://www.mimicpc.com/workflows">	MimicPC	</a>,
	<a href="https://www.mimicpc.com/learn">	Learn	</a>,

<li/>	<a href="https://nexa.ai/models">	nexa.ai (on-device models & inference	</a>
	<a href="https://nexa.ai/models">	Supported Models	</a>

<li/>	<a href="https://openart.ai/pricing">	openart	</a>

<li/>	<a href="https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b">	OpenRouter (who use & how much)	</a>

<li/>	Perplexity <a href="https://huggingface.co/perplexity-ai/r1-1776">	DeepSeek R1 1776	</a>

<li/>	<a href="https://rendernet.ai/">	RenderNet.ai	</a>

<li/>	<a href="https://replicate.com/explore">	Replicate	</a>
	<a href="https://github.com/replicate/comfyui-replicate/blob/main/supported_models.json">	supported models	</a>
	<a href="https://github.com/replicate/comfyui-replicate/tree/main">	comfyui-replicate	</a>

<li/>	<a href="https://runware.ai/models#image-flux">	RunWare.ai </a>

<li/>	<a href="https://www.runcomfy.com/">	RunComfy	</a>
<li/>	<a href="https://www.runninghub.ai/workflows">	RunningHub	</a>
<li/>	<a href="https://blog.runpod.io/how-to-get-stable-diffusion-set-up-with-comfyui-on-runpod/">	runpod.ai	</a>

<li/>	<a href="https://www.segmind.com/pixelflow/templates">	SegMind - PixelFlow	</a>

<li/>	<a href="https://www.shakker.ai/online-comfyui">	Shakker.ai	</a>

<li/>	<a href="https://sinkin.ai/?models=1">	sinkin	</a>

<li/>	<a href="https://tensor.art/">	tensor.art	</a>

<li/>	<a href="https://www.thinkdiffusion.com/#apps-comfyui">	ThinkDiffusion	</a>,
	<a href="https://civitai.com/articles/3244/comfyui-workflows-and-what-you-need-to-know-by-thinkdiffusion">	thinkdiffusion	</a>,
	<a href="https://www.floyo.ai/workflows">	Floyo		</a>, 
	<a href="https://learn.thinkdiffusion.com/tag/comfyui/">	tag/comfyui	</a>

<li/>	<a href="https://www.together.ai/models/flux-1-kontext-dev">	Together.AI	</a>

<li/>	<a href="https://www.together.ai/models/flux-1-kontext-dev">	Together.AI	</a>

<li/>	<a href="https://vast.ai/article/getting-started-with-comfy-UI">	vast.ai	</a>

<li/>	<a href="https://magicquill.art/demo/">	MagicQuill	</a>,
	<a href="https://github.com/magic-quill">	GitHub - MagicQuill	</a>,
	<a href="https://huggingface.co/spaces/AI4Editing/MagicQuill">	Demo	</a>,
	<a href="https://github.com/magic-quill/ComfyUI_MagicQuill/">	ComfyUI_MagicQuill	</a>

<li/>	<a href="https://github.com/showlab/PhotoDoodle">	PhotoDoodle	</a>, 
	<a href="https://github.com/smthemex/ComfyUI_PhotoDoodle">	smthemex/ComfyUI_PhotoDoodle	</a>, 
	<a href="https://huggingface.co/spaces/ameerazam08/PhotoDoodle-Image-Edit-GPU">	HuggingFace - PhotoDoodle-Image-Edit-GPU </a>

<li/>	<a href="https://aivideo.hunyuan.tencent.com/">	Tencent GenAI Platform	</a>

<li/> ComfyUI workflows <a href="https://civitai.com/user/UmeAiRT/models">		CivitAI - user - UmeAiRT </a>,
	<a href="https://civitai.com/user/yorgash/models">		CivitAI - user - yorgash (ComfyUI workflows)	</a>

</ol>
<hr><!------------------------------------------------------------------------>
<h3 id="tagRead">	GenAI - Reading Material	</h3>
<ol start=1 type=1>
<li/>	<a href="https://github.com/anthropics/anthropic-cookbook/tree/main">	Anthropic Cookbook	</a>,

<li/>	<a href="https://arxiv.org/html/2412.05127v1">		ArXiv - Prompt Canvas: A Literature-Based Practitioner Guide for Creating Effective Prompts in Large Language Models	</a>

<li/>	<a href="https://arxiv.org/html/2410.01731v1">		ArXiv - ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation (Tel Aviv University, NVIDIA)</a>

<li/>	<a href="https://arxiv.org/abs/2409.01392">	ArXiv - ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems	</a>

<li/>	<a href="https://www.skills.google/">	Google Skills	</a>

<li/>	<a href="https://jhave.substack.com/archive?sort=new">	Xtending Digital Narrative (Jhave's Ai links)	</a>
<li/>	<a href="https://comfyorg.notion.site/ComfyUI-Desktop-User-Guide-1146d73d365080a49058e8d629772f0a#aab1bb6f65b64b0d82a85a947781cd60">	ComfyUI Desktop User Guide 	</a>

<li/>	<a href="https://openart.ai/blog">	OpenArt - Prompts 	</a>


<li/>	<a href="https://collections.unu.edu/eserv/UNU:10175/Working_Paper_AI_Agents.pdf">	Does the United Nations Need Agents? (Amina, Abdalla)	</a>,
	<a href="https://www.404media.co/the-un-made-ai-generated-refugees/">		The UN Made AI-Generated Refugees (404media)	</a>,

<li/>	<a href="https://blog.segmind.com/">	SegMind Blog	</a>, 
	<a href="https://blog.segmind.com/qwen-image-prompt-parameter-guide/">	Qwen-Image: Prompt & Parameter Guide	</a>

<li/>	Sonar <a href="https://www.sonarsource.com/resources/white-papers/">		WhitePapers	</a>,
	<a href="https://www.sonarsource.com/resources/the-coding-personalities-of-leading-llms/">	Coding Persoalities	</a>

</ol>
<!----------------------------------------------------------------------------->
<!------------------------------------COLUMN 3--------------------------------->
<!----------------------------------------------------------------------------->
</td><td width="30%"><!-------------------------------------------------------->
<h3 id="tagRemove">	Object Background Remover / Segmentation / InPaint / OutPaint		</h3>
<li/>	Keywords: <a href="https://openart.ai/workflows/all?keyword=Segment">	OpenArt - Segment </a>,
	<a href="https://openart.ai/workflows/all?keyword=Background">	OpenArt - Background</a><br/><ol start=1 type=1>



<li/> <a href="https://huggingface.co/apple/DepthPro">	Appple DepthPro	</a>,
	<a href="https://github.com/spacepxl/ComfyUI-Depth-Pro">	ComfyUI-Depth-Pro	</a>,
	<a href="https://openart.ai/workflows/ailab/depthflow-with-comfyui-depth-pro/4TQF8VyJgkCY4RuHYA79">	depthflow-with-comfyui-depth-pro	</a>

<li/>	<a href="https://articulate-anything.github.io/">	Articulate-Anything - Automatic Modeling of Articulated Objects	</a>

<li/>	<a href="https://depth-anything-3.github.io/" id="mark">	Depth Anything 3: Recovering the Visual Space from Any Views	</a>, 
	<a href="https://depth-anything-v2.github.io/">	Depth Anything v2	</a>, 
	<a href="https://github.com/DepthAnything/Depth-Anything-V2">		GitHub - Depth-Anything-V2	</a>,
	<a href="https://huggingface.co/spaces/depth-anything/Depth-Anything-V2">	spaces/depth-anything - demo	</a>

<li/>	<a href="https://github.com/john-mnz/ComfyUI-Inspyrenet-Rembg">	GitHub - Inspyrenet-Rembg	</a>

<li/>	<a href="https://github.com/1038lab/ComfyUI-RMBG">	GitHub - RMBG (BEN2, mask feather, dino object segmentation)	</a>,
	<a href="https://openart.ai/workflows/ailab/comfyui-rmbg-v120-rmbg-20-inspyrenet-and-ben-precision-background-removal/FyG6ZL1q3Z8zjSB3GPxY">	OpenArt - rmbg-v120-rmbg-20-inspyrenet	</a>, 
	<a href="https://github.com/PramaLLC/BEN2_ComfyUI">	PramaLLC/BEN2_ComfyUI	</a>

<li/> <a href="https://huggingface.co/briaai/RMBG-2.0">	BRIA Background Removal v2.0	</a>

<li/> <a href="https://github.com/plemeri/InSPyReNet">	Image Pyramid Structure for High Resolution Salient Object Detection (InSPyReNet)	</a>

<li/> <a href="https://github.com/ZhengPeng7/BiRefNet">	Bilateral Reference for High-Resolution Dichotomous Image Segmentation (BiRefNet)	</a>,
	<a href="https://github.com/Visionatrix/ComfyUI-BiRefNet">	ComfyUI-BiRefNet	</a>,
	<a href="https://github.com/MoonHugo/ComfyUI-BiRefNet-Hugo">	MoonHugo/ComfyUI-BiRefNet-Hugo	</a>

<li/> <a href="https://huggingface.co/PramaLLC/BEN">		Background Erase Network (BEN)	</a>
	<a href="https://civitai.com/articles/8845/ben-background-erase-network-in-comfyui">		CivitAI ComfyUI </a>

<li/> <a href="https://amap-ml.github.io/FE2E/">	FE2E: From Editor to Dense Geometry Estimator	</a>

<li/>	<a href="https://github.com/Layer-norm/comfyui-lama-remover">	lama-remover	</a>,
	<a href="https://civitai.com/articles/6255/using-comfy-to-batch-process-images-with-lama-cleaner">	batch-process-images-with-lama-cleaner	</a>

<li/>	<a href="https://openart.ai/workflows/fish_intent_33/removeanything/guex9gtyTcSQMslwjvl8">	OpenArt - fish_intent_33/removeanything	</a>
	<a href="https://huggingface.co/blog/OzzyGT/diffusers-image-fill">	Diffusers Image Fill Guide	</a>

<li/>	<a href="https://openart.ai/workflows/emperor_rare_28/object-removal-workflow/346g2A7esW3lde8KZH5F">	OpenArt - emperor_rare_28/object-removal-workflow	</a>

<li/>	<a href="https://github.com/scraed/LanPaint">	Lanpaint: Training-Free Diffusion Inpainting with Exact and Fast Conditional Inference	</a>,
	<a href="https://openart.ai/workflows/makisekurisu/flux-lanpaint-training-free-inpainting/Tpdv6Wor3IOGIs1jPfXu">	makisekurisu/flux-lanpaint-training-free-inpainting	</a>

<li/>	<a href="https://mega-sam.github.io/#demo">	MegaSAM - Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos	</a>

<li/>	<a href="https://github.com/magic-research/Sa2VA">	magic-research/Sa2VA	</a>, 
	<a href="https://huggingface.co/spaces/fffiloni/Sa2VA-simple-demo">	Sa2VA-simple-demo	</a>

<li/>	<a href="https://pq-yang.github.io/projects/MatAnyone/">	MatAnyone (NTU, SenseTime)	</a>,
	<a href="https://huggingface.co/spaces/PeiqingYang/MatAnyone">	HuggingFace demo	</a>

<li id="mark"/>	<a href="https://rex-omni.github.io/">	Rex-Omni: Detect Anything via Next Point Prediction	</a>, 
	<a href="https://huggingface.co/spaces/Mountchicken/Rex-Omni">	Rex-Omni demo	</a>

<li/>	<a href="https://rose2025-inpaint.github.io/">	ROSE: Remove Objects with Side Effects in Videos	</a>

<li/>	<a href="https://github.com/roboflow/rf-detr">	RF-DETR: SOTA Real-Time Object Detection Model	</a>, 
	<a href="https://huggingface.co/spaces/SkalskiP/RF-DETR">	RF-DETR demo	</a>

<li/> <a href="https://rookiexiong7.github.io/projects/SeC/">	SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction	</a>, 
	<a href="https://huggingface.co/OpenIXCLab/SeC-4B">	OpenIXCLab/SeC-4B	</a>, 
	<a href="https://github.com/9nate-drake/Comfyui-SecNodes">	9nate-drake/Comfyui-SecNodes	</a>


<li/>	<a href="https://teleport.varjo.com/">	Vargo Teleport - 3D from iPhone Video	</a>

<li/> Meituan	<a href="https://wanghao9610.github.io/X-SAM/">	X-SAM: From Segment Anything to Any Segmentation	</a>

</ol>
<!----------------------------------------------------------->
<h3 id="tagMusic">	Speech - Music 			</h3>
<li/>	Keywords: 
	<a href="https://openart.ai/workflows/all?keyword=Audio">	OpenArt - Audio	</a>,
	<a href="https://gist.github.com/0xdevalias/359f4265adf03b0142e4d0543c156a3e">	github - Awesome Audio	</a>
	<br/><ol start=1 type=1>

<li/>	<a href="https://github.com/abdozmantar/ComfyUI-DeepExtract">	ComfyUI-DeepExtract - separate vocals and sounds from audio files	</a>

<li/>	<a href="https://github.com/ASLP-lab/DiffRhythm">	DiRhythm: Blazingly Fast and Embarrassingly Simple End-to-End Full-Length Song Generation with Latent Diffusion	</a>

<li/>	FoleyCrafter	<a href="https://foleycrafter.github.io/">	FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds	</a>,
	<a href="https://szczesnys.github.io/hunyuanvideo-foley/">	HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation	</a>,

<li/>	MMAudio <a href="https://github.com/hkchengrex/MMAudio">	hkchengrex/MMAudio	</a>,
	<a href="https://github.com/kijai/ComfyUI-MMAudio">	kijai/ComfyUI-MMAudio	</a>,
	<a href="https://openart.ai/workflows/sneakyrobot/mmaudio-video-to-sound-audio-visual-workflow/zHLVMpwqHzqR9yHLzW06">		OpenArt - sneakyrobot/mmaudio-video-to-sound-audio-visual-workflow	</a>


<li/>	<a href="https://github.com/billwuhao/ComfyUI_DiffRhythm">	billwuhao/ComfyUI_DiffRhythm	</a>
<li/>	<a href="https://ace-step.github.io/">	ACE-Step: A Step Towards Music Generation Foundation Model	</a>, 
	<a href="https://github.com/ace-step/ACE-Step">	ace-step/ACE-Step	</a>

<li/> Platform - ElevenLabs  <a href="https://elevenlabs.io/voice-library/angry-voices">	voice-library/angry-voices </a>
<li/> Platform - Hume.AI	<a href="https://www.hume.ai/text-to-speech">		LLM for text-to-speech	</a>,
<li/> Platform - Play.HT	<a href="https://play.ht/text-to-speech/singaporean-english/#samples">		Play.HT - singaporean-english	</a>,
	<a href="https://app.play.ht/api/sandbox">	play.ht sandbox	</a>
<li/>	Platform - Resemble <a href="https://www.resemble.ai/">	resemble.ai (Fake Audio Detection)	</a>

<li/>	<a href="https://github.com/roboflow/rf-detr">	RF-DETR: SOTA Real-Time Detection and Segmentation Model	</a>

<li/>	<a href="https://www.riffusion.com/?filter=staff-picks">	Riffusion (platform)	</a>

<li/>	<a href="https://cypress-yang.github.io/SongBloom_demo/">	SongBloom: Coherent Song Generation via Interleaved Autoregressive Sketching and Diffusion Refinement	</a>


<li/>	<a href="https://huggingface.co/spaces/tencent/SongGeneration">		SongGeneration - LeVo: High-Quality Song Generation with Multi-Preference Alignment	</a>

<li/>	<a href="https://thinksound-project.github.io/">	ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing	</a>, 
	<a href="https://github.com/FunAudioLLM/ThinkSound">	ThinkSound	</a>

<li/>	Music <a href="https://map-yue.github.io/">	YuE: Open Music Foundation Models for Full-Song Generation	</a>

</ol>
<!----------------------------------------------------------->
<h3 id="tagTTS">	Speech - Text-2-Speech (TTS) 			</h3>
<li/>	Keywords: 
	<a href="https://huggingface.co/papers?q=Speech%20Token">	HuggingFace - Speech	</a>,
	<a href="https://openart.ai/workflows/all?keyword=Audio">	OpenArt - Audio	</a>,
	<a href="https://gist.github.com/0xdevalias/359f4265adf03b0142e4d0543c156a3e">	github - Awesome Audio	</a>
	<br/><ol start=1 type=1>

<li/>	CanopyLabs <a href="https://huggingface.co/canopylabs">	Llama-based Speech-LLM designed for high-quality, empathetic text-to-speech generation	</a>,
	<a href="https://huggingface.co/canopylabs/orpheus-3b-0.1-ft">	canopylabs/orpheus TTS (emotion, training mesopolitica)	</a>, 
	<a href="https://github.com/ShmuelRonen/ComfyUI-Orpheus-TTS">	ShmuelRonen/ComfyUI-Orpheus-TTS	</a> 

<li/>	CosyVoice 
	<a href="https://funaudiollm.github.io/cosyvoice3/">	CosyVoice 3 (with samples)	</a>,
	<a href="https://funaudiollm.github.io/cosyvoice2/">	CosyVoice 2	</a>,
	<a href="https://openart.ai/workflows/t8star/cosyvoice2latentsync/sW87AX7TU9Lq6t4uHySn">	t8star/cosyvoice2latentsync 	</a>
	<a href="https://github.com/muxueChen/ComfyUI_NTCosyVoice">	muxueChen/ComfyUI_NTCosyVoice	</a>
	<a href="https://github.com/touge/ComfyUI-NCE_CosyVoice">	touge/ComfyUI-NCE_CosyVoice	</a>

<li/>	DiodioGod <a href="https://github.com/diodiogod/TTS-Audio-Suite">		TTS Audio Suite	</a>

<li/>	Data 
	<a href="AI Audio Datasets (AI-ADS)">	AI Audio Datasets (AI-ADS)	</a>, 
	<a href="https://www.openslr.org/82/">		CN-Celeb1, CN-Celeb2	</a>, 

<li/> DataoceanAI	<a href="https://huggingface.co/DataoceanAI/dolphin-base">	Dolphin (40 Eastern languages East Asia, South Asia, Southeast Asia, Middle East, 22 Chinese dialects) 	</a>

<li/> FishAudio 	<a href="https://github.com/fishaudio/fish-speech">	FishAudio (No ComfyUI)	</a>
	<a href="https://huggingface.co/fishaudio">	huggingface.co/fishaudio	</a>

<li/>	<a href="https://github.com/IuvenisSapiens/ComfyUI_Qwen2-Audio-7B-Instruct-Int4">	Qwen2-Audio-7B-Instruct-Int4	</a>

<li/> TTS - DeepGram <a href="https://playground.deepgram.com/?endpoint=listen&smart_format=true&language=en&model=nova-3">	TTS playground </a>, 
	<a href="https://developers.deepgram.com/docs/text-to-speech-prompting">	text-to-speech-prompting </a>

<li/>	TTS - F5-TTS <a href="https://huggingface.co/SWivid/F5-TTS">		HuggingFace - SWivid/F5-TTS	</a>,
	<a href="https://github.com/niknah/ComfyUI-F5-TTS">	niknah/ComfyUI-F5-TTS	</a>,
	<a href="https://huggingface.co/erax-ai/EraX-Smile-Female-F5-V1.0">	 erax-ai (vietnamese)	</a>,

<li/>	TTS - FreeVC <a href="https://olawod.github.io/FreeVC-demo/">	Github - FreeVC - One-Shot Voice Conversion 	</a>, 
	<a href="https://github.com/ShmuelRonen/ComfyUI-FreeVC_wrapper">	ShmuelRonen/ComfyUI-FreeVC_wrapper	</a>

<li/> TTS - IMS-Toucan	<a href="https://github.com/DigitalPhonetics/IMS-Toucan">	IMS-Toucan: Controllable Text-to-Speech for over 7000 Languages	</a>,
	<a href="https://huggingface.co/spaces/Flux9665/MassivelyMultilingualTTS">	MassivelyMultilingualTTS	</a>

<li/>	TTS - KaniTTS 
	<a href="https://huggingface.co/nineninesix/kani-tts-370m">	KaniTTS: Fast and Expressive Speech Generation Model	</a>,
	<a href="https://github.com/wildminder/ComfyUI-KaniTTS">	wildminder/ComfyUI-KaniTTS	</a>, 

<li/>	TTS - Kokoro <a href="https://huggingface.co/spaces/ysharma/Make_Custom_Voices_With_KokoroTTS">	Voice Mixer Studio	</a>, 
	<a href="https://github.com/MushroomFleet/DJZ-KokoroTTS">	MushroomFleet/DJZ-KokoroTTS	</a>

<li/> TTS = Llasa-3B <a href="https://huggingface.co/spaces/srinivasbilla/llasa-3b-tts">	Llasa-3B	</a>, 
	<a href="https://huggingface.co/HKUSTAudio/Llasa-3B">	HKUSTAudio/Llasa-3B	</a>,
	<a href="https://replicate.com/kjjk10/llasa-3b-long">	Replicate - kjjk10/llasa-3b-long	</a>

<li/> TTS - Marvis-AI	<a href="https://huggingface.co/Marvis-AI">	Marvis-TTS-250m (Sesame CSM-1B, Kyutai mimi codec)	</a>, 
	<a href="https://github.com/Marvis-Labs/marvis-tts">		Marvis-Labs/marvis-tts	</a>

<li/> TTS - MegaTTS3 	<a href="https://github.com/bytedance/MegaTTS3">	bytedance - MegaTTS3 (voice cloning)	</a>, 
	<a href="https://huggingface.co/spaces/ByteDance/MegaTTS3">	MegaTTS3 Demo	</a>, 
	<a href="https://openart.ai/workflows/ailab/comfyui-megatts/K0urCR510JCn2FAcYxoW">	ailab/comfyui-megatts	</a>

<li/>	TTS - Microsoft VibeVoice <a href="https://huggingface.co/microsoft/VibeVoice-1.5B">		microsoft/VibeVoice-1.5B	</a>,
	<a href="https://github.com/Enemyx-net/VibeVoice-ComfyUI">	Enemyx-net/VibeVoice-ComfyUI	</a>,
	<a href="https://86636c494bbddc69c7.gradio.live/">	Demo	</a>, 
	<a href="https://github.com/voicepowered-ai/VibeVoice-finetuning">	Fine-Tuning	</a>,
	<a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B">	VibeVoice-Realtime is a lightweight real-time text-to-speech model supporting streaming text input and robust long-form speech generation	</a>

<li/>	TTS - Microsoft EdgeTTS 
	<a href="https://huggingface.co/spaces/innoai/Edge-TTS-Text-to-Speech">	Microsoft Edge-TTS-Text-to-Speech	</a>
	<a href="https://github.com/1038lab/ComfyUI-EdgeTTS">	1038lab/ComfyUI-EdgeTTS	</a>,

<li/>	TTS - Nari-Labs <a href="https://github.com/nari-labs/dia">	DIA (2-pax dialogue)	</a>

<li/>	TTS - Sesame <a href="https://www.sesame.com/research/crossing_the_uncanny_valley_of_voice">	Sesame - Crossing the uncanny valley of conversational voice	</a>, 
	<a href="https://levelup.gitconnected.com/sesame-csm-1b-for-multi-speaker-ai-conversations-complete-guide-to-installing-and-running-e76b202e5b91">	Sesame CSM 1B for Multi-Speaker AI Conversations	</a>,
	<a href="https://github.com/SesameAILabs/csm">	SesameAILabs/csm	</a>, 
	<a href="https://huggingface.co/sesame/csm-1b">	Sesame - CSM (Conversational Speech Model) 	</a>,
	<a href="https://github.com/billwuhao/ComfyUI_CSM">	billwuhao/ComfyUI_CSM	</a>, 
	<a href="https://blog.speechmatics.com/sesame-finetune">	SpeechMatics - How to Finetune Sesame AI's Speech Model on New Languages and Voices	</a>, 
	<a href="https://github.com/knottwill/sesame-finetune">	SpeechMatics - knottwill/sesame-finetune </a>

<li/>	TTS - SparkTTS 
	<a href="https://sparkaudio.github.io/spark-tts/">	An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens	</a>
	<a href="https://github.com/billwuhao/ComfyUI_SparkTTS">	billwuhao/ComfyUI_SparkTTS	</a>, 
	<a href="https://huggingface.co/spaces/Mobvoi/Offical-Spark-TTS">	SparkTTS Demo </a>, 
	<a href="https://github.com/1038lab/ComfyUI-SparkTTS">	1038lab/ComfyUI-SparkTTS	</a>, 
	<a href="https://openart.ai/workflows/ailab/comfyui-sparktts-advanced-text-to-speech-for-comfyui/3WhEqE9ejVinv1RLG3DI">	comfyui-sparktts-advanced-text-to-speech-for-comfyui/3WhEqE9ejVinv1RLG3DI	</a>, 
	<a href="https://github.com/tuanh123789/Spark-TTS-finetune">	Spark-TTS-finetune	</a>, 
	<a href="https://github.com/SparkAudio/Spark-TTS">	SparkAudio/Spark-TTS (NTU)	</a>

<li/>	TTS - Seed-VC 
	<a href="https://plachtaa.github.io/seed-vc/">	Zero Shot Voice Conversion (Singing VoiceOver)	</a>, 
	<a href="https://github.com/Plachtaa/seed-vc">	Plachtaa/seed-vc	</a>

<li/>	TTS - SoulX-Podcast <a href="https://soul-ailab.github.io/soulx-podcast/">		SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity	</a>

<li/> TTS - StepFun.AI	
	<a href="https://github.com/stepfun-ai/Step-Audio2">	StepAudio2: Speech and Audio Understanding & Conversation </a>, 
	<a href="https://stepfun.ai/chats/new">	StepAudio Demo </a>, 
	<a href="https://github.com/billwuhao/ComfyUI_StepAudioTTS ">	billwuhao/ComfyUI_StepAudioTTS 	</a> 

<li/>	Tool <a href="https://github.com/rsxdalv/TTS-WebUI">	TTS-WebUI	</a>

<li/>	TTS - Zonos <a href="https://www.zyphra.com/post/beta-release-of-zonos-v0-1">	Zyphra	</a>, 
	<a href="https://playground.zyphra.com/">	Zyphra playground	</a>, 
	<a href="https://www.zyphra.com/post/beta-release-of-zonos-v0-1/">	Zonos v01 Speech TTS	</a>, 

</ol>
<!----------------------------------------------------------->
<h3 id="tagTalkingHead">	Talking Head			</h3>
<li/>	Keywords: 
	<a href="https://openart.ai/workflows/all?keyword=talking">	OpenArt - Talking	</a>,
	<a href="https://github.com/harlanhong/awesome-talking-head-generation">	harlanhong/awesome-talking-head-generation	</a>,
	<a href="https://github.com/JosephPai/Awesome-Talking-Face">	JosephPai/Awesome-Talking-Face	</a>,
	<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis">	Kedreamix/Awesome-Talking-Head-Synthesis	</a>,
	<a href="https://github.com/Curated-Awesome-Lists/awesome-ai-talking-heads">	awesome-ai-talking-heads	</a>,
	<a href="https://github.com/weihaox/awesome-digital-human">	awesome-digital-human	</a>,

	<a href="https://cvpr.thecvf.com/">	IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)	</a>,
	<a href="https://eccv.ecva.ne">	European Conference on Computer Vision (ECCV)	</a>,
	<a href="https://iccv.thecvf.com/">	International Conference on Computer Vision (ICCV)		</a>, 
	<a href="https://iclr.cc/">	International Conference on Learning Representations (ICLR)	</a>, 
	<a href="https://github.com/topics/lip-sync">	Github - lip-sync		</a>

	<br/>
	<ol start=1 type=1>

<li/>	Alibaba <a href="https://fantasy-amap.github.io/fantasy-talking/">	Fantasy Talking	</a>, 
	<a href="https://github.com/kijai/ComfyUI-WanVideoWrapper">	kijai/ComfyUI-WanVideoWrapper	</a>, 
	<a href="https://openart.ai/workflows/pix_studio/fantasy-talking/dy5O3i4qKYaTns7ZgUVF">	pix_studio/fantasy-talking	</a>, 
	<a href="https://omni-avatar.github.io/">	Alibaba - OmniAvatar	</a>, 
	<a href="https://github.com/Omni-Avatar/OmniAvatar">	OmniAvatar	</a>, 
	<a href="https://humanaigc.github.io/omnitalker/">	OmniTalker	</a>
	<a href="https://www.youtube.com/watch?v=bSssQdqXy9A&t=538s">	YouTube	</a>, 
	<a href="https://fantasy-amap.github.io/fantasy-portrait/">	FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers </a>,
	<a href="https://johnneywang.github.io/EchoShot-webpage/">	EchoShot: Multi-Shot Portrait Video Generation	</a>, 
	<a href="https://github.com/JoHnneyWang/EchoShot">	JoHnneyWang/EchoShot	</a>, 
	<a href="https://arxiv.org/html/2506.15838v1">	Paper	</a>

<li/>	<a href="https://github.com/harlanhong/ACTalker">	ACTalker	</a>, 
	<a href="https://harlanhong.github.io/publications/actalker/index.html">	Paper 	</a>

<li/>	<a href="https://github.com/oneThousand1000/AnimPortrait3D">	AnimPortrait3D - text to 3D animation	</a>

<li/>	<a href="https://humanaigc.github.io/animate-anyone-2/">	animate-anyone-2 - High-Fidelity Character Image Animation with Environment Affordance	</a>, 
	<a href="https://github.com/HumanAIGC/AnimateAnyone">	AnimateAnyone </a>

<li/>	<a href="https://arxiv.org/abs/2502.20220?ref=uploadvr.com">	Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars	</a>,
	<a href="https://tobias-kirschstein.github.io/avat3r/">	tobias-kirschstein.github.io/avat3r	</a>

<li/>	ByteDance <a href="https://bytedance.github.io/InfiniteYou/">	InfiniteYou - Flexible Photo Recrafting While Preserving Your Identity	</a>,
	<a href="https://arxiv.org/html/2503.16418v1">	Paper </a>,
	<a href="https://github.com/bytedance/InfiniteYou">	bytedance/InfiniteYou	</a>,
	<a href="https://huggingface.co/spaces/ByteDance/InfiniteYou-FLUX">	InfiniteYou-FLUX demo	</a>, 
	<a href="https://openart.ai/workflows/t8star/infiniteyou/0BGujVa1yfZzispihLjU">	t8star/infiniteyou 	</a>, 
	<a href="https://openart.ai/workflows/bulldog_fruitful_46/flux_infiniteyou/OXicXXXSB1V4KIF0wXdz">	bulldog_fruitful_46/flux_infiniteyou	</a>, 
	<a href="https://github.com/ZenAI-Vietnam/ComfyUI_InfiniteYou">	ZenAI-Vietnam/ComfyUI_InfiniteYou 	</a>
	<a href="https://github.com/bytedance/LatentSync">	LatentSync </a>

<li/> <a href="https://character-ai.github.io/avatar-fx/">	Character.AI avatar-fx	</a>

<li/> <a href="https://murray-wang.github.io/CharaConsist/">	CharaConsist: Fine-Grained Consistent Character Generation	</a>

<li/>	<a href="https://github.com/SamKhoze/ComfyUI-DeepFuze">	GitHub - DeepFuze	</a>,
	<a href="https://www.youtube.com/watch?v=elQzQo__kWI">	YouTube 	</a>, Facial transformations, lipsyncing, video generation, voice cloning, face swapping, and lipsync translation

<li/>	<a href="https://github.com/toto222/DICE-Talk">	DICE-Talk - Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation	</a>, 
	<a href="https://github.com/smthemex/ComfyUI_DICE_Talk">	smthemex/ComfyUI_DICE_Talk	</a>

<li/>	<a href="https://arxiv.org/abs/2503.07027">	EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer	</a>, 
	<a href="https://github.com/jax-explorer/ComfyUI-easycontrol">		jax-explorer/ComfyUI-easycontrol </a>, 
	<a href="https://arxiv.org/abs/2503.07027">	Paper	</a>

<li/>	<a href="https://github.com/antgroup/echomimic_v2">		EchoMimic	</a>

<li/>	<a href="https://github.com/deepbrainai-research/float">	FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait	</a>, 
	<a href="https://github.com/yuvraj108c/ComfyUI-FLOAT">	yuvraj108c/ComfyUI-FLOAT	</a>, 
	<a href="https://deepbrainai-research.github.io/float/">	deepbrainai-research.github.io/float	</a>

<li/>	<a href="https://lllyasviel.github.io/frame_pack_gitpage/">	FramePack - Packing Input Frame Context in Next-Frame Prediction Models for Video Generation </a>, 
	<a href="https://github.com/lllyasviel/FramePack">	lllyasviel/FramePack	</a>, 
	<a href="https://openart.ai/workflows/pix_studio/framepack6g/kZCYax19o8PgNs25y1Xr">	pix_studio/framepack6g	</a>

<li/>	<a href="https://github.com/HelloVision/ComfyUI_HelloMeme">	HelloMeme	</a>

<li/>	HumanMLLM <a href="https://github.com/HumanMLLM/HumanOmniV2">	HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context	</a>

<li/>	KwaiVGI <a href="https://github.com/KwaiVGI/ReCamMaster">	ReCamMaster: Camera-Controlled Generative Rendering from A Single Video 	</a>, 
	<a href="https://jianhongbai.github.io/ReCamMaster/">	jianhongbai.github.io/ReCamMaster	</a>, 
	<a href="https://arxiv.org/abs/2503.11647">	Paper	</a>, 
	<a href="https://github.com/kijai/ComfyUI-WanVideoWrapper">	kijai/ComfyUI-WanVideoWrapper	</a>,

<li/>	<a href="https://aigc3d.github.io/projects/LAM/">	LAM: Large Avatar Model for One-shot Animatable Gaussian Head	</a>

<li/>	<a href="https://liveavatar.github.io/">		Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length	</a>,
	<a href="https://github.com/Alibaba-Quark/LiveAvatar">	Alibaba-Quark/LiveAvatar	</a>

<li/>	<a href="https://github.com/KwaiVGI/LivePortrait">		LivePortrait	</a>
	<a href="https://github.com/kijai/ComfyUI-LivePortraitKJ">	GitHub - ComfyUI-LivePortraitKJ	</a>,
	<a href="https://openart.ai/workflows/datou/live-portrait/PMM4NSLnbhU08xvRqN2e">	OpenArt - datou/live-portrait	</a>,
	<a href="https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait">	GitHub - ComfyUI-AdvancedLivePortrait	</a>,
	<a href="https://www.youtube.com/watch?v=xGJ82MVOTcU">	YouTube	</a>


<li/>	<a href="https://lingtengqiu.github.io/LHM/">	LHM: Large Animatable Human Reconstruction Model for Single Image to 3D in Seconds	</a>, 
	<a href="https://huggingface.co/spaces/DyrusQZ/LHM">	spaces/DyrusQZ/LHM demo	</a>, 
	<a href="https://github.com/aigc3d/LHM/tree/feat/comfyui">	LHM ComfyUI	</a>,
	<a href="https://github.com/aigc3d/LAM">	LAM: Large Avatar Model for One-shot Animatable Gaussian Head	</a>, 
	<a href="https://arxiv.org/html/2509.07552v1">	PanoLAM: Large Avatar Model for Gaussian Full-Head Synthesis from One-shot Unposed Image	</a>, 

<li/>	<a href="https://github.com/menyifang/MIMO">	MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling	</a>

<li/>	<a href="https://arxiv.org/html/2506.18871v1">		OmniGen2: Exploration to Advanced Multimodal Generation	</a>,
	<a href="https://github.com/VectorSpaceLab/OmniGen2">		VectorSpaceLab/OmniGen2		</a>,
	<a href="https://github.com/neverbiasu/ComfyUI-OmniGen2">		ComfyUI-OmniGen2	</a>, 

<li/>	<a href="https://memoavatar.github.io/">	MemoAvatar - MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation	</a>,
	<a href="https://github.com/if-ai/ComfyUI-IF_MemoAvatar">	ComfyUI-IF_MemoAvatar	</a>,
	<a href="https://openart.ai/workflows/dashen/memoavatar/2tsyggrYyZtWeHiyYagV">	OpenArt - dashen/memoavatar	</a>,
	<a href="https://openart.ai/workflows/t8star/memoavatar/yH1rroH1nLA6O0Os4lh3">	OpenArt - t8star/memoavatar	</a>,
	<a href="https://openart.ai/workflows/cat_untimely_42/memoavatar_photo-photo_talk/Dw9ZIG27X017M2DHwkSW">	cat_untimely_42/memoavatar_photo-photo_talk	</a>

<li/>	<a href="https://github.com/Orange-3DV-Team/MoCha">	MoCha: End-to-End Video Character Replacement without Structural Guidance	</a>, 

<li/> <a href="https://huggingface.co/huaichang/PersonaLive">	PersonaLive: Expressive Portrait Image Animation for Live Streaming	</a>

<li/>	<a href="https://real3dportrait.github.io/">	Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis	</a>

<li/>	<a href="https://zhongleilz.github.io/Sketch2Anim/">	Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation	</a>

<li/>	<a href="https://skyworkai.github.io/skyreels-audio.github.io/">	SkyReels-Audio	</a>

<li/>	<a href="https://jixiaozhong.github.io/Sonic/">	Sonic: Shifting Focus to Global Audio Perception in Portrait Animation	</a>, 
	<a href="https://huggingface.co/spaces/xiaozhongji/Sonic">	xiaozhongji/Sonic Demo	</a>
	<a href="https://github.com/smthemex/ComfyUI_Sonic">	smthemex/ComfyUI_Sonic	</a>

<li/>	<a href="https://francis-rings.github.io/StableAvatar/">	StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation	</a>	
	<a href="https://github.com/Francis-Rings/StableAvatar">	Francis-Rings/StableAvatar	</a>

<li/> TaoAvatar <a href="https://pixelai-team.github.io/TaoAvatar/">	pixelai-team.github.io/TaoAvatar	</a>, 
	<a href="https://arxiv.org/abs/2503.17032">	TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting	</a>, 
	<a href="https://medium.com/@nimritakoul01/taoavatar-a-full-body-talking-avatar-for-mobile-devices-using-3d-gaussian-splatting-and-smplx-247e48c42933">	Medium	</a>

<li/>	Tencent <a href="https://github.com/kijai/ComfyUI-MimicMotionWrapper">	GitHub - ComfyUI-MimicMotionWrapper	</a>
	<a href="https://github.com/Tencent/MimicMotion">	MimicMotion	</a>,
	<a href="https://github.com/TMElyralab/MusePose">	MusePose	</a>,

<li/>	UniAnimate / Animate-X	<a href="https://lucaria-academy.github.io/Animate-X/">	Animate-X	</a>,"
	<a href="https://unianimate.github.io/">		UniAnimate	</a>, 
	<a href="https://github.com/ali-vilab/UniAnimate">		ali-vilab/UniAnimate	</a>, 
	<a href="https://github.com/Isi-dev/ComfyUI-UniAnimate-W">	Isi-dev/ComfyUI-UniAnimate-W (UniAnimate=humans, Animate-X=animals/cartoons)	</a>,
	<a href="https://huggingface.co/Isi99999/UniAnimate_and_Animate-X_Models/tree/main">		UniAnimate/Animate-X models	</a>, 

<li/>	<a href="https://ewrfcas.github.io/Uni3C/">	Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation(Camera + Video Motion) 	</a>,
	<a href="https://github.com/alibaba-damo-academy/Uni3C">	alibaba-damo-academy/Uni3C	</a>

<li/>	<a href="https://zcai0612.github.io/UP2You/">	UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections	</a>

<li/>	X-Portrait <a href="https://github.com/akatz-ai/ComfyUI-X-Portrait-Nodes">	GitHub - akatz-ai/ComfyUI-X-Portrait	</a>


</ol>
<!----------------------------------------------------------->
<h3 id="tagPerformance">	Tool - Performance			</h3>
<ol start=1 type=1>


<li/>	<a href="https://github.com/aejion/AccVideo">	AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset	</a>

<li/>	DFloat11	<a href="https://github.com/LeanModels/DFloat11">	DFloat11: Lossless LLM Compression for Efficient GPU Inference	</a>,
	<a href="https://huggingface.co/DFloat11/FLUX.1-Kontext-dev-DF11">	DFloat11/FLUX.1-Kontext-dev-DF11	</a>

<li/> <a href="https://francis-rings.github.io/FlashPortrait/">	FlashPortrait: 6 X Faster Infinite Portrait Animation with Adaptive Latent Prediction	</a>

<li/>	Latent-Space		<a href="https://github.com/DenRakEiw/Latent_Nodes">	ComfyUI Latent Color Tools	</a>

<li/>	<a href="https://github.com/linkedin/Liger-Kernel">	Liger Kernel: Efficient Triton Kernels for LLM Training	</a>

<li/>	LightX2V <a href="https://huggingface.co/lightx2v">	Qwen Image + Wan Video	</a>,
	<a href="https://huggingface.co/lightx2v/Qwen-Image-Lightning">	Qwen T2I	</a>,
	<a href="https://huggingface.co/lightx2v/Wan2.1-I2V-14B-720P-StepDistill-CfgDistill-Lightx2v">	Wan I2V	</a>,

<li/>	<a href="https://zehong-ma.github.io/MagCache/">	MagCache	</a>,
	<a href="https://github.com/Zehong-Ma/ComfyUI-MagCache">	Zehong-Ma/ComfyUI-MagCache	</a>, 

<li/>	<a href="https://github.com/ModelTC">	ModelTC (Lightning, LightX2V)	</a>,
	<a href="https://github.com/ModelTC/LightCompress">	LightCompress: Towards Accurate and Efficient AIGC Model Compression	</a>

<li/>	<a href="https://liewfeng.github.io/TeaCache/">	TeaCache </a>

<li/>	<a href="https://github.com/Tencent/WeDLM">	WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference	</a>

<li/>	<a href="https://github.com/thu-ml/TurboDiffusion">	TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times	</a>

<li/>	<a href="https://huggingface.co/blog/zerogpu-aoti">	ZeroGPU ahead-of-time (AoT) on HuggingFace </a>

</ol>
<!----------------------------------------------------------->
<h3 id="tagTraining">	Tool / Training / Utility			</h3>
<ol start=1 type=1>

<li/>	<a href="https://education.civitai.com/using-civitai-the-on-site-lora-trainer/">	CivitAI LORA Trainer	</a>,
	<a href="https://education.civitai.com/quickstart-guide-to-flux-1/#rapid-flux-training">	CivitAI FLUX Trainer	</a>, 
	<a href="https://openart.ai/workflows/tenofas/flux-lora-trainer-20/VmxcKxjxRoN2Lrs9ESU7">	tenofas/flux-lora-trainer-20	</a>
	<a href="https://learn.thinkdiffusion.com/building-better-models-flux-loras-in-comfyui/">	ThinkDiffusion - building-better-models-flux-loras-in-comfyui	</a>

<li/>	3D Tools <a href="https://www.autodesk.com/solutions/wonder-dynamics">	AutoDesk - Wonder Dynamics	</a>, 
	<a href="https://www.deepmotion.com/animate-3d">	DeepMotion	</a>,
	<a href="https://www.rokoko.com/products/studio">	Rokoko	</a>,
	<a href="https://odyssey.systems/introducing-explorer">	Odyssey (3D scene generation)	</a>

<li/>	<a href="https://github.com/kijai/ComfyUI-FluxTrainer">	ComfyUI-FluxTrainer	</a>,
	<a href="https://openart.ai/workflows/-/flux-lora-trainer-on-comfyui-v11/XQuqTMSGQCzsgWgkQ1MN">	flux-lora-trainer-on-comfyui-v11	</a>,
	<a href="https://openart.ai/workflows/leeguandong/flux-trainer-lora/dERqhXJbsHtDDvxzQPxl">	leeguandong/flux-trainer-lora	</a>

<li/>	<a href="https://www.youtube.com/watch?v=bf8_mEvs9lE">	YouTube - Custom AI Digital Human with HeyGen's Lora Training	</a>
	<a href="https://www.youtube.com/watch?v=V0orBtT7SYU">	YouTube - How to Replace Yourself on Zoom Calls with an AI Clone from HeyGen	</a>

<li id="mark"/>	DiffSynth-Studio
	<a href="https://huggingface.co/DiffSynth-Studio/Qwen-Image-i2L">	Qwen-Image-i2L (Image to LoRA)	</a>, 
	<a href="https://github.com/modelscope/DiffSynth-Studio">	modelscope/DiffSynth-Studio	</a>, 

<li id="mark"/>	Ostris.AI	
	<a href="https://www.youtube.com/@ostrisai/videos">	YouTube - ostrisai (LORA=Qwen-Edit, Wan2.1 i2v)	</a>

<li/>	<a href="https://www.youtube.com/watch?v=ksUEu6c-ouc">	Training FLUX LORA 	</a>
<li/>	<a href="https://civitai.com/articles/7131/flux-lora-trainer-on-comfyui-v10">	Training FLUX LORA	</a>
<li/>	<a href="https://civitai.com/models/713258/flux-lora-trainer-on-comfyui">	Training FLUX LORA	</a>

<li/>	<a href="https://github.com/shootthesound/comfyUI-Realtime-Lora">		shootthesound/comfyUI-Realtime-Lora	</a>

<li/>	FlyMyAI	<a href="https://github.com/FlyMyAI/flymyai-lora-trainer">		flymyai-lora-trainer	</a>, 
	<a href="https://www.youtube.com/watch?v=1DyfPhIkX78">	FluxGym 	</a>,
	<a href="https://www.youtube.com/watch?v=gdEeceGYBu0">	Training FLUX LORA	</a>,
	<a href="https://www.youtube.com/watch?v=nySGu12Y05k">	Training FLUX LORA	</a>

<li/>	<a href="https://civitai.com/articles/9360/flux-guide-part-i-lora-training">	CivitAI - flux-guide-part-i-lora-training	</a>

<li/>	<a href="https://github.com/LarryJane491/Lora-Training-in-Comfy">	Training LORA	</a>
<li/>	<a href="https://civitai.com/articles/3406/lora-training-dataset-creation-comfyui-one-click-dataset">	Training lora-training-dataset-creation-comfyui-one-click-dataset	</a>

<li/>	<a href="https://github.com/modelscope/data-juicer">	ModelScope/data-juicer	</a>

<li/>	SpeechMatics <a href="https://blog.speechmatics.com/sesame-finetune">	How to Finetune Sesame AI's Speech Model on New Languages and Voices	</a>, 
	<a href="https://github.com/knottwill/sesame-finetune">	knottwill/sesame-finetune </a>

<li id="mark"/>
	<a href="https://arxiv.org/html/2506.05010v1">	ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development	</a>,
	<a href="https://github.com/AIDC-AI/ComfyUI-Copilot">	AIDC-AI/ComfyUI-Copilot	</a>

<li/> <a href="https://github.com/crystian/ComfyUI-Crystools">		  Crystools (CPU, GPU, RAM, VRAM, GPU Temp and space) 	</a>

<li/>	<a href="https://github.com/choey/Comfy-Topaz">	choey/Comfy-Topaz	</a>,
	<a href="https://openart.ai/workflows/t8star/gigapixel/e7l2FXI0D8qn0Bxa71Nb">	t8star/gigapixel	</a>

<li/>	<a href="https://github.com/pydn/ComfyUI-to-Python-Extension ">	ComfyUI-to-Python-Extension 	</a>

<li/>	<a href="https://www.bentoml.com/blog/comfy-pack-serving-comfyui-workflows-as-apis">	comfy-pack: Serving ComfyUI Workflows as APIs	</a>, 
	<a href="https://github.com/bentoml/comfy-pack">	bentoml/comfy-pack	</a>

<li/>	<a href="https://docs.comfy.org/tutorials/api-nodes/overview">	ComfyUI api-nodes	</a>

<li/>	Detectors - NSFW <a href="https://huggingface.co/Falconsai/nsfw_image_detection">	Falconsai/nsfw_image_detection	</a>,
	<a href="https://github.com/trumanwong/ComfyUI-NSFW-Detection">	ComfyUI-NSFW-Detection	</a>

<li/> <a href="https://github.com/daxcay/ComfyUI-JDCN">		GitHub - JDCN - Directory Path	</a>

<li/> <a href="https://github.com/liusida/ComfyUI-AutoCropFaces">	GitHub - liusida/ComfyUI-AutoCropFaces	</a>

<li/>	<a href="https://github.com/MushroomFleet/DJZ-Workflows/tree/main/Foda_Flux/Captioning%20Tools">	Captioning	</a>

<li/>	<a href="https://openart.ai/workflows/toucan_chilly_4/florence-run-batch-capture-prompt-maker/tmkxPTDT7sL2EuZgaQJe">	OpenArt  - Florence Run Batch Capture Prompt Maker	</a>

<li/>	PixelPruner <a href="https://github.com/theallyprompts/PixelPruner">	theallyprompts	</a>,
	<a href="https://civitai.com/models/465684/pixelpruner-crop-tool-for-data-set-prep">	civitai	</a>

<li/>	Prompt Lists <a href="https://prompter.fofr.ai/explore">	fofr	</a>,
	<a href="https://github.com/ai-prompts/prompt-lists/">	ai-prompts/prompt-lists	</a>, 
	<a href="https://github.com/marduk191/ComfyUI-Fluxpromptenhancer">	marduk191/ComfyUI-Fluxpromptenhancer	</a>

<li/>	<a href="https://img-comparison-slider.sneas.io/examples.html">	HTML img-comparison-slider	</a>,
	<a href="https://github.com/sneas/img-comparison-slider">	GitHub 	</a>,
	<a href="https://demo.photo.gallery/examples/features/captions/">	demo.photo.gallery	</a>

<li/>	<a href="https://github.com/chflame163/ComfyUI_LayerStyle">	LayerStyle	</a>
<li/>	<a href="https://github.com/digitaljohn/comfyui-propost/tree/master">	comfyui-propost	</a>

<li/>	NovaSky	<a href="https://novasky-ai.github.io/">	NovaSky: UC Berkeley's Sky Computing Lab	</a>

<li/>	<a href="https://github.com/chflame163/ComfyUI_LayerStyle_Advance">	chflame163/ComfyUI_LayerStyle_Advance (ZhiPu / SegmentAnything)	</a>

<li/>	<a href="https://omnisvg.github.io/">	OmniSVG: A Unified Scalable Vector Graphics Generation Model	</a>

<li/>	<a href="https://hkunlp.github.io/blog/2025/Polaris/">	POLARIS: A POst-training recipe for scaling reinforcement Learning on Advanced ReasonIng modelS </a>

<li/>	QuasiBlob - image processing <a href="https://github.com/quasiblob/ComfyUI-EsesImageCompare">	ComfyUI-EsesImageCompare	</a>

<li/>	<a href="https://github.com/FlyMyAI/flymyai-lora-trainer">	Qwen-Image & Qwen-Image-Edit LoRA Training	</a>

</ol>
<!----------------------------------------------------------->
<h3>	Tool - Prompt Engineering			</h3><ol start=1 type=1>

<li/>	<a href="https://github.com/adieyal/comfyui-dynamicprompts">	adieyal/comfyui-dynamicprompts	</a>
<li/>	<a href="https://github.com/MushroomFleet/LLM-Base-Prompts">	MushroomFleet/LLM-Base-Prompts (mixed)	 </a>
<li/>	<a href="https://github.com/AIrjen/OneButtonPrompt">	AIrjen/OneButtonPrompt 	</a>
<li/>	<a href="https://prompthero.com/portraits-prompts">	PromptHero - portraits-prompts	</a>

</ol>
<!----------------------------------------------------------->
<h3 id="tagTechScan">	Tool - General / TechScan / Research / DeepResearch	</h3><ol start=1 type=1>

<li id="mark"/>
	<a href="https://github.com/scienceaix/deepresearch">	Awesome Deep Research 	</a>, 
	<a href="https://huggingface.co/spaces/muset-ai/DeepResearch-Bench-Leaderboard">	DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents	</a>,
	<a href="https://deep-research-survey.github.io/">	A Systematic Survey of Deep Research	</a>,
	<a href="https://huggingface.co/papers">	HuggingFace - Daily Papers	</a>, 
	<a href="https://blog.comfy.org/">	blog.comfy.org	</a>, 
	<a href="https://github.com/ComfyUI-Workflow/awesome-comfyui">	awesome-comfyui </a>

<li/>	<a href="https://news.smol.ai/issues">	AI News	</a>

<li/>	<a href="https://github.com/HKUDS/AI-Researcher">	AI-Researcher: Autonomous Scientific Innovation	</a>

<li/>	<a href="https://ai-4-research.github.io/">	AI4Research: A Survey of Artificial Intelligence for Scientific Research	</a>,
	<a href="https://arxiv.org/html/2507.01903v1">	Paper	</a>

<li/>	Alibaba	
	<a href="https://github.com/Alibaba-NLP/WebAgent">	WebAgent for Information Seeking (WebShaper, WebSailor, WebDancer, WebWalker)	</a>
	<a href="https://github.com/Alibaba-NLP/DeepResearch">	Alibaba-NLP/DeepResearch	</a>, 

<li/>	<a href="https://fireplexity.org/">	Fireplexity: Open Source Perplexity AI Clone	</a>

<li/>	Note-taking / Whiteboards 	
	<a href="https://wiki.heptabase.com/organize-knowledge-and-projects">	Heptabase	</a>,
	<a href="https://milanote.com/product/storyboarding">	MilaNote	</a>, 
	<a href="https://mixboard.google.com/projects">	Google Mixboard	</a>,
	<a href="https://scrintal.com/comparisons/milanote-alternative">	Scrintal	</a>, 

<li/>	Nvidia <a href="https://research.nvidia.com/labs/lpr/udr/">	Universal Deep Research: Bring Your Own Model and Strategy	</a>

<li/> Research Assistant 	<a href="https://github.com/bytedance/pasa">	ByteDance PaSa: An LLM Agent for Comprehensive Academic Paper Search </a>,
	<a href="https://storm.genie.stanford.edu/">	Stanford STORM </a>,
	<a href="https://github.com/dzhng/deep-research">	Github - Deep Research 	</a>,
	<a href="https://github.com/masterFoad/NanoSage">	NanoSage - Advanced Recursive Search & Report Generation	</a>, 
	<a href="https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research">	Perplexity - Deep Research 	</a>,
	<a href="https://github.com/MODSetter/SurfSense">	SurfSense	</a>
	<a href="https://github.com/bytedance/deer-flow">	ByteDance - DeerFlow (Deep Exploration and Efficient Research Flow)	</a>,
	<a href="https://github.com/lfnovo/open-notebook">	OpenNotebook		</a>,
	<a href="https://github.com/CaviraOSS/PageLM">	PageLM		</a>,
	<a href="https://github.com/hyperbrowserai/hyperbooklm">	HyperBookLM	</a>, 

<li/>	<a href="https://dify.ai/blog/dify-deepseek-deploy-a-private-ai-assistant">	dify-deepseek-deploy-a-private-ai-assistant	</a>

<li/>	<a href="https://arxiv.org/abs/2504.08066">	AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search	</a>, 
	<a href="https://github.com/SakanaAI/AI-Scientist-v2">	SakanaAI/AI-Scientist-v2	</a>

<li/>	<a href="https://arxiv.org/abs/2504.21776">	WebThinker: Empowering Large Reasoning Models with Deep Research Capability (paper)	</a>, 
	<a href="https://foremost-beechnut-8ed.notion.site/WebThinker-Empowering-Large-Reasoning-Models-with-Deep-Research-Capability-d13158a27d924a4b9df7f9ab94066b64">	WebThinker 	</a>, 
	<a href="https://github.com/RUC-NLPIR/WebThinker">	RUC-NLPIR/WebThinker	</a>, 



<li/>	TopazLabs <a href="https://openart.ai/workflows/t8star/-topaz-aigigapixel/nfK3ydWg8NYZIoYn9wwc">	PhotoAI GigaPixel	</a>, 
	<a href="https://openart.ai/workflows/t8star/-topaz-video/XsQRrLpnfNfLqlg7emVz">	Video AI	</a>

</ol>
<!----------------------------------------------------------------------------->
<!------------------------------------COLUMN 4--------------------------------->
<!----------------------------------------------------------------------------->
</td><td width="30%"><!-------------------------------------------------------->
<!----------------------------------------------------------->
<h3 id="tagUpscale">	Upscale SUPIR			</h3>
<li/>	Keywords: 
	<a href="https://github.com/yjsunnn/Awesome-video-super-resolution-diffusion">	Awesome-video-super-resolution-diffusion	</a>,
	<a href="https://github.com/yjsunnn/Awesome-video-super-resolution-diffusion">	Awesome Diffusion Models for Video Super-Resolution	</a>,
	<a href="https://openart.ai/workflows/all?keyword=upscale">	OpenArt - Upscale	</a>,
	<a href="https://openart.ai/workflows/all?keyword=SUPIR">	OpenArt - SUPIR	</a>, 
	<a href="https://openmodeldb.info/">		OpenModelDB	</a>,
	<a href="https://huggingface.co/Phips">	HuggingFace - Phips </a>,
	<a href="https://openmodeldb.info/?q=Skin">		realistic skin	</a>
	<br/><ol start=1 type=1>

<li/>	<a href="https://huggingface.co/ac-pill/upscale_models/tree/main">	ac-pill/upscale_models (e.g. RealESRGAN_x4plus_anime_6B.pth) 	</a>

<li/>	<a href="https://github.com/bryanswkim/Chain-of-Zoom">	Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment	 (No COmfyUI)	</a>, 
	<a href="https://huggingface.co/spaces/alexnasa/Chain-of-Zoom">		</a>

<li/>	<a href="https://github.com/Eyeline-Labs/CineScale">	CineScale: High-Resolution Cinematic Visual Generation </a>, 
	<a href="https://github.com/ali-vilab/FreeScale">	ali-vilab/FreeScale	</a>

<li/>	<a href="https://github.com/zsyOAOA/InvSR">	InvSR - Arbitrary-steps Image Super-resolution via Diffusion Inversion (No ComfyUI)	</a>,
	<a href="https://huggingface.co/spaces/OAOA/InvSR">	OAOA/InvSR demo	</a>

<li/> <a href="https://huggingface.co/camenduru/SUPIR/tree/main">		camenduru/SUPIR	</a>,
	<a href="https://openart.ai/workflows/crocodile_ruddy_19/supir-upscale/WdeLcIKRRaPNDnayUy49">	SUPIR	</a>,
	<a href="https://openart.ai/workflows/meerkat_elliptical_71/supir-image-scale/elXpUhL9AN0UzHS5zRYh">		supir-image-scale	</a>,
	<a href="https://openart.ai/workflows/jerrydavos/ultimate-flux-upscaler---2k---4k---8k---16k---32k/viXz5ezmbAEcqORoN9iW">	jerrydavos/ultimate-flux-upscaler 2k, 4k, 8k, 16k	</a>

<li/> <a href="https://github.com/wildminder/ComfyUI-DyPE">	Dynamic Position Extrapolation (DyPE) - supports FLUX, Qwen Image, and Z-Image	</a>

<li/> <a href="https://zhuang2002.github.io/FlashVSR/">	FLASHVSR:	Towards Real-Time Diffusion-Based Streaming Video Super-Resolution	</a>

<li/> <a href="https://huggingface.co/uwg/upscaler/tree/main/SwinIR">		HuggingFace - upscaler	</a>

<li/> <a href="https://github.com/XPixelGroup/HYPIR">		HYPIR	</a>,
	<a href="https://github.com/XPixelGroup">		XPixelGroup	</a>

<li/>	<a href="https://openart.ai/workflows/comfyuiblog/upscale-flux1-dev-controlnet-union-pro/hY1q5HqEY8jDu2KrHUE3">	Flux ControlNet Upscale	</a>

<li/>	<a href="https://github.com/shiimizu/ComfyUI-TiledDiffusion">	GitHub - shiimizu/ComfyUI-TiledDiffusion	</a>
<li/>	<a href="https://github.com/ssitu/ComfyUI_UltimateSDUpscale">	GitHub - ssitu/ComfyUI_UltimateSDUpscale	</a>

<li/>	<a href="https://therasr.github.io/">	Thera: Aliasing-Free Arbitrary-Scale Super-Resolution with Neural Heat Fields (No COmfyUI)	</a>

<li/>	OPPO Research Institute <a href="https://github.com/yjsunnn/DLoRAL">	One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution (DLoRAL)	</a>



<li/>	<a href="https://4kagent.github.io/">	4kagent (satellite)	</a>

<li id="mark"/>
	SeedVR
	<a href="https://iceclear.github.io/projects/seedvr2/">	seedvr2	</a>,
	<a href="https://github.com/ByteDance-Seed/SeedVR">	ByteDance-Seed/SeedVR	</a>, 
	<a href="https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler">	ComfyUI-SeedVR2_VideoUpscaler	</a>
	<a href="https://huggingface.co/numz/SeedVR2_comfyUI/tree/main">	SeedVR2_comfyUI (6Gb, 13Gb)	</a>
	<a href="https://huggingface.co/ByteDance-Seed/SeedVR2-7B/tree/main">	SeedVR2-7B (33Gb)	</a>,
	<a href="https://huggingface.co/ByteDance-Seed/SeedVR2-3B/tree/main">	SeedVR2-3B (14Gb)	</a>


</ol>
<!----------------------------------------------------------->
<h3 id="tagVideo">	Video			</h3>
<li/>	Keywords: <a href="https://github.com/showlab/Awesome-Video-Diffusion">	Github - Awesome Video Diffusion	</a>,
	<a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding">	Github - Awesome-LLMs-for-Video-Understanding	</a><br/><ol start=1 type=1>

<li/> <a href="https://ssj9596.github.io/one-to-all-animation-project/">	One-to-All Animation: Alignment-Free Character Animation and Image Pose Transfer (supports long video and misaligned characters)	</a>,
	<a href="https://github.com/ssj9596/One-to-All-Animation">		ssj9596/One-to-All-Animation	</a>

<li/> Subject-to-Video (s2v) 
	<a href="https://github.com/SkyworkAI/SkyReels-V1">	SkyworkAI/SkyReels-V1 </a>,
	<a href="https://huggingface.co/Kijai/SkyReels-V1-Hunyuan_comfy">	Kijai/SkyReels-V1-Hunyuan_comfy </a>,

<li id="mark"/>
	<a href="https://github.com/lllyasviel/FramePack">	FramePack - generate 1-minute video (60 seconds)	</a>, 
	<a href="https://openart.ai/workflows/pix_studio/framepack6g/kZCYax19o8PgNs25y1Xr">	pix_studio/framepack6g	</a>, 
	<a href="https://openart.ai/workflows/t8star/framepack-v1/5RCvSnlYsoe94w6NknBl">	t8star/framepack-v1	</a>

<li/>	<a href="https://www.genmo.ai/">	Genmo	</a>
	<a href="https://github.com/genmoai/mochi">	Mochi	</a>,
	<a href="https://github.com/kijai/ComfyUI-MochiWrapper">	ComfyUI-MochiWrapper	</a>
	<a href="https://github.com/logtd/ComfyUI-MochiEdit">		GitHub - logtd/ComfyUI-MochiEdit	</a>,

<li/>	<a href="https://github.com/logtd/ComfyUI-LTXTricks">		GitHub - logtd/ComfyUI-LTXTricks	</a>,
	<a href="https://openart.ai/workflows/pika_creamy_57/ltx091-i2vgen-long-video/b8ftrZGQ76AN2JDl6chW">		pika_creamy_57/ltx091-i2vgen-long-video	</a>,
	<a href="https://openart.ai/workflows/neofuturist/ltx-91-llm-movie-directors/YvlF3w9ndoUFYVchFc1x">	neofuturist/ltx-91-llm-movie-directors	</a>

<li/>	<a href="https://xiaoyushi97.github.io/Motion-I2V/">	Motion-I2V (No ComfyUI)	</a>
<li/>	<a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">	Google Genie-2 (No ComfyUI)	</a>
<li/>	<a href="https://huggingface.co/THUDM">	Tsinghua University Knowledge Engineering Group (KEG) & Data Mining 	</a>
	<a href="https://huggingface.co/THUDM/CogVideoX-5b">	CogVideoX-5b	</a>
	<a href="https://docs.google.com/spreadsheets/d/16eA6mSL8XkTcu9fSWkPSHfRIqyAKJbR1O99xnuGdCKY/edit?gid=0#gid=0">	CogVideoX models	</a>

<li/>	<a href="https://openart.ai/workflows/datou/cogvideo-tora/pfHf1wJRmxItzrqx6nHJ">	Alibaba TORA	</a>

<li/>	<a href="https://openart.ai/workflows/cat_untimely_42/ltx-video-enhance-stg/wMIwpeYhc372cOtpiiNo">		OpenArt - cat_untimely_42/ltx-video-enhance-stg	</a>

<li/>	<a href="https://openart.ai/workflows/cat_untimely_42/gimmvfi-video/Z4OcbafEIPpppewJLn7V">		OpenArt - cat_untimely_42/gimmvfi-video (frame smoothen)	</a>

<li/>	<a href="https://openart.ai/workflows/llama_gifted_48/basic-video-face-swap/brRQtSZeVrb4W62TL4Ln">	OpenArt - llama_gifted_48/basic-video-face-swap	</a>
<li/>	<a href="https://openart.ai/workflows/cat_untimely_42/gimmvfi-video/Z4OcbafEIPpppewJLn7V">	OpenArt - cat_untimely_42/gimmvfi-video	</a>

<li/>	<a href="https://magref-video.github.io/magref.github.io/">		MAGREF - Masked Guidance for Any-Reference Video Generation	</a>, 
	<a href="https://github.com/MAGREF-Video/MAGREF">	MAGREF-Video/MAGREF		</a>, 

<li/>	Phantom (Subject2Video)
	<a href="https://github.com/Phantom-video/Phantom">	Phantom: Subject-Consistent Video Generation via Cross-Modal Alignment	</a>, 
	<a href="https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/dev/example_workflows">	kijai/ComfyUI-WanVideoWrapper	</a>, 
	<a href="https://openart.ai/workflows/pix_studio/phantom/OHTBKYkLglPQCwKmwTxM">	pix_studio/phantom	</a>, 
	<a href="https://www.youtube.com/watch?v=ZYnhSTMa5VQ"> YouTube - Phantom workflow	</a>


<li/>	Remade-AI <a href="https://huggingface.co/Remade-AI">	HuggingFace - Remade-AI (video LORA) 	</a>, 
	<a href="https://huggingface.co/spaces/Remade-AI/remade-effects">	remade-effects	</a>,
	<a href="https://huggingface.co/Remade-AI/Selfie-With-Younger-Self">	Selfie-With-Younger-Self	</a>, 
	<a href="https://huggingface.co/Remade-AI/Rotate">	360 Degree Rotation	</a>, 
	<a href="https://huggingface.co/Remade-AI/Zoom-Call">	Zoom-Call	</a>, 
	<a href="https://github.com/amao2001/ganloss-latent-space/blob/main/workflow/2025-04-08%20%E4%BA%92%E5%8A%A8%E5%9E%8Blora.json">	workflow - Selfie-With-Younger-Self </a>

<li/>	SkyReels (e2v)
	<a href="https://huggingface.co/Skywork/SkyReels-V1-Hunyuan-I2V">	Skyreels V1: Human-Centric Video Foundation Model	</a>,
	<a href="https://github.com/SkyworkAI/SkyReels-V2">	SkyReels V2: Infinite-Length Film Generative Model	</a>, 
	<a href="https://openart.ai/workflows/pix_studio/aiskyreels-v2-dfcomfyui/gFBctY3VkaT8kIpYgPSd">	pix_studio/aiskyreels-v2-dfcomfyui	</a>,

<li/>	Tencent Hunyuan <a href="https://aivideo.hunyuan.tencent.com/">	Tencent Hunyuan	</a>,
	<a href="https://github.com/kijai/ComfyUI-HunyuanVideoWrapper">	ComfyUI-HunyuanVideoWrapper	</a>,
	<a href="https://huggingface.co/Kijai/HunyuanVideo_comfy/tree/main">	HunyuanVideo_comfy models	</a>,
	<a href="https://openart.ai/workflows/odam_ai/hunyuan-video-generation-face-swap/GyIn6sC02Pg0urjVDh7a">	hunyuan-video-generation-face-swap	</a>,
	<a href="https://openart.ai/workflows/latent_dream/hunyuan-vid2vid-txt2vid-fast-test-bench-v1/dSo9BIpAbkaalNZjL5RI">	latent_dream/hunyuan-vid2vid-txt2vid-fast-test-bench-v1	</a>,
	<a href="https://openart.ai/workflows/flounder_bowed_50/hunyuan-video-generation-large-model/VckyU6A9c7up0x2SvdL5">	flounder_bowed_50/hunyuan-video-generation-large-model	</a>, 
	<a href="https://github.com/Tencent-Hunyuan/HY-Motion-1.0">	HY-Motion 1.0: Scaling Flow Matching Models for 3D Motion Generation (text-to-3D==DeepMotion SayMotion)	</a>, 

<li/>	Video Frame Interpolation <a href="https://github.com/kijai/ComfyUI-GIMM-VFI">	kijai/ComfyUI-GIMM-VFI 	</a>,
	<a href="https://github.com/Fannovel16/ComfyUI-Frame-Interpolation">	Fannovel16/ComfyUI-Frame-Interpolation	</a>


<li/>	<a href="https://matankleiner.github.io/flowedit/">	FlowEdit 	</a>,
	<a href="https://www.youtube.com/watch?v=qJ21Vf0eHzg">	FlowEdit Image Editing (One-Click Text Modification)	</a>,
	<a href="https://github.com/logtd/ComfyUI-Fluxtapoz">		ComfyUI-Fluxtapoz	</a>,
	<a href="https://www.youtube.com/watch?v=s04W4tIyyKU">	FlowEdit Video Editing (No Masks, No Noise)	</a>,
	<a href="https://github.com/logtd/ComfyUI-LTXTricks">	logtd/ComfyUI-LTXTricks	</a>,
	<a href="https://github.com/logtd/ComfyUI-HunyuanLoom">	logtd/ComfyUI-HunyuanLoom	</a>,

<li/>	<a href="https://github.com/Fannovel16/ComfyUI-MotionDiff">	GitHub - Fannovel16/ComfyUI-MotionDiff	</a>

<li/>	WAN
	<a href="https://github.com/Wan-Video/Wan2.1">	Wan: Open Large-Scale Video Generative Models </a>,
	<a href="https://anytraj.github.io/">	ATI: Any Trajectory Instruction for Controllable Video Generation </a>, 
	<a href="https://self-forcing.github.io/">	Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion	</a>, 
	<a href="https://causvid.github.io/">	CausVid - From Slow Bidirectional to Fast Autoregressive Video Diffusion Models	</a>, 
	<a href="https://github.com/ali-vilab/VACE">	VACE: All-in-One Video Creation and Editing	</a>, 
	
</ol>
<hr><!----------------------------------------------------------->
<h3 id="tagDepthMap">	3D OpenPose / PoseNet / DepthMap	</h3>
<li/> Keyword:
	<a href="https://github.com/orgs/VAST-AI-Research/repositories">	VAST-AI-Research/repositories	</a>, 
	<a href="https://civitai.com/tag/poses">			CivitAI - poses	</a>,
	<a href="https://civitai.com/tag/openpose">		CivitAI - openpose	</a>,
	<a href="https://openart.ai/workflows/all?keyword=OpenPose">	openart- OpenPose	</a>,
	<a href="https://openart.ai/workflows/all?keyword=Depth">		openart- Depth	</a><br/><ol start=1 type=1>

<li/>	<a href="https://github.com/ZHO-ZHO-ZHO/X-Pose-ZHO"> GitHub - ZHO-ZHO-ZHO/X-Pose-ZHO	</a>

<li/>	<a href="https://openart.ai/workflows/pixeleasel/inpainting-pose-editor-color-match-composite/yqxeK0ENomLbCK8RLuXZ">	Inpainting Pose Editor, Color Match, Composite	</a>,
	<a href="https://github.com/hinablue/ComfyUI_3dPoseEditor"> GitHub - hinablue/ComfyUI_3dPoseEditor	</a>

<li/>	Data <a href="https://www.posemaniacs.com/poses"> PoseManiacs </a>,
	<a href="https://github.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset/tree/master"> Bandai-Namco  </a>,
	<a href="https://github.com/a-lgil/pose-depot">	Pose-Depot  </a>,
	<a href="https://civitai.com/models/157187/poses-bundle-aio-collection-over-20000-poses-controlnet">	CivitAI (>5Gb)	    </a>,
	<a href="https://posemy.art/angry-poses/">	PoseMyArt 	</a>,
	<a href="https://app.anything.world/">	AppAnything 1       </a>,
	<a href="https://app.anything.world/gallery/m/pieter_adams%230000">	AppAnything 2       </a>,
	<a href="https://app.anything.world/gallery/m/casual_winter_clothes_man%230000">	AppAnything 3       </a>,
	<a href="https://humandataset.com/">	HumanDataset 1   </a>,
	<a href="https://humandataset.com/individuals/">	HumanDataset 2   </a>,
	<a href="https://www.3dscanstore.com/blog">	3DScanStore   </a>,
	<a href="https://renderpeople.com/3d-people/?_product=rigged-people%2Canimated-people&_type=bundles">	RenderPeople   </a>,
	<a href="https://mocap.cs.cmu.edu/search.php?subjectnumber=%&motion=%">	CMU Graphics Lab Motion Capture Database	</a>, 
	<a href="https://github.com/microsoft/Microsoft-Rocketbox">		Microsoft-Rocketbox </a>,

<li/>	MarketPlace	 <a href="https://www.deviantart.com/tag/poses">	DevianArt	</a>,
	<a href="https://www.proko.com/@stan/tools">	Proko  	</a>,
	<a href="https://mocapcentral.com/collections/mocap-studio-series">`	MocapCentral	</a>

<li/>	Tool - Poses <a href="https://openposeai.com/">	OpenPoseAI (detect pose from image)	</a>

<li/>	<a href="https://github.com/MVIG-SJTU/AlphaPose">	AlphaPose (out-dated)	</a>

<li/> <a href="https://github.com/Fannovel16/comfyui_controlnet_aux">	comfyui_controlnet_aux-Midas, Zoe Depth	</a>,
	<a href="https://github.com/kijai/ComfyUI-Marigold">	ComfyUI-Marigold	</a>

<li/>	<a href="https://sotamak1r.github.io/deepverse/">	DeepVerse - 4D Autoregressive Video Generation as a World Model	</a>

<li/>	<a href="https://www.wlyu.me/FaceLift/">	FaceLift: Single Image to 3D Head	</a>

<li/>	<a href="https://github.com/fkryan/gazelle">	Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders </a>,
	<a href="https://huggingface.co/spaces/fffiloni/Gaze-LLE">		HuggingFace - fffiloni/Gaze-LLE </a>

<li/> <a href="https://generative-refocusing.github.io/">	Generative Refocusing: Flexible Defocus Control from a Single Image	</a>

<li/>	<a href="https://insta360-research-team.github.io/DAP_website/">	Insta360 - Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation 	</a>

<li/>	<a href="https://wgsxm.github.io/projects/partcrafter/">	PartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers	</a>

<li/>	<a href="https://github.com/TMElyralab/Comfyui-MusePose">	TMElyralab/Comfyui-MusePose	</a>

<li/>	<a href="https://github.com/akatz-ai/ComfyUI-DepthCrafter-Nodes">	Tencent - akatz-ai/ComfyUI-DepthCrafter-Nodes	</a>


<li/>	Pose Estimation <a href="https://shubham-goel.github.io/4dhumans/">	4DHumans	</a>,
	<a href="https://github.com/shubham-goel/4D-Humans">	shubham-goel/4D-Humans	</a>,
	<a href="https://github.com/open-mmlab/mmpose">	open-mmlab/mmpose	</a>,
	<a href="https://github.com/TMElyralab/Comfyui-MusePose">	TMElyralab/Comfyui-MusePose	</a>,
	<a href="https://github.com/logtd/ComfyUI-4DHumans">	logtd/ComfyUI-4DHumans	</a>

<li/>	GeoWizard <a href="https://huggingface.co/spaces/lemonaddie/geowizard">	GeoWizard 2D->3D	</a>,
	<a href="https://github.com/fuxiao0719/GeoWizard">	GitHub - fuxiao0719/GeoWizard	</a>,
	<a href="https://github.com/kijai/ComfyUI-Geowizard">	kijai/ComfyUI-Geowizard	</a>

<li/>	<a href="https://github.com/Westlake-AGI-Lab/Distill-Any-Depth">	Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator	</a>

<li/>	<a href="https://github.com/smthemex/ComfyUI_Sapiens">	ComfyUI_Sapiens - (seg,normal,pose,depth,mask maps)	</a>, 
	<a href="https://huggingface.co/facebook/sapiens-pose-1b-torchscript/tree/main">	sapiens-pose-1b-torchscript	</a>

<li/>	<a href="https://kaist-viclab.github.io/moblurf-site/">	MoBluRF: Motion Deblurring Neural Radiance Fields for Blurry Monocular Video	</a>

<li/>	<a href="https://francis-rings.github.io/StableAnimator/">	StableAnimator: High-Quality Identity-Preserving Human Image Animation	</a>

<li/>	<a href="https://zjp-shadow.github.io/works/UniRig/">	Unirig: Diverse Skeleton Rigging - One Model to Rig Them All		</a>
 
</ol>
<hr><!----------------------------------------------------------->
<h3 id="tag3D">	3D - 2D to 3D Monocular / NERF / Gaussian Splatting / Multi-view	</h3>
<li/> Keyword:
	<a href="https://github.com/longxiang-ai/awesome-gaussians">		Github - awesome-gaussians	</a>,
	<a href="https://github.com/Awesome3DGS/3D-Gaussian-Splatting-Papers">	3D Gaussian Splatting Papers	</a>, 
	<a href="https://github.com/MrNeRF/awesome-3D-gaussian-splatting">	Github - awesome-3D-gaussian-splatting	</a><br/><ol start=1 type=1>

<li/>	AllenAI	<a href="https://objaverse.allenai.org/">	Objaverse-XL - A Universe of 10M+ 3D Objects	</a>

<li/>	<a href="https://sunshinewyc.github.io/BlockGaussian/">	BlockGaussian	</a>

<li/>	ByteDance	<a href="https://seed.bytedance.com/en/seed3d">	Seed3D	</a>

<li/>	CityGaussian <a href="https://github.com/Linketic/CityGaussian">	CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes	</a>,
	<a href="https://dekuliutesla.github.io/citygs/">	GitHub - citygs	</a>, 
	<a href="https://arxiv.org/abs/2411.00771v1">	Paper	</a>

<li/>	<a href="https://github.com/wyysf-98/CraftsMan3D">	CraftsMan3D: High-fidelity Mesh Generation with 3D Native Generation and Interactive Geometry Refiner	</a>

<li/>	DreamTechAI <a href="https://github.com/DreamTechAI/Direct3D-S2">	Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention	</a>

<li/>	<a href="https://elevate3d.pages.dev/">	Elevate3D:  Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model </a>

<li/>	<a href="https://rogermm14.github.io/eonerf/"> EO-NeRF - Multi-Date Earth Observation NeRF - The Detail Is in the Shadows	</a>, 
	<a href="https://mezzelfo.github.io/EOGS/">	EOGS - Gaussian Splatting for Efficient Satellite Image Photogrammetry	</a>, 
	<a href="https://arxiv.org/abs/2412.13047">	EOGS Paper	</a>

<li/>	<a href="https://zju3dv.github.io/free360/">	Free 360 : Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views	</a>

<li/>	Geo4D <a href="https://geo4d.github.io/">	Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction	</a>, 
	<a href="https://github.com/jzr99/Geo4D">	jzr99/Geo4D	</a>

<li/>	Google <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/" title="Generates diverse training environments for embodied agents. From a single image prompt, it creates playable virtual worlds controllable via keyboard and mouse usable by both humans and AI systems.">	Google Genie2: Generative Interactive Environments	</a>,
	<a href="https://arxiv.org/html/2512.17883v1">	Map2Video: Street View Imagery Driven AI Video Generation (paper)	</a>

<li id="mark"/>
	<a href="https://eastbeanzhang.github.io/GUAVA/">	GUAVA: Generalizable Upper Body 3D Gaussian Avatar	</a>

<li/>	<a href="https://vast-ai-research.github.io/HoloPart/">	HoloPart: Generative 3D Part Amodal Segmentation	</a>,
	<a href="https://huggingface.co/spaces/VAST-AI/HoloPart">	HoloPart demo	</a>, 
	<a href="https://github.com/Pointcept/SAMPart3D">	SAMPart3D: Segment Any Part in 3D Objects		</a>

<li/>	<a href="https://3d-models.hunyuan.tencent.com/">	Hunyuan3D-2: High Resolution Textured 3D Assets Generation	</a>, 
	<a href="https://github.com/tencent/Hunyuan3D-2">	tencent/Hunyuan3D-2	</a>
	<a href="https://huggingface.co/spaces/tencent/Hunyuan3D-2">	Hunyuan3D-2 demo	 	</a>, 

<li/>	<a href="https://github.com/jtydhr88/ComfyUI-InstantMesh">	jtydhr88/ComfyUI-InstantMesh	</a>

<li/>	<a href="https://muelea.github.io/hsfm/">	Humans and Structure from Motion (HSfM) - Reconstructing People, Places, and Cameras	</a>

<li/>	<a href="https://hypernerf.github.io/">	HyperNerf : A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields	</a>

<li/>	<a href="https://immersegen.github.io/">	ImmerseGen - Agent-Guided Immersive World Generation with Alpha-Textured Proxies	</a>

<li/>	<a href="https://github.com/LargeWorldModel/LWM">	Large World Model (LWM)		</a>

<li/>	<a href="https://lotus-2.github.io/">	Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model	</a>,

<li/>	<a href="https://github.com/jasongzy/Make-It-Animatable">	Make-It-Animatable	</a>,
	<a href="https://huggingface.co/spaces/jasongzy/Make-It-Animatable">	Demo	</a>

<li/>	Meta - Multi-SpatialMLLM
	<a href="https://runsenxu.com/projects/Multi-SpatialMLLM/">	Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models	</a>, 
	<a href="https://www.amirbar.net/nwm/">	Navigation World Models		</a>, 
	<a href="https://map-anything.github.io/">	MapAnything: Universal Feed-Forward Metric 3D Reconstruction	</a>, 
	<a href="https://huggingface.co/spaces/facebook/map-anything">	facebook/map-anything		</a>,
	<a href="https://ai.meta.com/blog/sam-3d/">	SAM 3D Body: Robust Full-Body Human Mesh Recovery	</a>,
	<a href="https://www.aidemos.meta.com/segment-anything/editor/convert-body-to-3d">	SAM3D - Human demo	</a>,
	<a href="https://www.aidemos.meta.com/segment-anything/editor/convert-image-to-3d">	SAM3D - Object demo	</a>,
	<a href="https://github.com/facebookresearch/sam-3d-objects">	SAM 3D Objects	</a>,
	<a href="https://worldgen.github.io/">	WorldGen: Generate Any 3D Scene in Seconds	</a>,
	<a href="https://assetgen.github.io/">	AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials	</a>,  
	<a href="https://tuna-ai.org/">	TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models	</a>

<li/>	Microsoft	<a href="https://github.com/microsoft/MoGe">	MoGe - Monocular 2D->3D </a>,
	<a href="https://github.com/kijai/ComfyUI-MoGe">	kijai/ComfyUI-MoGe </a>

<li/>	<a href="https://blog.dynamicslab.ai/">	Mirage 2 - Generative World Engines	</a>,
	<a href="https://demo.dynamicslab.ai/chaos">	Mirage 2 - Demo	</a>,

<li/>	<a href="https://lizhiqi49.github.io/MoCA/">	MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation	</a>

<li/>	<a href="https://github.com/huanngzh/MV-Adapter">	MV-Adapter: Multi-view Consistent Image Generation Made Easy	</a>, 
	<a href="https://github.com/huanngzh/ComfyUI-MVAdapter">	ComfyUI-MVAdapter	</a>, 
	<a href="https://huggingface.co/collections/huanngzh/mv-adapter-spaces-677e497578747fd734a1b999">	MVAdapter-demo	</a>, 
	<a href="https://arxiv.org/abs/2412.03632">	Paper 	</a>

<li/>	<a href="https://nerfies.github.io/">	Nerfies: Deformable Neural Radiance Fields	</a>

<li/>	Niantic Labs <a href="https://nianticlabs.com/news/largegeospatialmodel/">	Large Geospatial Model	</a>

<li/>	Nvidia <a href="https://huggingface.co/papers/2501.03575" title="1) Cosmos-Predict1 simulates how the visual world evolves over time, learning physical world dynamics from video clips; 2) Cosmos-Transfer1 allows to guide world generation using multiple spatial control signals: segmentation, depth, edge maps, blurred visual inputs, etc.; 3) Cosmos-Reason1 reasons about what is happening, what will happen next, and what actions are feasible.">	 Cosmos World Foundation Models	</a>
	<a href="https://github.com/nvpro-samples/vk_gaussian_splatting">	Vulkan Gaussian Splatting	</a>, 

<li/>	OccluGaussian <a href="https://occlugaussian.github.io/">	OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering	</a>, 
	<a href="https://arxiv.org/abs/2503.16177v1">	Paper	</a>

<li/>	<a href="https://huggingface.co/spaces/omnipart/OmniPart">	OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion	</a>

<li/>	<a href="https://silent-chen.github.io/PartGen/">	PartGen - Part-level 3D Generation and Reconstruction	</a>

<li/>	<a href="https://panowan.variantconst.com/">	PanoWan: Lifting Diffusion Video Generation Models to 360 with Latitude/Longitude-aware Mechanisms	</a>

<li/>	<a href="https://playerone-hku.github.io/">	PlayerOne: Egocentric World Simulator	</a>

<li/>	<a href="https://arxiv.org/abs/2512.23676">	Web World Models (princeton)	</a>, 
	<a href="https://github.com/Princeton-AI2-Lab/Web-World-Models">	Princeton-AI2-Lab/Web-World-Models	</a>

<li/>	<a href="https://manycore-research.github.io/SpatialLM/">	SpatialLM: Large Language Model for Spatial Understanding (No COmfyUI)	</a>, 
	<a href="https://github.com/manycore-research/SpatialLM">	manycore-research/SpatialLM	</a>,
	<a href="https://huggingface.co/manycore-research">	manycore research	</a>


<li/>	<a href="https://jamesyjl.github.io/ShapeLLM/">		ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding	</a>

<li/>	<a href="https://arxiv.org/html/2508.09479v1">	SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images (no code)	</a>, 
	<li/>	<a href="https://github.com/kyjohnso/skysplat_blender">	SkySplat: 3DGS Blender Toolkit	</a>

<li/>	<a href="https://skyfall-gs.jayinnn.dev/">	Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery		</a>



<li/>	SkyWorld.AI	
	<a href="https://matrix-3d.github.io/">	Matrix-3D: Omnidirectional Explorable 3D World Generation	</a>
	<a href="https://matrix-game-v2.github.io/">	Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model	</a>

<li/>	Stable-X
	<a href="https://github.com/Stable-X/Hi3DGen">	Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal Bridging (no textures)	</a>,
	<a href="https://huggingface.co/spaces/Stable-X/Hi3DGen">	Stable-X/Hi3DGen demo	</a>, 
	<a href="https://github.com/Stable-X/ComfyUI-Hi3DGen">	Stable-X/ComfyUI-Hi3DGen	</a>,
	<a href="https://openart.ai/workflows/t8star/hi-3dgen3d/FLKrkPWPiwNO5W0RYl8F">	t8star/hi-3dgen3d	</a>

<li/>	<a href="https://github.com/lizhihao6/Sparc3D">	Sparc3D: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling	</a>, 
	<a href="https://huggingface.co/spaces/ilcve21/Sparc3D">	ilcve21/Sparc3D	</a>

<li/>	SynCity <a href="https://research.paulengstler.com/syncity/">	SynCity: Training-Free Generation of 3D Worlds	</a>,
	<a href="https://arxiv.org/abs/2503.16420">	Paper	</a>

<li/>	TenCent 
	<a href="https://3d-models.hunyuan.tencent.com/world/">	HunyuanWorld 1.0	</a>	,
	<a href="https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0">	Tencent-Hunyuan/HunyuanWorld-1.0	</a>, 
	<a href="https://huggingface.co/tencent/HunyuanWorld-1">	tencent/HunyuanWorld-1	</a>, 
	<a href="https://hunyuan-gamecraft.github.io/">		GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition	</a>, 
	<a href="https://3d-models.hunyuan.tencent.com/world/">	HunyuanWorld-Voyager	</a>, 
	<a href="https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager">	HunyuanWorld-Voyager:  depth and RGB video for efficient and direct 3D reconstruction	</a>, 
	<a href="https://github.com/Tencent-Hunyuan/HunyuanWorld-Mirror">	Hunyuan World Reconstruction	</a>
	</a>, 
	<a href="https://imlixinyang.github.io/FlashWorld-Project-Page/">	FlashWorld: High-quality 3D Scene Generation within Seconds	</a>

<li/> <a href="https://worldcanvas.github.io/">	The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text	</a>

<li/>	<a href="https://aim-uofa.github.io/Tinker/">	Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization		</a>

<li/>	
	Trellis3D <a href="https://trellis3d.github.io/">	Trellis3d - Structured 3D Latents - for Scalable and Versatile 3D Generation	</a>,
	<a href="https://huggingface.co/spaces/JeffreyXiang/TRELLIS">	Trellis demo	</a>,
	<a href="https://github.com/smthemex/ComfyUI_TRELLIS">	smthemex/ComfyUI_TRELLIS	</a>,
	<a href="https://github.com/if-ai/ComfyUI-IF_Trellis">	if-ai/ComfyUI-IF_Trellis	</a>,
	<a href="https://microsoft.github.io/TRELLIS.2/">	Trellis2	</a>, 

<li id="mark"/>	<a href="https://pku-yuangroup.github.io/UltraShape-1.0/">	UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement 	</a>

<li/>	<a href="https://github.com/lpiccinelli-eth/UniK3D">	UniK3D: Universal Camera Monocular 3D Estimation	</a>, 
	<a href="https://huggingface.co/spaces/lpiccinelli/UniK3D-demo">	UniK3D-demo	</a>

<li/>	<a href="https://metadriverse.github.io//urbansim/">		UrbanSim: Towards Autonomous Micromobility through Scalable Urban Simulation	</a>

<li/>	VastGaussian <a href="https://vastgaussian.github.io/">	VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction	</a>, 
	<a href="https://arxiv.org/abs/2402.17427">	Paper	</a>

<li/>	<a href="https://github.com/kyle8581/WMA-Agents">	Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation	</a>

<li/>	<a href="https://lizizun.github.io/WinT3R.github.io/">		WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool	</a>

<li/>	<a href="https://www.worldlabs.ai/blog">	WorldLabs.AI (Li FeiFei)	</a>

<li/>	<a href="https://world-grow.github.io/">	WorldGrow: Generating Infinite 3D World		</a>

<li/>	<a href="https://greatx3.github.io/Yan/">		Yan: Foundational Interactive Video Generation	</a>	

<li/>	<a href="https://stdstu12.github.io/YUME-Project/">	YUME 1.5: A Text-Controlled Interactive World Generation Model	</a>

<li/>	<a href="https://eric-ai-lab.github.io/3dtown.github.io/">	3DTown: Constructing a 3D Town from a Single Image	</a>

<li/>	
	HunYuan 3D <a href="https://www.hunyuan-3d.com/">	hunyuan-3d	</a>, 
	<a href="https://github.com/Tencent/Hunyuan3D-2">	Tencent/Hunyuan3D-2	</a>, 
	<a href="https://github.com/MrForExample/ComfyUI-3D-Pack">	MrForExample/ComfyUI-3D-Pack	</a>, 
	<a href="https://github.com/niknah/ComfyUI-Hunyuan-3D-2">	niknah/ComfyUI-Hunyuan-3D-2	</a>
	
<li/>	Vast.AI <a href="https://github.com/VAST-AI-Research/TripoSG">	Github - TripoSG	</a>

<li/>	<a href="https://huanngzh.github.io/VoxHammer-Page/">	VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space	</a>

</ol>

<hr><!----------------------------------------------------------->
<h3 id="tagAgent">	Agents	</h3>
<li/> Keyword:
	<a href="https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI">	Awesome Adaptation of Agentic AI	</a>, 
	<a href="https://github.com/AGI-Edgerunners/LLM-Agents-Papers">		Github - LLM-Agents-Papers	</a>,
	<a href="https://scholar.google.com/citations?user=jW5e3jkAAAAJ&hl=en">	Google Scholar	</a>,
	<a href="https://github.com/restyler/awesome-n8n">	GitHub - restyler/awesome-n8n		</a>,
	<a href="https://github.com/enescingoz/awesome-n8n-templates">	GitHub - enescingoz/awesome-n8n-templates		</a>,
	<br/>
	<ol start=1 type=1>

<li/>	Argilla	
	<a href="https://huggingface.co/datasets/argilla/FinePersonas-v0.1">	FinePersonas-v0.1	</a>, 
	<a href="https://huggingface.co/datasets/argilla/FinePersonas-Synthetic-Email-Conversations">	FinePersonas-Synthetic-Email-Conversations	</a>, 
	<a href="https://huggingface.co/spaces/argilla/synthetic-data-generator-argilla-reviewer">	synthetic-data-generator-argilla-reviewer	</a>, 

<li/>	<a href="https://agentsociety.readthedocs.io/en/latest/">	AgentSociety: LLM Agents in City		</a>,
	<a href="https://github.com/tsinghua-fib-lab/AgentSociety">	tsinghua-fib-lab/AgentSociety	</a>

<li/>	<a href="https://arxiv.org/html/2501.13554v3">	One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt
	</a>
	<a href="https://byliutao.github.io/1Prompt1Story.github.io/">	1Prompt1Story	</a>,
	<a href="https://github.com/byliutao/1Prompt1Story">		byliutao/1Prompt1Story	</a>

<li/>	<a href="https://thzva.github.io/deeppersona.github.io/ ">	DeepPersona: A Depth-First Synthetic-Persona Engine for Highly Personalized Language Models	</a>,
	<a href="https://github.com/thzva/Deeppersona">		thzva/Deeppersona	</a>, 
	<a href="https://huggingface.co/spaces/THzva/deeppersona-experience">	DeepPersona demo	<a/>

<li/>	<a href="https://huggingface.co/collections/maius/open-character-training">	Open Character Training	</a>

<li/>	<a href="https://huggingface.co/datasets/proj-persona/PersonaHub">	Tencent PersonaHub	</a>,
	<a href="https://github.com/tencent-ailab/persona-hub">	tencent-ailab/persona-hub	</a>, 
	<a href="https://arxiv.org/abs/2406.20094">	Paper	</a>

<li/>	<a href="https://github.com/FudanDISC/SocioVerse">	SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users	</a>, 
	<a href="https://arxiv.org/abs/2504.10157">	Paper	</a>

<li/>	<a href="https://github.com/ljcleo/agent_sense">	AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios	</a>, 
	<a href="https://arxiv.org/abs/2410.19346">	Paper	</a>

<li/>	<a href="https://github.com/AIPress-Web/AIPress-code">	AIPress: A Muti-Agent News Generation and Feedback Simulation System	</a>,
	<a href="https://arxiv.org/abs/2410.07561">	Paper	</a>

<li/>	Microsoft <a href="https://huggingface.co/microsoft/UserLM-8b">	UserLM-8B: Flipping the Dialogue: Training and Evaluating User Language Models	</a>, 
	<a href="https://github.com/microsoft/TinyTroupe">	TinyTroupe:	LLM-powered multiagent persona simulation for imagination enhancement and business insights	</a>

<li/>	n8n 
	<a href="https://github.com/jackvandervall/agentic-archive">	Agentic-Archive	</a>, 
	<a href="https://www.youtube.com/watch?v=Jtc6b1P40fc">	n8n + comfyUI API: Batch Convert Images to Video	</a>, 
	<a href="https://www.youtube.com/watch?v=wqwZoIPq0Jw">	n8n + comfyUI API: Simple	</a>

<li/>	Nvidia <a href="https://huggingface.co/datasets/nvidia/Nemotron-Personas">	Nemotron-Personas (US)	</a>, 
	<a href="https://huggingface.co/datasets/nvidia/Nemotron-Personas-India">		Nemotron-Personas (India)	</a>, 
	<a href="https://huggingface.co/datasets/nvidia/Nemotron-Personas-Japan">		Nemotron-Personas (Japan)	</a>

<li/>	<a href="https://github.com/xxyQwQ/ComfyBench">	ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems	</a>

<li/>	<a href="https://arxiv.org/html/2506.09790v1">	ComfyUI-R1: Exploring Reasoning Models for Workflow Generation	</a>

<li/>	OPPO	<a href="https://github.com/OPPO-PersonalAI/PersonalizedDeepResearchBench">	Towards Personalized Deep Research: Benchmarks and Evaluations	</a>

</ol>


<hr><!----------------------------------------------------------->
<h3 id="tagSimulation">	Simulation Worlds / GIS	</h3>
<li/> Keyword:
	<a href="https://github.com/leofan90/Awesome-World-Models">		Awesome World Models for Robotics	</a>, 
	<br/>
	<ol start=1 type=1>

<li/>	<a href="https://ai2thor.allenai.org/">	AI2-THOR: An Interactive 3D Environment for Visual AI		</a>
<li/>	<a href="https://github.com/a16z-infra/ai-town">	AI-Town (a16z)	</a>

<li/>	<a href="https://behavior.stanford.edu/">	BEHAVIOR-1K:	1000 realistic, full-length household tasks		</a>
<li/>	<a href="https://carla.org/">	CARLA: Open-source simulator for autonomous driving research		</a>
<li/>	<a href="https://embodied-city.fiblab.net/">	Embodied City: Embodied Agent in Urban Environment		</a>

<li/>	<a href="https://genesis-embodied-ai.github.io/">	Genesis: A Generative and Universal Physics Engine for Robotics and Beyond		</a>
<li/>	<a href="https://linsun449.github.io/GeoVLA/">	GeoVLA: Empowering 3D Representations in Vision-Language-Action Models	</a>

<li/>	<a href="https://aihabitat.org/habitat3/">	Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots		</a>
<li/>	<a href="https://github.com/InternRobotics/InternUtopia">	InternUtopia: Dream General Robots in a City at Scale	</a>

<li/>	<a href="https://metadriverse.github.io/metaurban/">	MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility		</a>
<li/>	<a href="https://minedojo.org/">	MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge (Minecraft)	</a>

<li/>	<a href="https://mindcraft-minecollab.github.io/index.html">	MindCraft: Collaborating Action by Action: Multi-agent LLM Framework for Embodied Reasoning	</a>

<li id="mark"/>	<a href="https://simworld.org/">	SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds		</a>

<li/>	<a href="http://unrealzoo.site/">	UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI	</a>
<li/>	<a href="https://virtual-community-ai.github.io/">	Virtual Community: An Open World for Humans, Robots, and Society		</a>

</ol>
<hr><!----------------------------------------------------------->
<h3 id="tagData">	Datasets	</h3>
<br/><ol start=1 type=1>

<li id="mark"/>	<a href="https://amazon-berkeley-objects.s3.us-east-1.amazonaws.com/index.html#download">	Amazon Berkeley Objects (ABO) Dataset (household items)	</a>

<li/>	<a href="https://c8241998.github.io/HumanRig/">	HumanRig - Learning Automatic Rigging for Humanoid Character in a Large Scale Dataset	</a>

<li/>	<a href="https://huggingface.co/datasets/kubernetes-bad/CivitAI-As-Characters">	CivitAI-As-Characters	</a>

<li/>	<a href="https://huggingface.co/spaces/HuggingFaceM4/FineVision">	FineVision: Open Data Is All You Need	</a> 200 datasets containing 17M images, 89M question-answer turns, and 10B answer tokens, totaling 5TB of high-quality data

<li/>	<a href="https://github.com/Yuan-ManX/ai-audio-datasets">	Yuan-ManX/ai-audio-datasets 	</a>

<li/>	<a href="https://www.cartoonmovement.com/cartoonist/26783">	Cartoon Movement (Kenny Tosh)	</a>

<li/>	Data <a href="https://www.artelingo.org/">	No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages	</a>

<li/>	Data <a href="https://blog.eleuther.ai/common-pile/">	Common Pile v0.1	</a>

<li/>	Data <a href="https://huggingface.co/datasets/facebook/omnilingual-asr-corpus">	Meta Omnilingual ASR Corpus	</a>

<li/>	Data - Movies <a href="https://huggingface.co/datasets/aneeshas/imsdb-drama-movie-scripts">	Movie-Drama scripts	</a>

<li/>	HuggingFace 
	<a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">	FineWeb dataset consists of more than 18.5T tokens (originally 15T tokens) of cleaned and deduplicated english web data from CommonCrawl	</a>, 
	<a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu">	 FineWeb-Edu dataset consists of 1.3T tokens and 5.4T tokens	</a>,

<li/>	<a href="https://huggingface.co/datasets/FudanCVL/MOSEv2">	MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes	</a>


<li/>	NTU <a href="https://github.com/ntudsp">	NTU EEE - Digital Signal Processing Laboratory	</a>,
	<a href="https://researchdata.ntu.edu.sg/">	Research Data	</a>

<li/>	Nvidia 
	<a href="https://blogs.nvidia.com/blog/speech-ai-dataset-models/">	Granary - Multilingual Speech AI		</a>, 
	<a href="https://huggingface.co/datasets/nvidia/PhysicalAI-Autonomous-Vehicles-NuRec">	nvidia/PhysicalAI-Autonomous-Vehicles-NuRec	</a>

<li/>	<a href="https://huggingface.co/UniqueData">	UniqueData	</a>,
	<a href="https://huggingface.co/datasets/UniqueData/facial-emotion-recognition-dataset/tree/main">	UniqueData/facial-emotion-recognition-dataset	</a>

<li/>	Cartoons 
	<a href="https://cartoonmovement.com/collection/israeli-palestinian-conflict">	Cartoon Movement - Israeli-Palestinian-Conflict	</a>,
	<a href="https://cartoonmovement.com/cartoon/israel-war-cycle">	Israel-War-Cycle	</a>,
	<a href="https://cartoonmovement.com/cartoonist/23389">	Paresh Nath, India	</a>,
	<a href="https://cartoonmovement.com/cartoonist/260">	Marian Kamensky, Austria	</a>,
	<a href="https://cartoonmovement.com/cartoonist/26783">	Kenny Tosh, Nigeria	</a>,

	<a href="https://www.thinkchina.sg/cartoon/thinkcartoon-29-august">	ThinkChina	</a>,

</ol>
<hr><!----------------------------------------------------------->
<h3 id="tagLighting">	Lighting	</h3>
<li/> Keyword:
	<a href="https://civitai.com/search/models?sortBy=models_v9&query=lighting">			CivitAI - lighting	</a>,
	<a href="https://openart.ai/workflows/all?keyword=lighting">		openart- lighting	</a><br/><ol start=1 type=1>

<li/>	Apple	<a href="https://github.com/apple/pico-banana-400k">		pico-banana-400k	</a>

<li/>	IC-Light <a href="https://openart.ai/workflows/risunobushi/relight-with-ic-light-and-background-as-lighting-source/UPKc0ak0YJibwbDff85i">	risunobushi/relight-with-ic-light-and-background-as-lighting-source	</a>,
	<a href="https://openart.ai/workflows/quhan/ic-light-custom-lighting-ic-light/6LWfdnkUyoNmkeXdrlcm">	quhan/ic-light-custom-lighting-ic-light	</a>,
	<a href="https://openart.ai/workflows/risunobushi/relight-people-preserve-colors-and-details/W50hRGaBRUlBT1ReD4EF">	risunobushi/relight-people-preserve-colors-and-details	</a>,

<li/>	<a href="https://github.com/LAOGOU-666/Comfyui-LG_Relight">	GitHub - LAOGOU-666/Comfyui-LG_Relight	</a>

<li/>	<a href="https://github.com/kijai/ComfyUI-Geowizard">	GitHub - kijai/ComfyUI-Geowizard	</a>
<li/>	<a href="https://github.com/kijai/ComfyUI-Lotus">	GitHub - kijai/ComfyUI-Lotus	</a>

<li/>	<a href="https://openart.ai/workflows/profile/risunobushi?sort=latest">	risunobushi (IC Lighting workflows)	</a>

<li/>	<a href="https://gojasper.github.io/latent-bridge-matching/">	LBM: Latent Bridge Matching for Fast Image-to-Image Translation	</a>, 
	<a href="https://github.com/gojasper/LBM">	gojasper/LBM	</a>, 
	<a href="https://openart.ai/workflows/t8star/lbmiclight-v23d-relight-ultra-8/LAxqUznhNGKgNSk1YiSB">	t8star/lbmiclight-v23d-relight-ultra-8	</a>,
	<a href="https://huggingface.co/spaces/jasperai/LBM_relighting">	jasperai/LBM_relighting	</a>

<li/>	Qwen-Image
	<a href="https://github.com/ModelTC/Qwen-Image-Lightning">	Qwen-Image-Lightning	</a>,



</ol>

</td>

</tr>
</table>

</body>
</html>

